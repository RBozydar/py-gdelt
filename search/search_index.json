{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#gdelt-py","title":"gdelt-py","text":"<p>A comprehensive Python client library for the GDELT (Global Database of Events, Language, and Tone) project.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Unified Interface: Single client covering all 6 REST APIs, 3 database tables, and NGrams dataset</li> <li>Version Normalization: Transparent handling of GDELT v1/v2 differences with normalized output</li> <li>Resilience: Automatic fallback to BigQuery when APIs fail or rate limit</li> <li>Modern Python: 3.11+, Async-first, Pydantic models, type hints throughout</li> <li>Streaming: Generator-based iteration for large datasets with memory efficiency</li> <li>Developer Experience: Clear errors, progress indicators, comprehensive lookups</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code># Basic installation\npip install gdelt-py\n\n# With BigQuery support\npip install gdelt-py[bigquery]\n\n# With all optional dependencies\npip install gdelt-py[bigquery,pandas]\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from py_gdelt import GDELTClient\nfrom py_gdelt.filters import DateRange, EventFilter\nfrom datetime import date, timedelta\n\nasync with GDELTClient() as client:\n    # Query recent events\n    yesterday = date.today() - timedelta(days=1)\n    event_filter = EventFilter(\n        date_range=DateRange(start=yesterday, end=yesterday),\n        actor1_country=\"USA\",\n    )\n\n    result = await client.events.query(event_filter)\n    print(f\"Found {len(result)} events\")\n\n    # Query Visual GKG (image analysis)\n    from py_gdelt.filters import VGKGFilter\n    vgkg_filter = VGKGFilter(\n        date_range=DateRange(start=yesterday),\n        domain=\"cnn.com\",\n    )\n    images = await client.vgkg.query(vgkg_filter)\n\n    # Query TV NGrams (word frequencies from TV)\n    from py_gdelt.filters import BroadcastNGramsFilter\n    tv_filter = BroadcastNGramsFilter(\n        date_range=DateRange(start=yesterday),\n        station=\"CNN\",\n        ngram_size=1,\n    )\n    ngrams = await client.tv_ngrams.query(tv_filter)\n\n    # Query Graph Datasets (quotes, entities, frontpage links)\n    from py_gdelt.filters import GQGFilter, GEGFilter\n    gqg_filter = GQGFilter(date_range=DateRange(start=yesterday))\n    quotes = await client.graphs.query_gqg(gqg_filter)\n\n    geg_filter = GEGFilter(date_range=DateRange(start=yesterday))\n    async for entity in client.graphs.stream_geg(geg_filter):\n        print(f\"{entity.name}: {entity.entity_type}\")\n</code></pre>"},{"location":"#data-sources-covered","title":"Data Sources Covered","text":""},{"location":"#file-based-endpoints","title":"File-Based Endpoints","text":"<ul> <li>Events - Structured event data (who did what to whom, when, where)</li> <li>Mentions - Article mentions of events over time</li> <li>GKG - Global Knowledge Graph (themes, entities, tone, quotations)</li> <li>NGrams - Word and phrase occurrences in articles (Jan 2020+)</li> <li>VGKG - Visual GKG (image annotations via Cloud Vision API)</li> <li>TV-GKG - Television GKG (closed caption analysis from TV broadcasts)</li> <li>TV NGrams - Word frequencies from TV closed captions</li> <li>Radio NGrams - Word frequencies from radio transcripts</li> <li>Graph Datasets - GQG, GEG, GFG, GGG, GEMG, GAL (see below)</li> </ul>"},{"location":"#rest-apis","title":"REST APIs","text":"<ul> <li>DOC 2.0 - Full-text article search and discovery</li> <li>GEO 2.0 - Geographic analysis and mapping</li> <li>Context 2.0 - Sentence-level contextual search</li> <li>TV 2.0 - Television news closed caption search</li> <li>TV AI 2.0 - AI-enhanced visual TV search (labels, OCR, faces)</li> <li>LowerThird \ud83c\udfd7\ufe0f - TV chyron/lower-third text search</li> <li>TVV \ud83c\udfd7\ufe0f - TV Visual channel inventory</li> <li>GKG GeoJSON v1 \ud83c\udfd7\ufe0f - Legacy geographic GKG API</li> </ul>"},{"location":"#graph-datasets","title":"Graph Datasets","text":"<ul> <li>GQG - Global Quotation Graph (extracted quotes with context)</li> <li>GEG - Global Entity Graph (NER via Cloud NLP API)</li> <li>GFG - Global Frontpage Graph (homepage link tracking)</li> <li>GGG - Global Geographic Graph (location co-mentions)</li> <li>GDG \ud83c\udfd7\ufe0f - Global Difference Graph (article change detection)</li> <li>GEMG - Global Embedded Metadata Graph (meta tags, JSON-LD)</li> <li>GRG \ud83c\udfd7\ufe0f - Global Relationship Graph (subject-verb-object triples)</li> <li>GAL - Article List (lightweight article metadata)</li> </ul>"},{"location":"#lookup-tables","title":"Lookup Tables","text":"<ul> <li>CAMEO - Event classification codes and Goldstein scale</li> <li>Themes - GKG theme taxonomy</li> <li>Countries - Country code conversions (FIPS \u2194 ISO)</li> <li>Ethnic/Religious Groups - Group classification codes</li> <li>GCAM \ud83c\udfd7\ufe0f - 2,300+ emotional/thematic dimensions</li> <li>Image Tags \ud83c\udfd7\ufe0f - Cloud Vision labels for DOC API</li> <li>Languages \ud83c\udfd7\ufe0f - Supported language codes</li> </ul>"},{"location":"#data-source-matrix","title":"Data Source Matrix","text":"Data Type API BigQuery Raw Files Time Range Fallback Articles (fulltext) DOC 2.0 - - Rolling 3 months - Article geography GEO 2.0 - - Rolling 7 days - Sentence context Context 2.0 - - Rolling 72 hours - TV captions TV 2.0 - - Jul 2009+ - TV visual/AI TV AI 2.0 - - Jul 2010+ - TV chyrons \ud83c\udfd7\ufe0f LowerThird - - Aug 2017+ - Events v2 - \u2713 \u2713 Feb 2015+ \u2713 Events v1 - \u2713 \u2713 1979 - Feb 2015 \u2713 Mentions - \u2713 \u2713 Feb 2015+ \u2713 GKG v2 - \u2713 \u2713 Feb 2015+ \u2713 GKG v1 - \u2713 \u2713 Apr 2013 - Feb 2015 \u2713 Web NGrams - \u2713 \u2713 Jan 2020+ \u2713 VGKG - \u2713 \u2713 Dec 2015+ \u2713 TV-GKG - \u2713 \u2713 Jul 2009+ \u2713 TV NGrams - - \u2713 Jul 2009+ - Radio NGrams - - \u2713 2017+ - GQG - - \u2713 Jan 2020+ - GEG - - \u2713 Jul 2016+ - GFG - - \u2713 Mar 2018+ - GGG - - \u2713 Jan 2020+ - GEMG - - \u2713 Jan 2020+ - GAL - - \u2713 Jan 2020+ - <p>\ud83c\udfd7\ufe0f = Work in progress - coming in future releases</p>"},{"location":"#key-concepts","title":"Key Concepts","text":""},{"location":"#async-first-design","title":"Async-First Design","text":"<p>All I/O operations are async by default for optimal performance:</p> <pre><code>async with GDELTClient() as client:\n    articles = await client.doc.query(doc_filter)\n</code></pre> <p>Synchronous wrappers are available for compatibility:</p> <pre><code>with GDELTClient() as client:\n    articles = client.doc.query_sync(doc_filter)\n</code></pre>"},{"location":"#streaming-for-efficiency","title":"Streaming for Efficiency","text":"<p>Process large datasets without loading everything into memory:</p> <pre><code>async with GDELTClient() as client:\n    async for event in client.events.stream(event_filter):\n        process(event)  # Memory-efficient\n</code></pre>"},{"location":"#type-safety","title":"Type Safety","text":"<p>Pydantic models throughout with full type hints:</p> <pre><code>event: Event = result[0]\nassert event.goldstein_scale  # Type-checked\n</code></pre>"},{"location":"#configuration","title":"Configuration","text":"<p>Flexible configuration via environment variables, TOML files, or programmatic settings:</p> <pre><code>settings = GDELTSettings(\n    timeout=60,\n    max_retries=5,\n    cache_dir=Path(\"/custom/cache\"),\n)\n\nasync with GDELTClient(settings=settings) as client:\n    ...\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>Full documentation available at: https://rbozydar.github.io/py-gdelt/</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! See Contributing Guide for details.</p>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE file for details.</p>"},{"location":"#links","title":"Links","text":"<ul> <li>GitHub Repository</li> <li>PyPI Package</li> <li>Documentation</li> <li>GDELT Project</li> </ul>"},{"location":"architecture/","title":"Architecture","text":"<p>This page describes the high-level architecture of gdelt-py and how it manages multiple data sources.</p>"},{"location":"architecture/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        GDELTClient                              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502   doc   \u2502 \u2502   geo   \u2502 \u2502 context \u2502 \u2502   tv    \u2502 \u2502   tv_ai   \u2502 \u2502\n\u2502  \u2502  (API)  \u2502 \u2502  (API)  \u2502 \u2502  (API)  \u2502 \u2502  (API)  \u2502 \u2502   (API)   \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 events  \u2502 \u2502mentions \u2502 \u2502   gkg   \u2502 \u2502 ngrams  \u2502 \u2502  lookups  \u2502 \u2502\n\u2502  \u2502(Multi)  \u2502 \u2502 (Multi) \u2502 \u2502 (Multi) \u2502 \u2502 (Multi) \u2502 \u2502 (Bundled) \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u25bc                     \u25bc                     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  REST APIs    \u2502    \u2502   BigQuery    \u2502    \u2502  Raw Files    \u2502\n\u2502  (Real-time)  \u2502    \u2502  (Fallback/   \u2502    \u2502   (HTTP)      \u2502\n\u2502               \u2502    \u2502   Historical) \u2502    \u2502               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#endpoint-types","title":"Endpoint Types","text":"<p>The library provides two categories of endpoints:</p> <p>API-Only Endpoints (top row):</p> <ul> <li><code>doc</code> - Article fulltext search (DOC 2.0 API)</li> <li><code>geo</code> - Geographic visualizations (GEO 2.0 API)</li> <li><code>context</code> - Sentence-level search (Context 2.0 API)</li> <li><code>tv</code> - Television caption search (TV 2.0 API)</li> <li><code>tv_ai</code> - Visual TV search (TV AI 2.0 API)</li> </ul> <p>These endpoints only access GDELT REST APIs and have no fallback options.</p> <p>Multi-Source Endpoints (bottom row):</p> <ul> <li><code>events</code> - Event records (Files + BigQuery)</li> <li><code>mentions</code> - Article mentions of events (Files + BigQuery)</li> <li><code>gkg</code> - Global Knowledge Graph records (Files + BigQuery)</li> <li><code>ngrams</code> - Web NGrams 3.0 (Files + BigQuery)</li> <li><code>lookups</code> - Reference data (bundled with library)</li> </ul> <p>These endpoints can switch between raw files and BigQuery for resilience.</p>"},{"location":"architecture/#multi-source-endpoints","title":"Multi-Source Endpoints","text":"<p>For Events, Mentions, GKG, and NGrams, the library implements intelligent source selection:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            EventsEndpoint               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  query(filter, source=\"auto\")           \u2502\n\u2502                                         \u2502\n\u2502  source=\"auto\" logic:                   \u2502\n\u2502  1. Try raw files (free, fast)          \u2502\n\u2502  2. On failure/429 \u2192 BigQuery fallback  \u2502\n\u2502  3. Large historical \u2192 prefer BigQuery  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#source-selection","title":"Source Selection","text":"<pre><code># Auto mode (default) - intelligent source selection\nevents = await client.events.query(filter)\n\n# Explicit file source\nevents = await client.events.query(filter, source=\"files\")\n\n# Explicit BigQuery source\nevents = await client.events.query(filter, source=\"bigquery\")\n</code></pre>"},{"location":"architecture/#fallback-behavior","title":"Fallback Behavior","text":"<p>When <code>source=\"auto\"</code> (default):</p> <ol> <li>Primary: Raw files are tried first (free, no authentication required)</li> <li>Fallback: On rate limiting (HTTP 429) or errors, automatically switches to BigQuery</li> <li>Large queries: For historical queries spanning many days, BigQuery may be preferred</li> </ol>"},{"location":"architecture/#configuration","title":"Configuration","text":"<p>Control fallback behavior via settings:</p> <pre><code>from py_gdelt import GDELTClient, GDELTSettings\n\nsettings = GDELTSettings(\n    fallback_to_bigquery=True,  # Enable automatic fallback\n    bigquery_project=\"my-gcp-project\",  # Required for BigQuery\n)\n\nasync with GDELTClient(settings=settings) as client:\n    # Will fallback to BigQuery if files fail\n    events = await client.events.query(filter)\n</code></pre>"},{"location":"architecture/#data-flow","title":"Data Flow","text":""},{"location":"architecture/#query-execution","title":"Query Execution","text":"<ol> <li>User creates a filter object (e.g., <code>EventFilter</code>)</li> <li>User calls endpoint method (e.g., <code>client.events.query(filter)</code>)</li> <li>Endpoint determines optimal source based on:</li> <li>User preference (<code>source</code> parameter)</li> <li>Query time range</li> <li>Previous failures/rate limits</li> <li>Source adapter fetches and parses data</li> <li>Results are returned as Pydantic models</li> </ol>"},{"location":"architecture/#streaming","title":"Streaming","text":"<p>For large result sets, use streaming to process records without loading everything into memory:</p> <pre><code>async with GDELTClient() as client:\n    async for event in client.events.stream(filter):\n        process(event)  # Memory efficient\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions to py-gdelt are welcome!</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<pre><code># Clone repository\ngit clone https://github.com/rbwasilewski/py-gdelt.git\ncd py-gdelt\n\n# Install development dependencies\npip install -e \".[dev,bigquery,pandas]\"\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Unit tests\npytest tests/\n\n# Integration tests (requires live API access)\npytest tests/integration/ -m integration\n\n# With coverage\npytest --cov=py_gdelt tests/\n</code></pre>"},{"location":"contributing/#code-quality","title":"Code Quality","text":"<pre><code># Linting\nruff check .\n\n# Type checking\nmypy .\n\n# Format code\nruff format .\n</code></pre>"},{"location":"contributing/#guidelines","title":"Guidelines","text":"<ul> <li>Follow PEP 8 style guide</li> <li>Add type hints to all functions</li> <li>Write tests for new features</li> <li>Update documentation</li> <li>Use meaningful commit messages</li> </ul>"},{"location":"contributing/#pull-requests","title":"Pull Requests","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Add tests</li> <li>Update documentation</li> <li>Run quality checks</li> <li>Submit pull request</li> </ol>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":"<p>Use GitHub issues for: - Bug reports - Feature requests - Documentation improvements</p> <p>Include: - Python version - py-gdelt version - Minimal reproduction code - Expected vs actual behavior</p>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree your contributions will be licensed under the MIT License.</p>"},{"location":"api/client/","title":"Client API","text":""},{"location":"api/client/#py_gdelt.client.GDELTClient","title":"<code>GDELTClient</code>","text":"<p>Main client for accessing all GDELT data sources.</p> <p>This is the primary entry point for the py-gdelt library. It manages the lifecycle of all dependencies (HTTP client, file source, BigQuery source) and provides convenient namespace access to all endpoints.</p> <p>The client can be used as either an async or sync context manager, and supports dependency injection for testing.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>GDELTSettings | None</code> <p>Optional GDELTSettings instance. If None, creates default settings.</p> <code>None</code> <code>config_path</code> <code>Path | None</code> <p>Optional path to TOML configuration file. Only used if settings is None. If both are provided, settings takes precedence.</p> <code>None</code> <code>http_client</code> <code>AsyncClient | None</code> <p>Optional shared HTTP client for testing. If None, client creates and owns its own HTTP client. If provided, the lifecycle is managed externally and the client will not be closed on exit.</p> <code>None</code> Example <p>async with GDELTClient() as client: ...     events = await client.events.query(filter_obj) ...     articles = await client.doc.search(\"climate\") ...     theme = client.lookups.themes.get_category(\"ENV_CLIMATECHANGE\")</p> Source code in <code>src/py_gdelt/client.py</code> <pre><code>class GDELTClient:\n    \"\"\"Main client for accessing all GDELT data sources.\n\n    This is the primary entry point for the py-gdelt library. It manages the\n    lifecycle of all dependencies (HTTP client, file source, BigQuery source)\n    and provides convenient namespace access to all endpoints.\n\n    The client can be used as either an async or sync context manager, and\n    supports dependency injection for testing.\n\n    Args:\n        settings: Optional GDELTSettings instance. If None, creates default settings.\n        config_path: Optional path to TOML configuration file. Only used if\n            settings is None. If both are provided, settings takes precedence.\n        http_client: Optional shared HTTP client for testing. If None, client\n            creates and owns its own HTTP client. If provided, the lifecycle\n            is managed externally and the client will not be closed on exit.\n\n    Example:\n        &gt;&gt;&gt; async with GDELTClient() as client:\n        ...     events = await client.events.query(filter_obj)\n        ...     articles = await client.doc.search(\"climate\")\n        ...     theme = client.lookups.themes.get_category(\"ENV_CLIMATECHANGE\")\n\n        &gt;&gt;&gt; # With config file\n        &gt;&gt;&gt; async with GDELTClient(config_path=Path(\"gdelt.toml\")) as client:\n        ...     pass\n\n        &gt;&gt;&gt; # With custom settings\n        &gt;&gt;&gt; settings = GDELTSettings(timeout=60, max_retries=5)\n        &gt;&gt;&gt; async with GDELTClient(settings=settings) as client:\n        ...     pass\n\n        &gt;&gt;&gt; # With dependency injection for testing\n        &gt;&gt;&gt; async with httpx.AsyncClient() as http_client:\n        ...     async with GDELTClient(http_client=http_client) as client:\n        ...         pass\n    \"\"\"\n\n    def __init__(\n        self,\n        settings: GDELTSettings | None = None,\n        config_path: Path | None = None,\n        http_client: httpx.AsyncClient | None = None,\n    ) -&gt; None:\n        # Initialize settings\n        if settings is not None:\n            self.settings = settings\n        elif config_path is not None:\n            self.settings = GDELTSettings(config_path=config_path)\n        else:\n            self.settings = GDELTSettings()\n\n        # HTTP client management\n        self._http_client = http_client\n        self._owns_http_client = http_client is None\n\n        # Source instances (created lazily)\n        self._file_source: FileSource | None = None\n        self._bigquery_source: BigQuerySource | None = None\n        self._owns_sources = True\n\n        # Lifecycle state\n        self._initialized = False\n\n    async def _initialize(self) -&gt; None:\n        \"\"\"Initialize sources and HTTP client.\n\n        Called automatically on first use via context manager.\n        Creates HTTP client (if not injected) and initializes file source.\n        BigQuery source is created only if credentials are configured.\n        \"\"\"\n        if self._initialized:\n            return\n\n        # Create HTTP client if not injected\n        if self._owns_http_client:\n            self._http_client = httpx.AsyncClient(\n                timeout=httpx.Timeout(\n                    connect=10.0,\n                    read=self.settings.timeout,\n                    write=10.0,\n                    pool=5.0,\n                ),\n                follow_redirects=True,\n            )\n\n        # Initialize file source\n        self._file_source = FileSource(\n            settings=self.settings,\n            client=self._http_client,\n        )\n        await self._file_source.__aenter__()\n\n        # Initialize BigQuery source if credentials are configured\n        if self.settings.bigquery_project and self.settings.bigquery_credentials:\n            try:\n                self._bigquery_source = BigQuerySource(settings=self.settings)\n                logger.debug(\n                    \"Initialized BigQuerySource with project %s\",\n                    self.settings.bigquery_project,\n                )\n            except ImportError as e:\n                # google-cloud-bigquery package not installed\n                logger.warning(\n                    \"BigQuery package not installed: %s. \"\n                    \"Install with: pip install py-gdelt[bigquery]\",\n                    e,\n                )\n                self._bigquery_source = None\n            except (OSError, FileNotFoundError) as e:\n                # Credentials file not found or not readable\n                logger.warning(\n                    \"BigQuery credentials file error: %s. BigQuery fallback will be unavailable.\",\n                    e,\n                )\n                self._bigquery_source = None\n            except Exception as e:  # noqa: BLE001\n                # Catch all Google SDK errors without importing optional dependency\n                # This is an error boundary - BigQuery is optional, errors should not crash\n                logger.warning(\n                    \"Failed to initialize BigQuerySource (%s): %s. \"\n                    \"BigQuery fallback will be unavailable.\",\n                    type(e).__name__,\n                    e,\n                )\n                self._bigquery_source = None\n\n        self._initialized = True\n        logger.debug(\"GDELTClient initialized successfully\")\n\n    async def _cleanup(self) -&gt; None:\n        \"\"\"Clean up resources.\n\n        Closes file source, BigQuery source (if created), and HTTP client (if owned).\n        \"\"\"\n        if not self._initialized:\n            return\n\n        # Close file source\n        if self._file_source is not None:\n            await self._file_source.__aexit__(None, None, None)\n            self._file_source = None\n\n        # BigQuery source doesn't need explicit cleanup (no persistent connections)\n        self._bigquery_source = None\n\n        # Close HTTP client if we own it\n        if self._owns_http_client and self._http_client is not None:\n            await self._http_client.aclose()\n            self._http_client = None\n\n        self._initialized = False\n        logger.debug(\"GDELTClient cleaned up successfully\")\n\n    async def __aenter__(self) -&gt; GDELTClient:\n        \"\"\"Async context manager entry.\n\n        Returns:\n            Self for use in async with statement.\n\n        Example:\n            &gt;&gt;&gt; async with GDELTClient() as client:\n            ...     events = await client.events.query(filter_obj)\n        \"\"\"\n        await self._initialize()\n        return self\n\n    async def __aexit__(self, *args: Any) -&gt; None:\n        \"\"\"Async context manager exit.\n\n        Cleans up all owned resources.\n\n        Args:\n            *args: Exception info (unused, but required by protocol).\n        \"\"\"\n        await self._cleanup()\n\n    def __enter__(self) -&gt; GDELTClient:\n        \"\"\"Sync context manager entry.\n\n        This provides synchronous (blocking) access to the client for use in\n        non-async code. It uses asyncio.run() internally to manage the event loop.\n\n        Important Limitations:\n            - MUST be called from outside any existing async context/event loop.\n              Calling from within an async function will raise RuntimeError.\n            - Creates a new event loop for each context manager entry.\n            - Use the async context manager (async with) when possible for\n              better performance and compatibility.\n\n        Returns:\n            Self for use in with statement.\n\n        Raises:\n            RuntimeError: If called from within an already running event loop.\n\n        Example:\n            &gt;&gt;&gt; # Correct: Used from synchronous code\n            &gt;&gt;&gt; with GDELTClient() as client:\n            ...     events = client.events.query_sync(filter_obj)\n            ...\n            &gt;&gt;&gt; # Wrong: Don't use from async code - use 'async with' instead\n            &gt;&gt;&gt; async def bad_example():\n            ...     with GDELTClient() as client:  # RuntimeError!\n            ...         pass\n        \"\"\"\n        asyncio.run(self._initialize())\n        return self\n\n    def __exit__(self, *args: Any) -&gt; None:\n        \"\"\"Sync context manager exit.\n\n        Cleans up all owned resources. Uses asyncio.run() internally.\n\n        Args:\n            *args: Exception info (unused, but required by protocol).\n\n        Raises:\n            RuntimeError: If called from within an already running event loop.\n        \"\"\"\n        asyncio.run(self._cleanup())\n\n    # Endpoint namespaces (lazy initialization via cached_property)\n\n    @cached_property\n    def events(self) -&gt; EventsEndpoint:\n        \"\"\"Access the Events endpoint.\n\n        Provides methods for querying GDELT Events data from files or BigQuery.\n\n        Returns:\n            EventsEndpoint instance.\n\n        Raises:\n            RuntimeError: If client not initialized (use context manager).\n\n        Example:\n            &gt;&gt;&gt; async with GDELTClient() as client:\n            ...     filter_obj = EventFilter(date_range=DateRange(start=date(2024, 1, 1)))\n            ...     events = await client.events.query(filter_obj)\n        \"\"\"\n        if self._file_source is None:\n            msg = \"GDELTClient not initialized. Use 'async with GDELTClient() as client:'\"\n            raise RuntimeError(msg)\n        return EventsEndpoint(\n            file_source=self._file_source,\n            bigquery_source=self._bigquery_source,\n            fallback_enabled=self.settings.fallback_to_bigquery,\n        )\n\n    @cached_property\n    def mentions(self) -&gt; MentionsEndpoint:\n        \"\"\"Access the Mentions endpoint.\n\n        Provides methods for querying GDELT Mentions data from files or BigQuery.\n\n        Returns:\n            MentionsEndpoint instance.\n\n        Raises:\n            RuntimeError: If client not initialized (use context manager).\n\n        Example:\n            &gt;&gt;&gt; async with GDELTClient() as client:\n            ...     filter_obj = EventFilter(date_range=DateRange(start=date(2024, 1, 1)))\n            ...     mentions = await client.mentions.query(\"123456789\", filter_obj)\n        \"\"\"\n        if self._file_source is None:\n            msg = \"GDELTClient not initialized. Use 'async with GDELTClient() as client:'\"\n            raise RuntimeError(msg)\n        return MentionsEndpoint(\n            file_source=self._file_source,\n            bigquery_source=self._bigquery_source,\n            fallback_enabled=self.settings.fallback_to_bigquery,\n        )\n\n    @cached_property\n    def gkg(self) -&gt; GKGEndpoint:\n        \"\"\"Access the GKG (Global Knowledge Graph) endpoint.\n\n        Provides methods for querying GDELT GKG data from files or BigQuery.\n\n        Returns:\n            GKGEndpoint instance.\n\n        Raises:\n            RuntimeError: If client not initialized (use context manager).\n\n        Example:\n            &gt;&gt;&gt; async with GDELTClient() as client:\n            ...     filter_obj = GKGFilter(\n            ...         date_range=DateRange(start=date(2024, 1, 1)),\n            ...         themes=[\"ENV_CLIMATECHANGE\"]\n            ...     )\n            ...     records = await client.gkg.query(filter_obj)\n        \"\"\"\n        if self._file_source is None:\n            msg = \"GDELTClient not initialized. Use 'async with GDELTClient() as client:'\"\n            raise RuntimeError(msg)\n        return GKGEndpoint(\n            file_source=self._file_source,\n            bigquery_source=self._bigquery_source,\n            fallback_enabled=self.settings.fallback_to_bigquery,\n        )\n\n    @cached_property\n    def ngrams(self) -&gt; NGramsEndpoint:\n        \"\"\"Access the NGrams endpoint.\n\n        Provides methods for querying GDELT NGrams data (files only).\n\n        Returns:\n            NGramsEndpoint instance.\n\n        Raises:\n            RuntimeError: If client not initialized (use context manager).\n\n        Example:\n            &gt;&gt;&gt; async with GDELTClient() as client:\n            ...     filter_obj = NGramsFilter(\n            ...         date_range=DateRange(start=date(2024, 1, 1)),\n            ...         language=\"en\"\n            ...     )\n            ...     records = await client.ngrams.query(filter_obj)\n        \"\"\"\n        if self._file_source is None:\n            msg = \"GDELTClient not initialized. Use 'async with GDELTClient() as client:'\"\n            raise RuntimeError(msg)\n        return NGramsEndpoint(\n            settings=self.settings,\n            file_source=self._file_source,\n        )\n\n    @cached_property\n    def tv_ngrams(self) -&gt; TVNGramsEndpoint:\n        \"\"\"Access the TV NGrams endpoint.\n\n        Provides methods for querying word frequency from TV broadcast closed captions.\n\n        Returns:\n            TVNGramsEndpoint instance.\n\n        Raises:\n            RuntimeError: If client not initialized (use context manager).\n\n        Example:\n            &gt;&gt;&gt; async with GDELTClient() as client:\n            ...     filter_obj = BroadcastNGramsFilter(\n            ...         date_range=DateRange(start=date(2024, 1, 1)),\n            ...         station=\"CNN\"\n            ...     )\n            ...     records = await client.tv_ngrams.query(filter_obj)\n        \"\"\"\n        if self._file_source is None:\n            msg = \"GDELTClient not initialized. Use 'async with GDELTClient() as client:'\"\n            raise RuntimeError(msg)\n        return TVNGramsEndpoint(\n            settings=self.settings,\n            file_source=self._file_source,\n        )\n\n    @cached_property\n    def radio_ngrams(self) -&gt; RadioNGramsEndpoint:\n        \"\"\"Access the Radio NGrams endpoint.\n\n        Provides methods for querying word frequency from radio broadcast transcripts.\n\n        Returns:\n            RadioNGramsEndpoint instance.\n\n        Raises:\n            RuntimeError: If client not initialized (use context manager).\n\n        Example:\n            &gt;&gt;&gt; async with GDELTClient() as client:\n            ...     filter_obj = BroadcastNGramsFilter(\n            ...         date_range=DateRange(start=date(2024, 1, 1)),\n            ...         station=\"NPR\"\n            ...     )\n            ...     records = await client.radio_ngrams.query(filter_obj)\n        \"\"\"\n        if self._file_source is None:\n            msg = \"GDELTClient not initialized. Use 'async with GDELTClient() as client:'\"\n            raise RuntimeError(msg)\n        return RadioNGramsEndpoint(\n            settings=self.settings,\n            file_source=self._file_source,\n        )\n\n    @cached_property\n    def vgkg(self) -&gt; VGKGEndpoint:\n        \"\"\"Access the VGKG (Visual Global Knowledge Graph) endpoint.\n\n        Provides methods for querying Google Cloud Vision API analysis of news images.\n\n        Returns:\n            VGKGEndpoint instance.\n\n        Raises:\n            RuntimeError: If client not initialized (use context manager).\n\n        Example:\n            &gt;&gt;&gt; async with GDELTClient() as client:\n            ...     filter_obj = VGKGFilter(\n            ...         date_range=DateRange(start=date(2024, 1, 1)),\n            ...         domain=\"cnn.com\"\n            ...     )\n            ...     records = await client.vgkg.query(filter_obj)\n        \"\"\"\n        if self._file_source is None:\n            msg = \"GDELTClient not initialized. Use 'async with GDELTClient() as client:'\"\n            raise RuntimeError(msg)\n        return VGKGEndpoint(\n            settings=self.settings,\n            file_source=self._file_source,\n        )\n\n    @cached_property\n    def tv_gkg(self) -&gt; TVGKGEndpoint:\n        \"\"\"Access the TV GKG (TV Global Knowledge Graph) endpoint.\n\n        Provides methods for querying GKG data from TV broadcast closed captions.\n\n        Returns:\n            TVGKGEndpoint instance.\n\n        Raises:\n            RuntimeError: If client not initialized (use context manager).\n\n        Example:\n            &gt;&gt;&gt; async with GDELTClient() as client:\n            ...     filter_obj = TVGKGFilter(\n            ...         date_range=DateRange(start=date(2024, 1, 1)),\n            ...         station=\"CNN\"\n            ...     )\n            ...     records = await client.tv_gkg.query(filter_obj)\n        \"\"\"\n        if self._file_source is None:\n            msg = \"GDELTClient not initialized. Use 'async with GDELTClient() as client:'\"\n            raise RuntimeError(msg)\n        return TVGKGEndpoint(\n            settings=self.settings,\n            file_source=self._file_source,\n        )\n\n    @cached_property\n    def graphs(self) -&gt; GraphEndpoint:\n        \"\"\"Access the Graph datasets endpoint.\n\n        Provides methods for querying GDELT Graph datasets (GQG, GEG, GFG, GGG, GEMG, GAL)\n        from file downloads.\n\n        Returns:\n            GraphEndpoint instance.\n\n        Raises:\n            RuntimeError: If client not initialized (use context manager).\n\n        Example:\n            &gt;&gt;&gt; async with GDELTClient() as client:\n            ...     from py_gdelt.filters import GQGFilter, DateRange\n            ...     filter_obj = GQGFilter(\n            ...         date_range=DateRange(start=date(2025, 1, 20))\n            ...     )\n            ...     result = await client.graphs.query_gqg(filter_obj)\n            ...     for record in result:\n            ...         print(record.quotes)\n        \"\"\"\n        if self._file_source is None:\n            msg = \"GDELTClient not initialized. Use 'async with GDELTClient() as client:'\"\n            raise RuntimeError(msg)\n        return GraphEndpoint(\n            file_source=self._file_source,\n        )\n\n    @cached_property\n    def doc(self) -&gt; DocEndpoint:\n        \"\"\"Access the DOC 2.0 API endpoint.\n\n        Provides methods for searching GDELT articles via the DOC API.\n\n        Returns:\n            DocEndpoint instance.\n\n        Raises:\n            RuntimeError: If client not initialized (use context manager).\n\n        Example:\n            &gt;&gt;&gt; async with GDELTClient() as client:\n            ...     articles = await client.doc.search(\"climate change\", max_results=100)\n        \"\"\"\n        if self._http_client is None:\n            msg = \"GDELTClient not initialized. Use 'async with GDELTClient() as client:'\"\n            raise RuntimeError(msg)\n        return DocEndpoint(\n            settings=self.settings,\n            client=self._http_client,\n        )\n\n    @cached_property\n    def geo(self) -&gt; GeoEndpoint:\n        \"\"\"Access the GEO 2.0 API endpoint.\n\n        Provides methods for querying geographic locations from news articles.\n\n        Returns:\n            GeoEndpoint instance.\n\n        Raises:\n            RuntimeError: If client not initialized (use context manager).\n\n        Example:\n            &gt;&gt;&gt; async with GDELTClient() as client:\n            ...     result = await client.geo.search(\"earthquake\", max_points=100)\n        \"\"\"\n        if self._http_client is None:\n            msg = \"GDELTClient not initialized. Use 'async with GDELTClient() as client:'\"\n            raise RuntimeError(msg)\n        return GeoEndpoint(\n            settings=self.settings,\n            client=self._http_client,\n        )\n\n    @cached_property\n    def context(self) -&gt; ContextEndpoint:\n        \"\"\"Access the Context 2.0 API endpoint.\n\n        Provides methods for contextual analysis of search terms.\n\n        Returns:\n            ContextEndpoint instance.\n\n        Raises:\n            RuntimeError: If client not initialized (use context manager).\n\n        Example:\n            &gt;&gt;&gt; async with GDELTClient() as client:\n            ...     result = await client.context.analyze(\"climate change\")\n        \"\"\"\n        if self._http_client is None:\n            msg = \"GDELTClient not initialized. Use 'async with GDELTClient() as client:'\"\n            raise RuntimeError(msg)\n        return ContextEndpoint(\n            settings=self.settings,\n            client=self._http_client,\n        )\n\n    @cached_property\n    def tv(self) -&gt; TVEndpoint:\n        \"\"\"Access the TV API endpoint.\n\n        Provides methods for querying television news transcripts.\n\n        Returns:\n            TVEndpoint instance.\n\n        Raises:\n            RuntimeError: If client not initialized (use context manager).\n\n        Example:\n            &gt;&gt;&gt; async with GDELTClient() as client:\n            ...     clips = await client.tv.search(\"climate change\", station=\"CNN\")\n        \"\"\"\n        if self._http_client is None:\n            msg = \"GDELTClient not initialized. Use 'async with GDELTClient() as client:'\"\n            raise RuntimeError(msg)\n        return TVEndpoint(\n            settings=self.settings,\n            client=self._http_client,\n        )\n\n    @cached_property\n    def tv_ai(self) -&gt; TVAIEndpoint:\n        \"\"\"Access the TVAI API endpoint.\n\n        Provides methods for AI-enhanced television news analysis.\n\n        Returns:\n            TVAIEndpoint instance.\n\n        Raises:\n            RuntimeError: If client not initialized (use context manager).\n\n        Example:\n            &gt;&gt;&gt; async with GDELTClient() as client:\n            ...     result = await client.tv_ai.analyze(\"election coverage\")\n        \"\"\"\n        if self._http_client is None:\n            msg = \"GDELTClient not initialized. Use 'async with GDELTClient() as client:'\"\n            raise RuntimeError(msg)\n        return TVAIEndpoint(\n            settings=self.settings,\n            client=self._http_client,\n        )\n\n    @cached_property\n    def lowerthird(self) -&gt; LowerThirdEndpoint:\n        \"\"\"Access the LowerThird (Chyron) API.\n\n        Provides methods for searching OCR'd TV chyrons (lower-third text overlays).\n\n        Returns:\n            LowerThirdEndpoint for searching TV chyrons.\n\n        Raises:\n            RuntimeError: If client not initialized (use context manager).\n\n        Example:\n            &gt;&gt;&gt; async with GDELTClient() as client:\n            ...     clips = await client.lowerthird.search(\"breaking news\")\n        \"\"\"\n        if self._http_client is None:\n            msg = \"GDELTClient not initialized. Use 'async with GDELTClient() as client:'\"\n            raise RuntimeError(msg)\n        return LowerThirdEndpoint(\n            settings=self.settings,\n            client=self._http_client,\n        )\n\n    @cached_property\n    def tvv(self) -&gt; TVVEndpoint:\n        \"\"\"Access the TV Visual (TVV) API for channel inventory.\n\n        Provides methods for retrieving TV channel metadata.\n\n        Returns:\n            TVVEndpoint for channel metadata.\n\n        Raises:\n            RuntimeError: If client not initialized (use context manager).\n\n        Example:\n            &gt;&gt;&gt; async with GDELTClient() as client:\n            ...     channels = await client.tvv.get_inventory()\n        \"\"\"\n        if self._http_client is None:\n            msg = \"GDELTClient not initialized. Use 'async with GDELTClient() as client:'\"\n            raise RuntimeError(msg)\n        return TVVEndpoint(\n            settings=self.settings,\n            client=self._http_client,\n        )\n\n    @cached_property\n    def gkg_geojson(self) -&gt; GKGGeoJSONEndpoint:\n        \"\"\"Access the GKG GeoJSON API (v1.0 Legacy).\n\n        Provides methods for querying geographic GKG data as GeoJSON.\n\n        Returns:\n            GKGGeoJSONEndpoint for geographic GKG queries.\n\n        Raises:\n            RuntimeError: If client not initialized (use context manager).\n\n        Example:\n            &gt;&gt;&gt; async with GDELTClient() as client:\n            ...     result = await client.gkg_geojson.search(\"TERROR\", timespan=60)\n        \"\"\"\n        if self._http_client is None:\n            msg = \"GDELTClient not initialized. Use 'async with GDELTClient() as client:'\"\n            raise RuntimeError(msg)\n        return GKGGeoJSONEndpoint(\n            settings=self.settings,\n            client=self._http_client,\n        )\n\n    @cached_property\n    def lookups(self) -&gt; Lookups:\n        \"\"\"Access lookup tables for CAMEO codes, themes, and countries.\n\n        Provides access to all GDELT lookup tables with lazy loading.\n\n        Returns:\n            Lookups instance for code/theme/country lookups.\n\n        Example:\n            &gt;&gt;&gt; async with GDELTClient() as client:\n            ...     # CAMEO codes\n            ...     event_entry = client.lookups.cameo[\"14\"]\n            ...     event_name = event_entry.name  # \"PROTEST\"\n            ...\n            ...     # GKG themes\n            ...     category = client.lookups.themes.get_category(\"ENV_CLIMATECHANGE\")\n            ...\n            ...     # Country codes\n            ...     iso_code = client.lookups.countries.fips_to_iso3(\"US\")  # \"USA\"\n        \"\"\"\n        return Lookups()\n</code></pre>"},{"location":"api/client/#py_gdelt.client.GDELTClient--with-config-file","title":"With config file","text":"<p>async with GDELTClient(config_path=Path(\"gdelt.toml\")) as client: ...     pass</p>"},{"location":"api/client/#py_gdelt.client.GDELTClient--with-custom-settings","title":"With custom settings","text":"<p>settings = GDELTSettings(timeout=60, max_retries=5) async with GDELTClient(settings=settings) as client: ...     pass</p>"},{"location":"api/client/#py_gdelt.client.GDELTClient--with-dependency-injection-for-testing","title":"With dependency injection for testing","text":"<p>async with httpx.AsyncClient() as http_client: ...     async with GDELTClient(http_client=http_client) as client: ...         pass</p>"},{"location":"api/client/#py_gdelt.client.GDELTClient.events","title":"<code>events</code>  <code>cached</code> <code>property</code>","text":"<p>Access the Events endpoint.</p> <p>Provides methods for querying GDELT Events data from files or BigQuery.</p> <p>Returns:</p> Type Description <code>EventsEndpoint</code> <p>EventsEndpoint instance.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If client not initialized (use context manager).</p> Example <p>async with GDELTClient() as client: ...     filter_obj = EventFilter(date_range=DateRange(start=date(2024, 1, 1))) ...     events = await client.events.query(filter_obj)</p>"},{"location":"api/client/#py_gdelt.client.GDELTClient.mentions","title":"<code>mentions</code>  <code>cached</code> <code>property</code>","text":"<p>Access the Mentions endpoint.</p> <p>Provides methods for querying GDELT Mentions data from files or BigQuery.</p> <p>Returns:</p> Type Description <code>MentionsEndpoint</code> <p>MentionsEndpoint instance.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If client not initialized (use context manager).</p> Example <p>async with GDELTClient() as client: ...     filter_obj = EventFilter(date_range=DateRange(start=date(2024, 1, 1))) ...     mentions = await client.mentions.query(\"123456789\", filter_obj)</p>"},{"location":"api/client/#py_gdelt.client.GDELTClient.gkg","title":"<code>gkg</code>  <code>cached</code> <code>property</code>","text":"<p>Access the GKG (Global Knowledge Graph) endpoint.</p> <p>Provides methods for querying GDELT GKG data from files or BigQuery.</p> <p>Returns:</p> Type Description <code>GKGEndpoint</code> <p>GKGEndpoint instance.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If client not initialized (use context manager).</p> Example <p>async with GDELTClient() as client: ...     filter_obj = GKGFilter( ...         date_range=DateRange(start=date(2024, 1, 1)), ...         themes=[\"ENV_CLIMATECHANGE\"] ...     ) ...     records = await client.gkg.query(filter_obj)</p>"},{"location":"api/client/#py_gdelt.client.GDELTClient.ngrams","title":"<code>ngrams</code>  <code>cached</code> <code>property</code>","text":"<p>Access the NGrams endpoint.</p> <p>Provides methods for querying GDELT NGrams data (files only).</p> <p>Returns:</p> Type Description <code>NGramsEndpoint</code> <p>NGramsEndpoint instance.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If client not initialized (use context manager).</p> Example <p>async with GDELTClient() as client: ...     filter_obj = NGramsFilter( ...         date_range=DateRange(start=date(2024, 1, 1)), ...         language=\"en\" ...     ) ...     records = await client.ngrams.query(filter_obj)</p>"},{"location":"api/client/#py_gdelt.client.GDELTClient.tv_ngrams","title":"<code>tv_ngrams</code>  <code>cached</code> <code>property</code>","text":"<p>Access the TV NGrams endpoint.</p> <p>Provides methods for querying word frequency from TV broadcast closed captions.</p> <p>Returns:</p> Type Description <code>TVNGramsEndpoint</code> <p>TVNGramsEndpoint instance.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If client not initialized (use context manager).</p> Example <p>async with GDELTClient() as client: ...     filter_obj = BroadcastNGramsFilter( ...         date_range=DateRange(start=date(2024, 1, 1)), ...         station=\"CNN\" ...     ) ...     records = await client.tv_ngrams.query(filter_obj)</p>"},{"location":"api/client/#py_gdelt.client.GDELTClient.radio_ngrams","title":"<code>radio_ngrams</code>  <code>cached</code> <code>property</code>","text":"<p>Access the Radio NGrams endpoint.</p> <p>Provides methods for querying word frequency from radio broadcast transcripts.</p> <p>Returns:</p> Type Description <code>RadioNGramsEndpoint</code> <p>RadioNGramsEndpoint instance.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If client not initialized (use context manager).</p> Example <p>async with GDELTClient() as client: ...     filter_obj = BroadcastNGramsFilter( ...         date_range=DateRange(start=date(2024, 1, 1)), ...         station=\"NPR\" ...     ) ...     records = await client.radio_ngrams.query(filter_obj)</p>"},{"location":"api/client/#py_gdelt.client.GDELTClient.vgkg","title":"<code>vgkg</code>  <code>cached</code> <code>property</code>","text":"<p>Access the VGKG (Visual Global Knowledge Graph) endpoint.</p> <p>Provides methods for querying Google Cloud Vision API analysis of news images.</p> <p>Returns:</p> Type Description <code>VGKGEndpoint</code> <p>VGKGEndpoint instance.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If client not initialized (use context manager).</p> Example <p>async with GDELTClient() as client: ...     filter_obj = VGKGFilter( ...         date_range=DateRange(start=date(2024, 1, 1)), ...         domain=\"cnn.com\" ...     ) ...     records = await client.vgkg.query(filter_obj)</p>"},{"location":"api/client/#py_gdelt.client.GDELTClient.tv_gkg","title":"<code>tv_gkg</code>  <code>cached</code> <code>property</code>","text":"<p>Access the TV GKG (TV Global Knowledge Graph) endpoint.</p> <p>Provides methods for querying GKG data from TV broadcast closed captions.</p> <p>Returns:</p> Type Description <code>TVGKGEndpoint</code> <p>TVGKGEndpoint instance.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If client not initialized (use context manager).</p> Example <p>async with GDELTClient() as client: ...     filter_obj = TVGKGFilter( ...         date_range=DateRange(start=date(2024, 1, 1)), ...         station=\"CNN\" ...     ) ...     records = await client.tv_gkg.query(filter_obj)</p>"},{"location":"api/client/#py_gdelt.client.GDELTClient.graphs","title":"<code>graphs</code>  <code>cached</code> <code>property</code>","text":"<p>Access the Graph datasets endpoint.</p> <p>Provides methods for querying GDELT Graph datasets (GQG, GEG, GFG, GGG, GEMG, GAL) from file downloads.</p> <p>Returns:</p> Type Description <code>GraphEndpoint</code> <p>GraphEndpoint instance.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If client not initialized (use context manager).</p> Example <p>async with GDELTClient() as client: ...     from py_gdelt.filters import GQGFilter, DateRange ...     filter_obj = GQGFilter( ...         date_range=DateRange(start=date(2025, 1, 20)) ...     ) ...     result = await client.graphs.query_gqg(filter_obj) ...     for record in result: ...         print(record.quotes)</p>"},{"location":"api/client/#py_gdelt.client.GDELTClient.doc","title":"<code>doc</code>  <code>cached</code> <code>property</code>","text":"<p>Access the DOC 2.0 API endpoint.</p> <p>Provides methods for searching GDELT articles via the DOC API.</p> <p>Returns:</p> Type Description <code>DocEndpoint</code> <p>DocEndpoint instance.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If client not initialized (use context manager).</p> Example <p>async with GDELTClient() as client: ...     articles = await client.doc.search(\"climate change\", max_results=100)</p>"},{"location":"api/client/#py_gdelt.client.GDELTClient.geo","title":"<code>geo</code>  <code>cached</code> <code>property</code>","text":"<p>Access the GEO 2.0 API endpoint.</p> <p>Provides methods for querying geographic locations from news articles.</p> <p>Returns:</p> Type Description <code>GeoEndpoint</code> <p>GeoEndpoint instance.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If client not initialized (use context manager).</p> Example <p>async with GDELTClient() as client: ...     result = await client.geo.search(\"earthquake\", max_points=100)</p>"},{"location":"api/client/#py_gdelt.client.GDELTClient.context","title":"<code>context</code>  <code>cached</code> <code>property</code>","text":"<p>Access the Context 2.0 API endpoint.</p> <p>Provides methods for contextual analysis of search terms.</p> <p>Returns:</p> Type Description <code>ContextEndpoint</code> <p>ContextEndpoint instance.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If client not initialized (use context manager).</p> Example <p>async with GDELTClient() as client: ...     result = await client.context.analyze(\"climate change\")</p>"},{"location":"api/client/#py_gdelt.client.GDELTClient.tv","title":"<code>tv</code>  <code>cached</code> <code>property</code>","text":"<p>Access the TV API endpoint.</p> <p>Provides methods for querying television news transcripts.</p> <p>Returns:</p> Type Description <code>TVEndpoint</code> <p>TVEndpoint instance.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If client not initialized (use context manager).</p> Example <p>async with GDELTClient() as client: ...     clips = await client.tv.search(\"climate change\", station=\"CNN\")</p>"},{"location":"api/client/#py_gdelt.client.GDELTClient.tv_ai","title":"<code>tv_ai</code>  <code>cached</code> <code>property</code>","text":"<p>Access the TVAI API endpoint.</p> <p>Provides methods for AI-enhanced television news analysis.</p> <p>Returns:</p> Type Description <code>TVAIEndpoint</code> <p>TVAIEndpoint instance.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If client not initialized (use context manager).</p> Example <p>async with GDELTClient() as client: ...     result = await client.tv_ai.analyze(\"election coverage\")</p>"},{"location":"api/client/#py_gdelt.client.GDELTClient.lowerthird","title":"<code>lowerthird</code>  <code>cached</code> <code>property</code>","text":"<p>Access the LowerThird (Chyron) API.</p> <p>Provides methods for searching OCR'd TV chyrons (lower-third text overlays).</p> <p>Returns:</p> Type Description <code>LowerThirdEndpoint</code> <p>LowerThirdEndpoint for searching TV chyrons.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If client not initialized (use context manager).</p> Example <p>async with GDELTClient() as client: ...     clips = await client.lowerthird.search(\"breaking news\")</p>"},{"location":"api/client/#py_gdelt.client.GDELTClient.tvv","title":"<code>tvv</code>  <code>cached</code> <code>property</code>","text":"<p>Access the TV Visual (TVV) API for channel inventory.</p> <p>Provides methods for retrieving TV channel metadata.</p> <p>Returns:</p> Type Description <code>TVVEndpoint</code> <p>TVVEndpoint for channel metadata.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If client not initialized (use context manager).</p> Example <p>async with GDELTClient() as client: ...     channels = await client.tvv.get_inventory()</p>"},{"location":"api/client/#py_gdelt.client.GDELTClient.gkg_geojson","title":"<code>gkg_geojson</code>  <code>cached</code> <code>property</code>","text":"<p>Access the GKG GeoJSON API (v1.0 Legacy).</p> <p>Provides methods for querying geographic GKG data as GeoJSON.</p> <p>Returns:</p> Type Description <code>GKGGeoJSONEndpoint</code> <p>GKGGeoJSONEndpoint for geographic GKG queries.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If client not initialized (use context manager).</p> Example <p>async with GDELTClient() as client: ...     result = await client.gkg_geojson.search(\"TERROR\", timespan=60)</p>"},{"location":"api/client/#py_gdelt.client.GDELTClient.lookups","title":"<code>lookups</code>  <code>cached</code> <code>property</code>","text":"<p>Access lookup tables for CAMEO codes, themes, and countries.</p> <p>Provides access to all GDELT lookup tables with lazy loading.</p> <p>Returns:</p> Type Description <code>Lookups</code> <p>Lookups instance for code/theme/country lookups.</p> Example <p>async with GDELTClient() as client: ...     # CAMEO codes ...     event_entry = client.lookups.cameo[\"14\"] ...     event_name = event_entry.name  # \"PROTEST\" ... ...     # GKG themes ...     category = client.lookups.themes.get_category(\"ENV_CLIMATECHANGE\") ... ...     # Country codes ...     iso_code = client.lookups.countries.fips_to_iso3(\"US\")  # \"USA\"</p>"},{"location":"api/client/#py_gdelt.client.GDELTClient.__aenter__","title":"<code>__aenter__()</code>  <code>async</code>","text":"<p>Async context manager entry.</p> <p>Returns:</p> Type Description <code>GDELTClient</code> <p>Self for use in async with statement.</p> Example <p>async with GDELTClient() as client: ...     events = await client.events.query(filter_obj)</p> Source code in <code>src/py_gdelt/client.py</code> <pre><code>async def __aenter__(self) -&gt; GDELTClient:\n    \"\"\"Async context manager entry.\n\n    Returns:\n        Self for use in async with statement.\n\n    Example:\n        &gt;&gt;&gt; async with GDELTClient() as client:\n        ...     events = await client.events.query(filter_obj)\n    \"\"\"\n    await self._initialize()\n    return self\n</code></pre>"},{"location":"api/client/#py_gdelt.client.GDELTClient.__aexit__","title":"<code>__aexit__(*args)</code>  <code>async</code>","text":"<p>Async context manager exit.</p> <p>Cleans up all owned resources.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Exception info (unused, but required by protocol).</p> <code>()</code> Source code in <code>src/py_gdelt/client.py</code> <pre><code>async def __aexit__(self, *args: Any) -&gt; None:\n    \"\"\"Async context manager exit.\n\n    Cleans up all owned resources.\n\n    Args:\n        *args: Exception info (unused, but required by protocol).\n    \"\"\"\n    await self._cleanup()\n</code></pre>"},{"location":"api/client/#py_gdelt.client.GDELTClient.__enter__","title":"<code>__enter__()</code>","text":"<p>Sync context manager entry.</p> <p>This provides synchronous (blocking) access to the client for use in non-async code. It uses asyncio.run() internally to manage the event loop.</p> Important Limitations <ul> <li>MUST be called from outside any existing async context/event loop.   Calling from within an async function will raise RuntimeError.</li> <li>Creates a new event loop for each context manager entry.</li> <li>Use the async context manager (async with) when possible for   better performance and compatibility.</li> </ul> <p>Returns:</p> Type Description <code>GDELTClient</code> <p>Self for use in with statement.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called from within an already running event loop.</p> Example Source code in <code>src/py_gdelt/client.py</code> <pre><code>def __enter__(self) -&gt; GDELTClient:\n    \"\"\"Sync context manager entry.\n\n    This provides synchronous (blocking) access to the client for use in\n    non-async code. It uses asyncio.run() internally to manage the event loop.\n\n    Important Limitations:\n        - MUST be called from outside any existing async context/event loop.\n          Calling from within an async function will raise RuntimeError.\n        - Creates a new event loop for each context manager entry.\n        - Use the async context manager (async with) when possible for\n          better performance and compatibility.\n\n    Returns:\n        Self for use in with statement.\n\n    Raises:\n        RuntimeError: If called from within an already running event loop.\n\n    Example:\n        &gt;&gt;&gt; # Correct: Used from synchronous code\n        &gt;&gt;&gt; with GDELTClient() as client:\n        ...     events = client.events.query_sync(filter_obj)\n        ...\n        &gt;&gt;&gt; # Wrong: Don't use from async code - use 'async with' instead\n        &gt;&gt;&gt; async def bad_example():\n        ...     with GDELTClient() as client:  # RuntimeError!\n        ...         pass\n    \"\"\"\n    asyncio.run(self._initialize())\n    return self\n</code></pre>"},{"location":"api/client/#py_gdelt.client.GDELTClient.__enter__--correct-used-from-synchronous-code","title":"Correct: Used from synchronous code","text":"<p>with GDELTClient() as client: ...     events = client.events.query_sync(filter_obj) ...</p>"},{"location":"api/client/#py_gdelt.client.GDELTClient.__enter__--wrong-dont-use-from-async-code-use-async-with-instead","title":"Wrong: Don't use from async code - use 'async with' instead","text":"<p>async def bad_example(): ...     with GDELTClient() as client:  # RuntimeError! ...         pass</p>"},{"location":"api/client/#py_gdelt.client.GDELTClient.__exit__","title":"<code>__exit__(*args)</code>","text":"<p>Sync context manager exit.</p> <p>Cleans up all owned resources. Uses asyncio.run() internally.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Exception info (unused, but required by protocol).</p> <code>()</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called from within an already running event loop.</p> Source code in <code>src/py_gdelt/client.py</code> <pre><code>def __exit__(self, *args: Any) -&gt; None:\n    \"\"\"Sync context manager exit.\n\n    Cleans up all owned resources. Uses asyncio.run() internally.\n\n    Args:\n        *args: Exception info (unused, but required by protocol).\n\n    Raises:\n        RuntimeError: If called from within an already running event loop.\n    \"\"\"\n    asyncio.run(self._cleanup())\n</code></pre>"},{"location":"api/client/#py_gdelt.config.GDELTSettings","title":"<code>GDELTSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration settings for the GDELT client library.</p> <p>Settings can be configured via: - Environment variables with GDELT_ prefix (e.g., GDELT_TIMEOUT=60) - TOML configuration file passed to config_path parameter - Default values</p> <p>Environment variables take precedence over TOML configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>Path | None</code> <p>Optional path to TOML configuration file. If provided and exists, settings will be loaded from it. Environment variables will override TOML settings.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for setting field values.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>model_config</code> <p>Pydantic settings configuration (env prefix, case sensitivity)</p> <code>bigquery_project</code> <code>str | None</code> <p>Google Cloud project ID for BigQuery access</p> <code>bigquery_credentials</code> <code>str | None</code> <p>Path to Google Cloud credentials JSON file</p> <code>cache_dir</code> <code>Path</code> <p>Directory for caching downloaded GDELT data</p> <code>cache_ttl</code> <code>int</code> <p>Cache time-to-live in seconds</p> <code>master_file_list_ttl</code> <code>int</code> <p>Master file list cache TTL in seconds</p> <code>max_retries</code> <code>int</code> <p>Maximum number of HTTP request retries</p> <code>timeout</code> <code>int</code> <p>HTTP request timeout in seconds</p> <code>max_concurrent_requests</code> <code>int</code> <p>Maximum concurrent HTTP requests</p> <code>max_concurrent_downloads</code> <code>int</code> <p>Maximum concurrent file downloads</p> <code>fallback_to_bigquery</code> <code>bool</code> <p>Whether to fallback to BigQuery when APIs fail</p> <code>validate_codes</code> <code>bool</code> <p>Whether to validate CAMEO/country codes</p> Example Source code in <code>src/py_gdelt/config.py</code> <pre><code>class GDELTSettings(BaseSettings):\n    \"\"\"Configuration settings for the GDELT client library.\n\n    Settings can be configured via:\n    - Environment variables with GDELT_ prefix (e.g., GDELT_TIMEOUT=60)\n    - TOML configuration file passed to config_path parameter\n    - Default values\n\n    Environment variables take precedence over TOML configuration.\n\n    Args:\n        config_path: Optional path to TOML configuration file.\n            If provided and exists, settings will be loaded from it.\n            Environment variables will override TOML settings.\n        **kwargs: Additional keyword arguments for setting field values.\n\n    Attributes:\n        model_config: Pydantic settings configuration (env prefix, case sensitivity)\n        bigquery_project: Google Cloud project ID for BigQuery access\n        bigquery_credentials: Path to Google Cloud credentials JSON file\n        cache_dir: Directory for caching downloaded GDELT data\n        cache_ttl: Cache time-to-live in seconds\n        master_file_list_ttl: Master file list cache TTL in seconds\n        max_retries: Maximum number of HTTP request retries\n        timeout: HTTP request timeout in seconds\n        max_concurrent_requests: Maximum concurrent HTTP requests\n        max_concurrent_downloads: Maximum concurrent file downloads\n        fallback_to_bigquery: Whether to fallback to BigQuery when APIs fail\n        validate_codes: Whether to validate CAMEO/country codes\n\n    Example:\n        &gt;&gt;&gt; # Using defaults\n        &gt;&gt;&gt; settings = GDELTSettings()\n\n        &gt;&gt;&gt; # Loading from TOML file\n        &gt;&gt;&gt; settings = GDELTSettings(config_path=Path(\"gdelt.toml\"))\n\n        &gt;&gt;&gt; # Environment variables override TOML\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; os.environ[\"GDELT_TIMEOUT\"] = \"60\"\n        &gt;&gt;&gt; settings = GDELTSettings()\n        &gt;&gt;&gt; settings.timeout\n        60\n    \"\"\"\n\n    model_config = SettingsConfigDict(\n        env_prefix=\"GDELT_\",\n        case_sensitive=False,\n        extra=\"ignore\",\n    )\n\n    # BigQuery settings (optional)\n    bigquery_project: str | None = Field(\n        default=None,\n        description=\"Google Cloud project ID for BigQuery access\",\n    )\n    bigquery_credentials: str | None = Field(\n        default=None,\n        description=\"Path to Google Cloud credentials JSON file\",\n    )\n\n    # Cache settings\n    cache_dir: Path = Field(\n        default_factory=lambda: Path.home() / \".cache\" / \"gdelt\",\n        description=\"Directory for caching downloaded GDELT data\",\n    )\n    cache_ttl: int = Field(\n        default=3600,\n        description=\"Cache time-to-live in seconds\",\n    )\n    master_file_list_ttl: int = Field(\n        default=300,\n        description=\"Master file list cache TTL in seconds (default 5 minutes)\",\n    )\n\n    # HTTP settings\n    max_retries: int = Field(\n        default=3,\n        description=\"Maximum number of HTTP request retries\",\n    )\n    timeout: int = Field(\n        default=30,\n        description=\"HTTP request timeout in seconds\",\n    )\n    max_concurrent_requests: int = Field(\n        default=10,\n        description=\"Maximum concurrent HTTP requests\",\n    )\n    max_concurrent_downloads: int = Field(\n        default=10,\n        description=\"Maximum concurrent file downloads\",\n    )\n\n    # Behavior settings\n    fallback_to_bigquery: bool = Field(\n        default=True,\n        description=\"Whether to fallback to BigQuery when APIs fail\",\n    )\n    validate_codes: bool = Field(\n        default=True,\n        description=\"Whether to validate CAMEO/country codes\",\n    )\n\n    # Class variable to store config_path during initialization\n    _current_config_path: Path | None = None\n\n    def __init__(self, config_path: Path | None = None, **kwargs: Any) -&gt; None:\n        # Store config_path temporarily on class for settings_customise_sources\n        GDELTSettings._current_config_path = config_path\n        try:\n            # Initialize the parent BaseSettings\n            super().__init__(**kwargs)\n        finally:\n            # Clean up class variable\n            GDELTSettings._current_config_path = None\n\n    @classmethod\n    def settings_customise_sources(\n        cls,\n        settings_cls: type[BaseSettings],\n        init_settings: PydanticBaseSettingsSource,\n        env_settings: PydanticBaseSettingsSource,\n        dotenv_settings: PydanticBaseSettingsSource,  # noqa: ARG003\n        file_secret_settings: PydanticBaseSettingsSource,  # noqa: ARG003\n        **_kwargs: Any,\n    ) -&gt; tuple[PydanticBaseSettingsSource, ...]:\n        \"\"\"Customize settings sources to include TOML configuration.\n\n        The order of sources determines precedence (first source wins):\n        1. Init settings (kwargs passed to __init__)\n        2. Environment variables (GDELT_ prefix)\n        3. TOML configuration file\n        4. Default values\n\n        Args:\n            settings_cls: The settings class being customized.\n            init_settings: Settings from __init__ kwargs.\n            env_settings: Settings from environment variables.\n            dotenv_settings: Settings from .env file (unused).\n            file_secret_settings: Settings from secret files (unused).\n            **_kwargs: Additional keyword arguments (unused).\n\n        Returns:\n            Tuple of settings sources in priority order.\n        \"\"\"\n        # Get config_path from class variable set in __init__\n        config_path = cls._current_config_path\n        toml_source = TOMLConfigSource(settings_cls, config_path=config_path)\n\n        # Return sources in priority order (first wins)\n        return (\n            init_settings,  # Highest priority: explicit kwargs\n            env_settings,  # Environment variables\n            toml_source,  # TOML configuration\n            # Default values are handled by Pydantic automatically\n        )\n</code></pre>"},{"location":"api/client/#py_gdelt.config.GDELTSettings--using-defaults","title":"Using defaults","text":"<p>settings = GDELTSettings()</p>"},{"location":"api/client/#py_gdelt.config.GDELTSettings--loading-from-toml-file","title":"Loading from TOML file","text":"<p>settings = GDELTSettings(config_path=Path(\"gdelt.toml\"))</p>"},{"location":"api/client/#py_gdelt.config.GDELTSettings--environment-variables-override-toml","title":"Environment variables override TOML","text":"<p>import os os.environ[\"GDELT_TIMEOUT\"] = \"60\" settings = GDELTSettings() settings.timeout 60</p>"},{"location":"api/client/#py_gdelt.config.GDELTSettings.settings_customise_sources","title":"<code>settings_customise_sources(settings_cls, init_settings, env_settings, dotenv_settings, file_secret_settings, **_kwargs)</code>  <code>classmethod</code>","text":"<p>Customize settings sources to include TOML configuration.</p> <p>The order of sources determines precedence (first source wins): 1. Init settings (kwargs passed to init) 2. Environment variables (GDELT_ prefix) 3. TOML configuration file 4. Default values</p> <p>Parameters:</p> Name Type Description Default <code>settings_cls</code> <code>type[BaseSettings]</code> <p>The settings class being customized.</p> required <code>init_settings</code> <code>PydanticBaseSettingsSource</code> <p>Settings from init kwargs.</p> required <code>env_settings</code> <code>PydanticBaseSettingsSource</code> <p>Settings from environment variables.</p> required <code>dotenv_settings</code> <code>PydanticBaseSettingsSource</code> <p>Settings from .env file (unused).</p> required <code>file_secret_settings</code> <code>PydanticBaseSettingsSource</code> <p>Settings from secret files (unused).</p> required <code>**_kwargs</code> <code>Any</code> <p>Additional keyword arguments (unused).</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[PydanticBaseSettingsSource, ...]</code> <p>Tuple of settings sources in priority order.</p> Source code in <code>src/py_gdelt/config.py</code> <pre><code>@classmethod\ndef settings_customise_sources(\n    cls,\n    settings_cls: type[BaseSettings],\n    init_settings: PydanticBaseSettingsSource,\n    env_settings: PydanticBaseSettingsSource,\n    dotenv_settings: PydanticBaseSettingsSource,  # noqa: ARG003\n    file_secret_settings: PydanticBaseSettingsSource,  # noqa: ARG003\n    **_kwargs: Any,\n) -&gt; tuple[PydanticBaseSettingsSource, ...]:\n    \"\"\"Customize settings sources to include TOML configuration.\n\n    The order of sources determines precedence (first source wins):\n    1. Init settings (kwargs passed to __init__)\n    2. Environment variables (GDELT_ prefix)\n    3. TOML configuration file\n    4. Default values\n\n    Args:\n        settings_cls: The settings class being customized.\n        init_settings: Settings from __init__ kwargs.\n        env_settings: Settings from environment variables.\n        dotenv_settings: Settings from .env file (unused).\n        file_secret_settings: Settings from secret files (unused).\n        **_kwargs: Additional keyword arguments (unused).\n\n    Returns:\n        Tuple of settings sources in priority order.\n    \"\"\"\n    # Get config_path from class variable set in __init__\n    config_path = cls._current_config_path\n    toml_source = TOMLConfigSource(settings_cls, config_path=config_path)\n\n    # Return sources in priority order (first wins)\n    return (\n        init_settings,  # Highest priority: explicit kwargs\n        env_settings,  # Environment variables\n        toml_source,  # TOML configuration\n        # Default values are handled by Pydantic automatically\n    )\n</code></pre>"},{"location":"api/endpoints/","title":"Endpoints API","text":""},{"location":"api/endpoints/#file-based-endpoints","title":"File-Based Endpoints","text":""},{"location":"api/endpoints/#eventsendpoint","title":"EventsEndpoint","text":""},{"location":"api/endpoints/#py_gdelt.endpoints.events.EventsEndpoint","title":"<code>EventsEndpoint</code>","text":"<p>Endpoint for querying GDELT Events data.</p> <p>This endpoint orchestrates querying GDELT Events data from multiple sources (files and BigQuery) using a DataFetcher. It handles: - Source selection and fallback logic - Type conversion from internal _RawEvent to public Event models - Optional deduplication - Both streaming and batch query modes</p> <p>The endpoint uses dependency injection to receive source instances, making it easy to test and configure.</p> <p>Parameters:</p> Name Type Description Default <code>file_source</code> <code>FileSource</code> <p>FileSource instance for downloading GDELT files</p> required <code>bigquery_source</code> <code>BigQuerySource | None</code> <p>Optional BigQuerySource instance for fallback queries</p> <code>None</code> <code>fallback_enabled</code> <code>bool</code> <p>Whether to fallback to BigQuery on errors (default: True)</p> <code>True</code> Note <p>BigQuery fallback only activates if both fallback_enabled=True AND bigquery_source is provided AND credentials are configured.</p> Example <p>from py_gdelt.sources import FileSource from py_gdelt.filters import DateRange, EventFilter from datetime import date</p> <p>async with FileSource() as file_source: ...     endpoint = EventsEndpoint(file_source=file_source) ...     filter_obj = EventFilter( ...         date_range=DateRange(start=date(2024, 1, 1)), ...         actor1_country=\"USA\", ...     ) ...     # Batch query ...     result = await endpoint.query(filter_obj, deduplicate=True) ...     for event in result: ...         print(event.global_event_id) ...     # Streaming query ...     async for event in endpoint.stream(filter_obj): ...         process(event)</p> Source code in <code>src/py_gdelt/endpoints/events.py</code> <pre><code>class EventsEndpoint:\n    \"\"\"Endpoint for querying GDELT Events data.\n\n    This endpoint orchestrates querying GDELT Events data from multiple sources\n    (files and BigQuery) using a DataFetcher. It handles:\n    - Source selection and fallback logic\n    - Type conversion from internal _RawEvent to public Event models\n    - Optional deduplication\n    - Both streaming and batch query modes\n\n    The endpoint uses dependency injection to receive source instances, making\n    it easy to test and configure.\n\n    Args:\n        file_source: FileSource instance for downloading GDELT files\n        bigquery_source: Optional BigQuerySource instance for fallback queries\n        fallback_enabled: Whether to fallback to BigQuery on errors (default: True)\n\n    Note:\n        BigQuery fallback only activates if both fallback_enabled=True AND\n        bigquery_source is provided AND credentials are configured.\n\n    Example:\n        &gt;&gt;&gt; from py_gdelt.sources import FileSource\n        &gt;&gt;&gt; from py_gdelt.filters import DateRange, EventFilter\n        &gt;&gt;&gt; from datetime import date\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; async with FileSource() as file_source:\n        ...     endpoint = EventsEndpoint(file_source=file_source)\n        ...     filter_obj = EventFilter(\n        ...         date_range=DateRange(start=date(2024, 1, 1)),\n        ...         actor1_country=\"USA\",\n        ...     )\n        ...     # Batch query\n        ...     result = await endpoint.query(filter_obj, deduplicate=True)\n        ...     for event in result:\n        ...         print(event.global_event_id)\n        ...     # Streaming query\n        ...     async for event in endpoint.stream(filter_obj):\n        ...         process(event)\n    \"\"\"\n\n    def __init__(\n        self,\n        file_source: FileSource,\n        bigquery_source: BigQuerySource | None = None,\n        *,\n        fallback_enabled: bool = True,\n    ) -&gt; None:\n        # Import DataFetcher here to avoid circular imports\n        from py_gdelt.sources.fetcher import DataFetcher\n\n        self._fetcher: DataFetcher = DataFetcher(\n            file_source=file_source,\n            bigquery_source=bigquery_source,\n            fallback_enabled=fallback_enabled,\n        )\n\n        logger.debug(\n            \"EventsEndpoint initialized (fallback_enabled=%s)\",\n            fallback_enabled,\n        )\n\n    async def query(\n        self,\n        filter_obj: EventFilter,\n        *,\n        deduplicate: bool = False,\n        dedupe_strategy: DedupeStrategy | None = None,\n        use_bigquery: bool = False,\n    ) -&gt; FetchResult[Event]:\n        \"\"\"Query GDELT Events with automatic fallback.\n\n        This is a batch query method that materializes all results into memory.\n        For large datasets, prefer stream() for memory-efficient iteration.\n\n        Files are always tried first (free, no credentials), with automatic fallback\n        to BigQuery on rate limit/error if credentials are configured.\n\n        Args:\n            filter_obj: Event filter with date range and query parameters\n            deduplicate: If True, deduplicate events based on dedupe_strategy\n            dedupe_strategy: Deduplication strategy (default: URL_DATE_LOCATION)\n            use_bigquery: If True, skip files and use BigQuery directly\n\n        Returns:\n            FetchResult containing Event instances. Use .data to access the list,\n            .failed to see any failed requests, and .complete to check if all\n            requests succeeded.\n\n        Raises:\n            RateLimitError: If rate limited and fallback not available\n            APIError: If download fails and fallback not available\n            ConfigurationError: If BigQuery requested but not configured\n\n        Example:\n            &gt;&gt;&gt; filter_obj = EventFilter(\n            ...     date_range=DateRange(start=date(2024, 1, 1)),\n            ...     actor1_country=\"USA\",\n            ... )\n            &gt;&gt;&gt; result = await endpoint.query(filter_obj, deduplicate=True)\n            &gt;&gt;&gt; print(f\"Found {len(result)} unique events\")\n            &gt;&gt;&gt; for event in result:\n            ...     print(event.global_event_id)\n        \"\"\"\n        # Default dedupe strategy\n        if deduplicate and dedupe_strategy is None:\n            dedupe_strategy = DedupeStrategy.URL_DATE_LOCATION\n\n        # Fetch raw events first (for deduplication)\n        raw_events_list: list[_RawEvent] = [\n            raw_event\n            async for raw_event in self._fetcher.fetch_events(\n                filter_obj,\n                use_bigquery=use_bigquery,\n            )\n        ]\n\n        logger.info(\"Fetched %d raw events from sources\", len(raw_events_list))\n\n        # Apply deduplication on raw events if requested\n        # Deduplication happens on _RawEvent which implements HasDedupeFields protocol\n        if deduplicate and dedupe_strategy is not None:\n            original_count = len(raw_events_list)\n            # Convert to iterator, deduplicate, then back to list\n            raw_events_list = list(apply_dedup(iter(raw_events_list), dedupe_strategy))\n            logger.info(\n                \"Deduplicated %d events to %d unique (strategy=%s)\",\n                original_count,\n                len(raw_events_list),\n                dedupe_strategy,\n            )\n\n        # Convert _RawEvent to Event models after deduplication\n        events: list[Event] = []\n        for raw_event in raw_events_list:\n            event = Event.from_raw(raw_event)\n            events.append(event)\n\n        logger.info(\"Converted %d events to Event models\", len(events))\n\n        # Return as FetchResult (no failed requests tracked yet)\n        return FetchResult(data=events)\n\n    async def stream(\n        self,\n        filter_obj: EventFilter,\n        *,\n        deduplicate: bool = False,\n        dedupe_strategy: DedupeStrategy | None = None,\n        use_bigquery: bool = False,\n    ) -&gt; AsyncIterator[Event]:\n        \"\"\"Stream GDELT Events with memory-efficient iteration.\n\n        This is a streaming method that yields events one at a time, making it\n        suitable for large datasets. Memory usage is constant regardless of\n        result size.\n\n        Files are always tried first (free, no credentials), with automatic fallback\n        to BigQuery on rate limit/error if credentials are configured.\n\n        Args:\n            filter_obj: Event filter with date range and query parameters\n            deduplicate: If True, deduplicate events based on dedupe_strategy\n            dedupe_strategy: Deduplication strategy (default: URL_DATE_LOCATION)\n            use_bigquery: If True, skip files and use BigQuery directly\n\n        Yields:\n            Event: Individual Event instances matching the filter\n\n        Raises:\n            RateLimitError: If rate limited and fallback not available\n            APIError: If download fails and fallback not available\n            ConfigurationError: If BigQuery requested but not configured\n\n        Example:\n            &gt;&gt;&gt; filter_obj = EventFilter(\n            ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7)),\n            ...     actor1_country=\"USA\",\n            ... )\n            &gt;&gt;&gt; count = 0\n            &gt;&gt;&gt; async for event in endpoint.stream(filter_obj, deduplicate=True):\n            ...     print(event.global_event_id)\n            ...     count += 1\n            &gt;&gt;&gt; print(f\"Streamed {count} unique events\")\n        \"\"\"\n        # Default dedupe strategy\n        if deduplicate and dedupe_strategy is None:\n            dedupe_strategy = DedupeStrategy.URL_DATE_LOCATION\n\n        # Fetch raw events from DataFetcher\n        raw_events = self._fetcher.fetch_events(\n            filter_obj,\n            use_bigquery=use_bigquery,\n        )\n\n        # Apply deduplication if requested\n        if deduplicate and dedupe_strategy is not None:\n            logger.debug(\"Applying deduplication (strategy=%s)\", dedupe_strategy)\n            raw_events = apply_dedup_async(raw_events, dedupe_strategy)\n\n        # Convert _RawEvent to Event at yield boundary\n        count = 0\n        async for raw_event in raw_events:\n            event = Event.from_raw(raw_event)\n            yield event\n            count += 1\n\n        logger.info(\"Streamed %d events\", count)\n\n    def query_sync(\n        self,\n        filter_obj: EventFilter,\n        *,\n        deduplicate: bool = False,\n        dedupe_strategy: DedupeStrategy | None = None,\n        use_bigquery: bool = False,\n    ) -&gt; FetchResult[Event]:\n        \"\"\"Synchronous wrapper for query().\n\n        This is a convenience method that runs the async query() method\n        in a new event loop. Prefer using the async version when possible.\n\n        Args:\n            filter_obj: Event filter with date range and query parameters\n            deduplicate: If True, deduplicate events based on dedupe_strategy\n            dedupe_strategy: Deduplication strategy (default: URL_DATE_LOCATION)\n            use_bigquery: If True, skip files and use BigQuery directly\n\n        Returns:\n            FetchResult containing Event instances\n\n        Raises:\n            RateLimitError: If rate limited and fallback not available\n            APIError: If download fails and fallback not available\n            ConfigurationError: If BigQuery requested but not configured\n            RuntimeError: If called from within an already running event loop\n\n        Example:\n            &gt;&gt;&gt; filter_obj = EventFilter(\n            ...     date_range=DateRange(start=date(2024, 1, 1)),\n            ...     actor1_country=\"USA\",\n            ... )\n            &gt;&gt;&gt; result = endpoint.query_sync(filter_obj)\n            &gt;&gt;&gt; for event in result:\n            ...     print(event.global_event_id)\n        \"\"\"\n        return asyncio.run(\n            self.query(\n                filter_obj,\n                deduplicate=deduplicate,\n                dedupe_strategy=dedupe_strategy,\n                use_bigquery=use_bigquery,\n            ),\n        )\n\n    def stream_sync(\n        self,\n        filter_obj: EventFilter,\n        *,\n        deduplicate: bool = False,\n        dedupe_strategy: DedupeStrategy | None = None,\n        use_bigquery: bool = False,\n    ) -&gt; Iterator[Event]:\n        \"\"\"Synchronous wrapper for stream().\n\n        This method provides a synchronous iterator interface over async streaming.\n        It internally manages the event loop and yields events one at a time,\n        providing true streaming behavior with memory efficiency.\n\n        Note: This creates a new event loop for each iteration, which has some overhead.\n        For better performance, use the async stream() method directly if possible.\n\n        Args:\n            filter_obj: Event filter with date range and query parameters\n            deduplicate: If True, deduplicate events based on dedupe_strategy\n            dedupe_strategy: Deduplication strategy (default: URL_DATE_LOCATION)\n            use_bigquery: If True, skip files and use BigQuery directly\n\n        Returns:\n            Iterator that yields Event instances for each matching event\n\n        Raises:\n            RateLimitError: If rate limited and fallback not available\n            APIError: If download fails and fallback not available\n            ConfigurationError: If BigQuery requested but not configured\n            RuntimeError: If called from within an already running event loop\n\n        Example:\n            &gt;&gt;&gt; filter_obj = EventFilter(\n            ...     date_range=DateRange(start=date(2024, 1, 1)),\n            ...     actor1_country=\"USA\",\n            ... )\n            &gt;&gt;&gt; for event in endpoint.stream_sync(filter_obj, deduplicate=True):\n            ...     print(event.global_event_id)\n        \"\"\"\n\n        async def _async_generator() -&gt; AsyncIterator[Event]:\n            \"\"\"Internal async generator for sync wrapper.\"\"\"\n            async for event in self.stream(\n                filter_obj,\n                deduplicate=deduplicate,\n                dedupe_strategy=dedupe_strategy,\n                use_bigquery=use_bigquery,\n            ):\n                yield event\n\n        # Run async generator and yield results synchronously\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        try:\n            async_gen = _async_generator()\n            while True:\n                try:\n                    event = loop.run_until_complete(async_gen.__anext__())\n                    yield event\n                except StopAsyncIteration:\n                    break\n        finally:\n            loop.close()\n\n    async def _build_url(self, **kwargs: Any) -&gt; str:\n        \"\"\"Build URL for events endpoint.\n\n        Note: Events endpoint doesn't use URLs since it fetches from files/BigQuery.\n        This method is provided for compatibility with BaseEndpoint pattern but\n        is not used in practice.\n\n        Args:\n            **kwargs: Unused, but kept for interface consistency.\n\n        Returns:\n            Empty string (not used for file/BigQuery sources).\n        \"\"\"\n        return \"\"\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.events.EventsEndpoint.query","title":"<code>query(filter_obj, *, deduplicate=False, dedupe_strategy=None, use_bigquery=False)</code>  <code>async</code>","text":"<p>Query GDELT Events with automatic fallback.</p> <p>This is a batch query method that materializes all results into memory. For large datasets, prefer stream() for memory-efficient iteration.</p> <p>Files are always tried first (free, no credentials), with automatic fallback to BigQuery on rate limit/error if credentials are configured.</p> <p>Parameters:</p> Name Type Description Default <code>filter_obj</code> <code>EventFilter</code> <p>Event filter with date range and query parameters</p> required <code>deduplicate</code> <code>bool</code> <p>If True, deduplicate events based on dedupe_strategy</p> <code>False</code> <code>dedupe_strategy</code> <code>DedupeStrategy | None</code> <p>Deduplication strategy (default: URL_DATE_LOCATION)</p> <code>None</code> <code>use_bigquery</code> <code>bool</code> <p>If True, skip files and use BigQuery directly</p> <code>False</code> <p>Returns:</p> Type Description <code>FetchResult[Event]</code> <p>FetchResult containing Event instances. Use .data to access the list,</p> <code>FetchResult[Event]</code> <p>.failed to see any failed requests, and .complete to check if all</p> <code>FetchResult[Event]</code> <p>requests succeeded.</p> <p>Raises:</p> Type Description <code>RateLimitError</code> <p>If rate limited and fallback not available</p> <code>APIError</code> <p>If download fails and fallback not available</p> <code>ConfigurationError</code> <p>If BigQuery requested but not configured</p> Example <p>filter_obj = EventFilter( ...     date_range=DateRange(start=date(2024, 1, 1)), ...     actor1_country=\"USA\", ... ) result = await endpoint.query(filter_obj, deduplicate=True) print(f\"Found {len(result)} unique events\") for event in result: ...     print(event.global_event_id)</p> Source code in <code>src/py_gdelt/endpoints/events.py</code> <pre><code>async def query(\n    self,\n    filter_obj: EventFilter,\n    *,\n    deduplicate: bool = False,\n    dedupe_strategy: DedupeStrategy | None = None,\n    use_bigquery: bool = False,\n) -&gt; FetchResult[Event]:\n    \"\"\"Query GDELT Events with automatic fallback.\n\n    This is a batch query method that materializes all results into memory.\n    For large datasets, prefer stream() for memory-efficient iteration.\n\n    Files are always tried first (free, no credentials), with automatic fallback\n    to BigQuery on rate limit/error if credentials are configured.\n\n    Args:\n        filter_obj: Event filter with date range and query parameters\n        deduplicate: If True, deduplicate events based on dedupe_strategy\n        dedupe_strategy: Deduplication strategy (default: URL_DATE_LOCATION)\n        use_bigquery: If True, skip files and use BigQuery directly\n\n    Returns:\n        FetchResult containing Event instances. Use .data to access the list,\n        .failed to see any failed requests, and .complete to check if all\n        requests succeeded.\n\n    Raises:\n        RateLimitError: If rate limited and fallback not available\n        APIError: If download fails and fallback not available\n        ConfigurationError: If BigQuery requested but not configured\n\n    Example:\n        &gt;&gt;&gt; filter_obj = EventFilter(\n        ...     date_range=DateRange(start=date(2024, 1, 1)),\n        ...     actor1_country=\"USA\",\n        ... )\n        &gt;&gt;&gt; result = await endpoint.query(filter_obj, deduplicate=True)\n        &gt;&gt;&gt; print(f\"Found {len(result)} unique events\")\n        &gt;&gt;&gt; for event in result:\n        ...     print(event.global_event_id)\n    \"\"\"\n    # Default dedupe strategy\n    if deduplicate and dedupe_strategy is None:\n        dedupe_strategy = DedupeStrategy.URL_DATE_LOCATION\n\n    # Fetch raw events first (for deduplication)\n    raw_events_list: list[_RawEvent] = [\n        raw_event\n        async for raw_event in self._fetcher.fetch_events(\n            filter_obj,\n            use_bigquery=use_bigquery,\n        )\n    ]\n\n    logger.info(\"Fetched %d raw events from sources\", len(raw_events_list))\n\n    # Apply deduplication on raw events if requested\n    # Deduplication happens on _RawEvent which implements HasDedupeFields protocol\n    if deduplicate and dedupe_strategy is not None:\n        original_count = len(raw_events_list)\n        # Convert to iterator, deduplicate, then back to list\n        raw_events_list = list(apply_dedup(iter(raw_events_list), dedupe_strategy))\n        logger.info(\n            \"Deduplicated %d events to %d unique (strategy=%s)\",\n            original_count,\n            len(raw_events_list),\n            dedupe_strategy,\n        )\n\n    # Convert _RawEvent to Event models after deduplication\n    events: list[Event] = []\n    for raw_event in raw_events_list:\n        event = Event.from_raw(raw_event)\n        events.append(event)\n\n    logger.info(\"Converted %d events to Event models\", len(events))\n\n    # Return as FetchResult (no failed requests tracked yet)\n    return FetchResult(data=events)\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.events.EventsEndpoint.stream","title":"<code>stream(filter_obj, *, deduplicate=False, dedupe_strategy=None, use_bigquery=False)</code>  <code>async</code>","text":"<p>Stream GDELT Events with memory-efficient iteration.</p> <p>This is a streaming method that yields events one at a time, making it suitable for large datasets. Memory usage is constant regardless of result size.</p> <p>Files are always tried first (free, no credentials), with automatic fallback to BigQuery on rate limit/error if credentials are configured.</p> <p>Parameters:</p> Name Type Description Default <code>filter_obj</code> <code>EventFilter</code> <p>Event filter with date range and query parameters</p> required <code>deduplicate</code> <code>bool</code> <p>If True, deduplicate events based on dedupe_strategy</p> <code>False</code> <code>dedupe_strategy</code> <code>DedupeStrategy | None</code> <p>Deduplication strategy (default: URL_DATE_LOCATION)</p> <code>None</code> <code>use_bigquery</code> <code>bool</code> <p>If True, skip files and use BigQuery directly</p> <code>False</code> <p>Yields:</p> Name Type Description <code>Event</code> <code>AsyncIterator[Event]</code> <p>Individual Event instances matching the filter</p> <p>Raises:</p> Type Description <code>RateLimitError</code> <p>If rate limited and fallback not available</p> <code>APIError</code> <p>If download fails and fallback not available</p> <code>ConfigurationError</code> <p>If BigQuery requested but not configured</p> Example <p>filter_obj = EventFilter( ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7)), ...     actor1_country=\"USA\", ... ) count = 0 async for event in endpoint.stream(filter_obj, deduplicate=True): ...     print(event.global_event_id) ...     count += 1 print(f\"Streamed {count} unique events\")</p> Source code in <code>src/py_gdelt/endpoints/events.py</code> <pre><code>async def stream(\n    self,\n    filter_obj: EventFilter,\n    *,\n    deduplicate: bool = False,\n    dedupe_strategy: DedupeStrategy | None = None,\n    use_bigquery: bool = False,\n) -&gt; AsyncIterator[Event]:\n    \"\"\"Stream GDELT Events with memory-efficient iteration.\n\n    This is a streaming method that yields events one at a time, making it\n    suitable for large datasets. Memory usage is constant regardless of\n    result size.\n\n    Files are always tried first (free, no credentials), with automatic fallback\n    to BigQuery on rate limit/error if credentials are configured.\n\n    Args:\n        filter_obj: Event filter with date range and query parameters\n        deduplicate: If True, deduplicate events based on dedupe_strategy\n        dedupe_strategy: Deduplication strategy (default: URL_DATE_LOCATION)\n        use_bigquery: If True, skip files and use BigQuery directly\n\n    Yields:\n        Event: Individual Event instances matching the filter\n\n    Raises:\n        RateLimitError: If rate limited and fallback not available\n        APIError: If download fails and fallback not available\n        ConfigurationError: If BigQuery requested but not configured\n\n    Example:\n        &gt;&gt;&gt; filter_obj = EventFilter(\n        ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7)),\n        ...     actor1_country=\"USA\",\n        ... )\n        &gt;&gt;&gt; count = 0\n        &gt;&gt;&gt; async for event in endpoint.stream(filter_obj, deduplicate=True):\n        ...     print(event.global_event_id)\n        ...     count += 1\n        &gt;&gt;&gt; print(f\"Streamed {count} unique events\")\n    \"\"\"\n    # Default dedupe strategy\n    if deduplicate and dedupe_strategy is None:\n        dedupe_strategy = DedupeStrategy.URL_DATE_LOCATION\n\n    # Fetch raw events from DataFetcher\n    raw_events = self._fetcher.fetch_events(\n        filter_obj,\n        use_bigquery=use_bigquery,\n    )\n\n    # Apply deduplication if requested\n    if deduplicate and dedupe_strategy is not None:\n        logger.debug(\"Applying deduplication (strategy=%s)\", dedupe_strategy)\n        raw_events = apply_dedup_async(raw_events, dedupe_strategy)\n\n    # Convert _RawEvent to Event at yield boundary\n    count = 0\n    async for raw_event in raw_events:\n        event = Event.from_raw(raw_event)\n        yield event\n        count += 1\n\n    logger.info(\"Streamed %d events\", count)\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.events.EventsEndpoint.query_sync","title":"<code>query_sync(filter_obj, *, deduplicate=False, dedupe_strategy=None, use_bigquery=False)</code>","text":"<p>Synchronous wrapper for query().</p> <p>This is a convenience method that runs the async query() method in a new event loop. Prefer using the async version when possible.</p> <p>Parameters:</p> Name Type Description Default <code>filter_obj</code> <code>EventFilter</code> <p>Event filter with date range and query parameters</p> required <code>deduplicate</code> <code>bool</code> <p>If True, deduplicate events based on dedupe_strategy</p> <code>False</code> <code>dedupe_strategy</code> <code>DedupeStrategy | None</code> <p>Deduplication strategy (default: URL_DATE_LOCATION)</p> <code>None</code> <code>use_bigquery</code> <code>bool</code> <p>If True, skip files and use BigQuery directly</p> <code>False</code> <p>Returns:</p> Type Description <code>FetchResult[Event]</code> <p>FetchResult containing Event instances</p> <p>Raises:</p> Type Description <code>RateLimitError</code> <p>If rate limited and fallback not available</p> <code>APIError</code> <p>If download fails and fallback not available</p> <code>ConfigurationError</code> <p>If BigQuery requested but not configured</p> <code>RuntimeError</code> <p>If called from within an already running event loop</p> Example <p>filter_obj = EventFilter( ...     date_range=DateRange(start=date(2024, 1, 1)), ...     actor1_country=\"USA\", ... ) result = endpoint.query_sync(filter_obj) for event in result: ...     print(event.global_event_id)</p> Source code in <code>src/py_gdelt/endpoints/events.py</code> <pre><code>def query_sync(\n    self,\n    filter_obj: EventFilter,\n    *,\n    deduplicate: bool = False,\n    dedupe_strategy: DedupeStrategy | None = None,\n    use_bigquery: bool = False,\n) -&gt; FetchResult[Event]:\n    \"\"\"Synchronous wrapper for query().\n\n    This is a convenience method that runs the async query() method\n    in a new event loop. Prefer using the async version when possible.\n\n    Args:\n        filter_obj: Event filter with date range and query parameters\n        deduplicate: If True, deduplicate events based on dedupe_strategy\n        dedupe_strategy: Deduplication strategy (default: URL_DATE_LOCATION)\n        use_bigquery: If True, skip files and use BigQuery directly\n\n    Returns:\n        FetchResult containing Event instances\n\n    Raises:\n        RateLimitError: If rate limited and fallback not available\n        APIError: If download fails and fallback not available\n        ConfigurationError: If BigQuery requested but not configured\n        RuntimeError: If called from within an already running event loop\n\n    Example:\n        &gt;&gt;&gt; filter_obj = EventFilter(\n        ...     date_range=DateRange(start=date(2024, 1, 1)),\n        ...     actor1_country=\"USA\",\n        ... )\n        &gt;&gt;&gt; result = endpoint.query_sync(filter_obj)\n        &gt;&gt;&gt; for event in result:\n        ...     print(event.global_event_id)\n    \"\"\"\n    return asyncio.run(\n        self.query(\n            filter_obj,\n            deduplicate=deduplicate,\n            dedupe_strategy=dedupe_strategy,\n            use_bigquery=use_bigquery,\n        ),\n    )\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.events.EventsEndpoint.stream_sync","title":"<code>stream_sync(filter_obj, *, deduplicate=False, dedupe_strategy=None, use_bigquery=False)</code>","text":"<p>Synchronous wrapper for stream().</p> <p>This method provides a synchronous iterator interface over async streaming. It internally manages the event loop and yields events one at a time, providing true streaming behavior with memory efficiency.</p> <p>Note: This creates a new event loop for each iteration, which has some overhead. For better performance, use the async stream() method directly if possible.</p> <p>Parameters:</p> Name Type Description Default <code>filter_obj</code> <code>EventFilter</code> <p>Event filter with date range and query parameters</p> required <code>deduplicate</code> <code>bool</code> <p>If True, deduplicate events based on dedupe_strategy</p> <code>False</code> <code>dedupe_strategy</code> <code>DedupeStrategy | None</code> <p>Deduplication strategy (default: URL_DATE_LOCATION)</p> <code>None</code> <code>use_bigquery</code> <code>bool</code> <p>If True, skip files and use BigQuery directly</p> <code>False</code> <p>Returns:</p> Type Description <code>Iterator[Event]</code> <p>Iterator that yields Event instances for each matching event</p> <p>Raises:</p> Type Description <code>RateLimitError</code> <p>If rate limited and fallback not available</p> <code>APIError</code> <p>If download fails and fallback not available</p> <code>ConfigurationError</code> <p>If BigQuery requested but not configured</p> <code>RuntimeError</code> <p>If called from within an already running event loop</p> Example <p>filter_obj = EventFilter( ...     date_range=DateRange(start=date(2024, 1, 1)), ...     actor1_country=\"USA\", ... ) for event in endpoint.stream_sync(filter_obj, deduplicate=True): ...     print(event.global_event_id)</p> Source code in <code>src/py_gdelt/endpoints/events.py</code> <pre><code>def stream_sync(\n    self,\n    filter_obj: EventFilter,\n    *,\n    deduplicate: bool = False,\n    dedupe_strategy: DedupeStrategy | None = None,\n    use_bigquery: bool = False,\n) -&gt; Iterator[Event]:\n    \"\"\"Synchronous wrapper for stream().\n\n    This method provides a synchronous iterator interface over async streaming.\n    It internally manages the event loop and yields events one at a time,\n    providing true streaming behavior with memory efficiency.\n\n    Note: This creates a new event loop for each iteration, which has some overhead.\n    For better performance, use the async stream() method directly if possible.\n\n    Args:\n        filter_obj: Event filter with date range and query parameters\n        deduplicate: If True, deduplicate events based on dedupe_strategy\n        dedupe_strategy: Deduplication strategy (default: URL_DATE_LOCATION)\n        use_bigquery: If True, skip files and use BigQuery directly\n\n    Returns:\n        Iterator that yields Event instances for each matching event\n\n    Raises:\n        RateLimitError: If rate limited and fallback not available\n        APIError: If download fails and fallback not available\n        ConfigurationError: If BigQuery requested but not configured\n        RuntimeError: If called from within an already running event loop\n\n    Example:\n        &gt;&gt;&gt; filter_obj = EventFilter(\n        ...     date_range=DateRange(start=date(2024, 1, 1)),\n        ...     actor1_country=\"USA\",\n        ... )\n        &gt;&gt;&gt; for event in endpoint.stream_sync(filter_obj, deduplicate=True):\n        ...     print(event.global_event_id)\n    \"\"\"\n\n    async def _async_generator() -&gt; AsyncIterator[Event]:\n        \"\"\"Internal async generator for sync wrapper.\"\"\"\n        async for event in self.stream(\n            filter_obj,\n            deduplicate=deduplicate,\n            dedupe_strategy=dedupe_strategy,\n            use_bigquery=use_bigquery,\n        ):\n            yield event\n\n    # Run async generator and yield results synchronously\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    try:\n        async_gen = _async_generator()\n        while True:\n            try:\n                event = loop.run_until_complete(async_gen.__anext__())\n                yield event\n            except StopAsyncIteration:\n                break\n    finally:\n        loop.close()\n</code></pre>"},{"location":"api/endpoints/#mentionsendpoint","title":"MentionsEndpoint","text":""},{"location":"api/endpoints/#py_gdelt.endpoints.mentions.MentionsEndpoint","title":"<code>MentionsEndpoint</code>","text":"<p>Endpoint for querying GDELT Mentions data.</p> <p>Mentions track individual occurrences of events across different news sources. Each mention links to an event in the Events table via GlobalEventID and contains metadata about the source, timing, document position, and confidence.</p> <p>This endpoint uses DataFetcher for multi-source orchestration: - Primary: File downloads (free, no credentials needed) - Fallback: BigQuery (on rate limit/error, if credentials configured)</p> <p>Parameters:</p> Name Type Description Default <code>file_source</code> <code>FileSource</code> <p>FileSource instance for downloading GDELT files</p> required <code>bigquery_source</code> <code>BigQuerySource | None</code> <p>Optional BigQuerySource instance for fallback queries</p> <code>None</code> <code>settings</code> <code>GDELTSettings | None</code> <p>Optional GDELTSettings for configuration (currently unused but reserved for future features like caching)</p> <code>None</code> <code>fallback_enabled</code> <code>bool</code> <p>Whether to fallback to BigQuery on errors (default: True)</p> <code>True</code> <code>error_policy</code> <code>ErrorPolicy</code> <p>How to handle errors - 'raise', 'warn', or 'skip' (default: 'warn')</p> <code>'warn'</code> Note <p>Mentions queries require BigQuery as files don't support event-specific filtering. File downloads would require fetching entire date ranges and filtering client-side, which is inefficient for single-event queries. BigQuery fallback only activates if both fallback_enabled=True AND bigquery_source is provided AND credentials are configured.</p> Example <p>from datetime import date from py_gdelt.filters import DateRange, EventFilter from py_gdelt.sources import FileSource, BigQuerySource from py_gdelt.sources.fetcher import DataFetcher</p> <p>async with FileSource() as file_source: ...     bq_source = BigQuerySource() ...     fetcher = DataFetcher(file_source=file_source, bigquery_source=bq_source) ...     endpoint = MentionsEndpoint(fetcher=fetcher) ... ...     filter_obj = EventFilter( ...         date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7)) ...     ) ... ...     # Batch query ...     result = await endpoint.query(global_event_id=\"123456789\", filter_obj=filter_obj) ...     print(f\"Found {len(result)} mentions\") ...     for mention in result: ...         print(mention.source_name) ... ...     # Streaming query ...     async for mention in endpoint.stream(global_event_id=\"123456789\", filter_obj=filter_obj): ...         print(mention.source_name)</p> Source code in <code>src/py_gdelt/endpoints/mentions.py</code> <pre><code>class MentionsEndpoint:\n    \"\"\"Endpoint for querying GDELT Mentions data.\n\n    Mentions track individual occurrences of events across different news sources.\n    Each mention links to an event in the Events table via GlobalEventID and contains\n    metadata about the source, timing, document position, and confidence.\n\n    This endpoint uses DataFetcher for multi-source orchestration:\n    - Primary: File downloads (free, no credentials needed)\n    - Fallback: BigQuery (on rate limit/error, if credentials configured)\n\n    Args:\n        file_source: FileSource instance for downloading GDELT files\n        bigquery_source: Optional BigQuerySource instance for fallback queries\n        settings: Optional GDELTSettings for configuration (currently unused but\n            reserved for future features like caching)\n        fallback_enabled: Whether to fallback to BigQuery on errors (default: True)\n        error_policy: How to handle errors - 'raise', 'warn', or 'skip' (default: 'warn')\n\n    Note:\n        Mentions queries require BigQuery as files don't support event-specific filtering.\n        File downloads would require fetching entire date ranges and filtering client-side,\n        which is inefficient for single-event queries.\n        BigQuery fallback only activates if both fallback_enabled=True AND\n        bigquery_source is provided AND credentials are configured.\n\n    Example:\n        &gt;&gt;&gt; from datetime import date\n        &gt;&gt;&gt; from py_gdelt.filters import DateRange, EventFilter\n        &gt;&gt;&gt; from py_gdelt.sources import FileSource, BigQuerySource\n        &gt;&gt;&gt; from py_gdelt.sources.fetcher import DataFetcher\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; async with FileSource() as file_source:\n        ...     bq_source = BigQuerySource()\n        ...     fetcher = DataFetcher(file_source=file_source, bigquery_source=bq_source)\n        ...     endpoint = MentionsEndpoint(fetcher=fetcher)\n        ...\n        ...     filter_obj = EventFilter(\n        ...         date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7))\n        ...     )\n        ...\n        ...     # Batch query\n        ...     result = await endpoint.query(global_event_id=\"123456789\", filter_obj=filter_obj)\n        ...     print(f\"Found {len(result)} mentions\")\n        ...     for mention in result:\n        ...         print(mention.source_name)\n        ...\n        ...     # Streaming query\n        ...     async for mention in endpoint.stream(global_event_id=\"123456789\", filter_obj=filter_obj):\n        ...         print(mention.source_name)\n    \"\"\"\n\n    def __init__(\n        self,\n        file_source: FileSource,\n        bigquery_source: BigQuerySource | None = None,\n        *,\n        settings: GDELTSettings | None = None,\n        fallback_enabled: bool = True,\n        error_policy: ErrorPolicy = \"warn\",\n    ) -&gt; None:\n        from py_gdelt.sources.fetcher import DataFetcher\n\n        self._settings = settings\n        self._fetcher: DataFetcher = DataFetcher(\n            file_source=file_source,\n            bigquery_source=bigquery_source,\n            fallback_enabled=fallback_enabled,\n            error_policy=error_policy,\n        )\n\n        logger.debug(\n            \"MentionsEndpoint initialized (fallback_enabled=%s, error_policy=%s)\",\n            fallback_enabled,\n            error_policy,\n        )\n\n    async def query(\n        self,\n        global_event_id: int,\n        filter_obj: EventFilter,\n        *,\n        use_bigquery: bool = True,\n    ) -&gt; FetchResult[Mention]:\n        \"\"\"Query mentions for a specific event and return all results.\n\n        This method collects all mentions into memory and returns them as a FetchResult.\n        For large result sets or memory-constrained environments, use stream() instead.\n\n        Args:\n            global_event_id: Global event ID to fetch mentions for (integer)\n            filter_obj: Filter with date range for the query window\n            use_bigquery: If True, use BigQuery directly (default: True, recommended for mentions)\n\n        Returns:\n            FetchResult[Mention]: Container with list of Mention objects and failure tracking\n\n        Raises:\n            ConfigurationError: If BigQuery not configured but required\n            ValueError: If date range is invalid or too large\n\n        Example:\n            &gt;&gt;&gt; filter_obj = EventFilter(\n            ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7))\n            ... )\n            &gt;&gt;&gt; result = await endpoint.query(123456789, filter_obj)\n            &gt;&gt;&gt; print(f\"Complete: {result.complete}, Count: {len(result)}\")\n            &gt;&gt;&gt; for mention in result:\n            ...     print(f\"{mention.source_name}: {mention.confidence}%\")\n        \"\"\"\n        logger.info(\n            \"Querying mentions for event %s (date_range=%s to %s, use_bigquery=%s)\",\n            global_event_id,\n            filter_obj.date_range.start,\n            filter_obj.date_range.end or filter_obj.date_range.start,\n            use_bigquery,\n        )\n\n        # Collect all mentions\n        mentions: list[Mention] = [\n            mention\n            async for mention in self.stream(\n                global_event_id=global_event_id,\n                filter_obj=filter_obj,\n                use_bigquery=use_bigquery,\n            )\n        ]\n\n        logger.info(\n            \"Query complete: fetched %d mentions for event %s\",\n            len(mentions),\n            global_event_id,\n        )\n\n        # For now, return FetchResult with no failures\n        # In future, we could track file-level failures if using file source\n        return FetchResult(data=mentions, failed=[])\n\n    async def stream(\n        self,\n        global_event_id: int,\n        filter_obj: EventFilter,\n        *,\n        use_bigquery: bool = True,\n    ) -&gt; AsyncIterator[Mention]:\n        \"\"\"Stream mentions for a specific event.\n\n        This method yields mentions one at a time, converting from internal _RawMention\n        to public Mention model at the yield boundary. Memory-efficient for large result sets.\n\n        Args:\n            global_event_id: Global event ID to fetch mentions for (integer)\n            filter_obj: Filter with date range for the query window\n            use_bigquery: If True, use BigQuery directly (default: True, recommended for mentions)\n\n        Yields:\n            Mention: Individual mention records with full type safety\n\n        Raises:\n            ConfigurationError: If BigQuery not configured but required\n            ValueError: If date range is invalid or too large\n\n        Example:\n            &gt;&gt;&gt; filter_obj = EventFilter(\n            ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7))\n            ... )\n            &gt;&gt;&gt; async for mention in endpoint.stream(123456789, filter_obj):\n            ...     if mention.confidence &gt;= 80:\n            ...         print(f\"High confidence: {mention.source_name}\")\n        \"\"\"\n        logger.debug(\n            \"Streaming mentions for event %s (date_range=%s to %s)\",\n            global_event_id,\n            filter_obj.date_range.start,\n            filter_obj.date_range.end or filter_obj.date_range.start,\n        )\n\n        mentions_count = 0\n\n        # Use DataFetcher to query mentions\n        # Note: fetch_mentions() returns AsyncIterator[_RawMention] (or dict from BigQuery)\n        raw_mentions: AsyncIterator[_RawMention | dict[str, Any]] = self._fetcher.fetch_mentions(\n            global_event_id=global_event_id,\n            filter_obj=filter_obj,\n            use_bigquery=use_bigquery,\n        )\n\n        # Convert _RawMention to Mention at yield boundary\n        async for raw_mention in raw_mentions:\n            # DataFetcher.fetch_mentions() returns dicts from BigQuery\n            # We need to convert them to Mention\n            # For now, assume BigQuery returns compatible dict structure\n            if isinstance(raw_mention, dict):\n                # BigQuery returns dict - convert to Mention directly\n                # This is a simplified implementation - in production, we'd need proper BigQuery row mapping\n                mention = self._dict_to_mention(raw_mention)\n            else:\n                # File source would return _RawMention (though mentions don't come from files typically)\n                mention = Mention.from_raw(raw_mention)\n\n            mentions_count += 1\n            yield mention\n\n        logger.debug(\"Streamed %d mentions for event %s\", mentions_count, global_event_id)\n\n    def _dict_to_mention(self, row: dict[str, Any]) -&gt; Mention:\n        \"\"\"Convert BigQuery row dict to Mention model.\n\n        This is a helper to bridge the gap between BigQuery result dicts and our Pydantic models.\n        BigQuery returns rows as dictionaries, which we need to map to our internal structure.\n\n        Args:\n            row: BigQuery row as dictionary\n\n        Returns:\n            Mention: Validated Mention instance\n\n        Note:\n            This is a temporary implementation. In production, we'd use a proper BigQuery row mapper\n            that handles field name translations and type conversions.\n        \"\"\"\n        # Import here to avoid circular dependency\n        from py_gdelt.models._internal import _RawMention\n\n        # Map BigQuery column names to _RawMention fields\n        # BigQuery uses different naming (e.g., EventTimeDate vs event_time_date)\n        raw_mention = _RawMention(\n            global_event_id=str(row.get(\"GlobalEventID\", \"\")),\n            event_time_date=str(row.get(\"EventTimeDate\", \"\")),\n            event_time_full=str(row.get(\"EventTimeFullDate\", \"\")),\n            mention_time_date=str(row.get(\"MentionTimeDate\", \"\")),\n            mention_time_full=str(row.get(\"MentionTimeFullDate\", \"\")),\n            mention_type=str(row.get(\"MentionType\", \"1\")),\n            mention_source_name=str(row.get(\"MentionSourceName\", \"\")),\n            mention_identifier=str(row.get(\"MentionIdentifier\", \"\")),\n            sentence_id=str(row.get(\"SentenceID\", \"0\")),\n            actor1_char_offset=str(row.get(\"Actor1CharOffset\", \"\")),\n            actor2_char_offset=str(row.get(\"Actor2CharOffset\", \"\")),\n            action_char_offset=str(row.get(\"ActionCharOffset\", \"\")),\n            in_raw_text=str(row.get(\"InRawText\", \"0\")),\n            confidence=str(row.get(\"Confidence\", \"50\")),\n            mention_doc_length=str(row.get(\"MentionDocLen\", \"0\")),\n            mention_doc_tone=str(row.get(\"MentionDocTone\", \"0.0\")),\n            mention_doc_translation_info=row.get(\"MentionDocTranslationInfo\"),\n            extras=row.get(\"Extras\"),\n        )\n\n        return Mention.from_raw(raw_mention)\n\n    def query_sync(\n        self,\n        global_event_id: int,\n        filter_obj: EventFilter,\n        *,\n        use_bigquery: bool = True,\n    ) -&gt; FetchResult[Mention]:\n        \"\"\"Synchronous wrapper for query().\n\n        This is a convenience method for synchronous code. It runs the async query()\n        method in a new event loop. For better performance, use the async version directly.\n\n        Args:\n            global_event_id: Global event ID to fetch mentions for (integer)\n            filter_obj: Filter with date range for the query window\n            use_bigquery: If True, use BigQuery directly (default: True)\n\n        Returns:\n            FetchResult[Mention]: Container with list of Mention objects\n\n        Raises:\n            ConfigurationError: If BigQuery not configured but required\n            ValueError: If date range is invalid\n\n        Example:\n            &gt;&gt;&gt; filter_obj = EventFilter(\n            ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7))\n            ... )\n            &gt;&gt;&gt; result = endpoint.query_sync(123456789, filter_obj)\n            &gt;&gt;&gt; for mention in result:\n            ...     print(mention.source_name)\n        \"\"\"\n        return asyncio.run(\n            self.query(\n                global_event_id=global_event_id,\n                filter_obj=filter_obj,\n                use_bigquery=use_bigquery,\n            ),\n        )\n\n    def stream_sync(\n        self,\n        global_event_id: int,\n        filter_obj: EventFilter,\n        *,\n        use_bigquery: bool = True,\n    ) -&gt; Iterator[Mention]:\n        \"\"\"Synchronous wrapper for stream().\n\n        This method provides a synchronous iterator interface over async streaming.\n        It internally manages the event loop and yields mentions one at a time,\n        providing true streaming behavior with memory efficiency.\n\n        Note: This creates a new event loop for each iteration, which has some overhead.\n        For better performance, use the async stream() method directly if possible.\n\n        Args:\n            global_event_id: Global event ID to fetch mentions for (integer)\n            filter_obj: Filter with date range for the query window\n            use_bigquery: If True, use BigQuery directly (default: True)\n\n        Returns:\n            Iterator of individual Mention records\n\n        Raises:\n            ConfigurationError: If BigQuery not configured but required\n            ValueError: If date range is invalid\n            RuntimeError: If called from within an already running event loop\n\n        Example:\n            &gt;&gt;&gt; filter_obj = EventFilter(\n            ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7))\n            ... )\n            &gt;&gt;&gt; for mention in endpoint.stream_sync(123456789, filter_obj):\n            ...     print(mention.source_name)\n        \"\"\"\n\n        async def _async_generator() -&gt; AsyncIterator[Mention]:\n            \"\"\"Internal async generator for sync wrapper.\"\"\"\n            async for mention in self.stream(\n                global_event_id=global_event_id,\n                filter_obj=filter_obj,\n                use_bigquery=use_bigquery,\n            ):\n                yield mention\n\n        # Run async generator and yield results synchronously\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        try:\n            async_gen = _async_generator()\n            while True:\n                try:\n                    mention = loop.run_until_complete(async_gen.__anext__())\n                    yield mention\n                except StopAsyncIteration:\n                    break\n        finally:\n            loop.close()\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.mentions.MentionsEndpoint.query","title":"<code>query(global_event_id, filter_obj, *, use_bigquery=True)</code>  <code>async</code>","text":"<p>Query mentions for a specific event and return all results.</p> <p>This method collects all mentions into memory and returns them as a FetchResult. For large result sets or memory-constrained environments, use stream() instead.</p> <p>Parameters:</p> Name Type Description Default <code>global_event_id</code> <code>int</code> <p>Global event ID to fetch mentions for (integer)</p> required <code>filter_obj</code> <code>EventFilter</code> <p>Filter with date range for the query window</p> required <code>use_bigquery</code> <code>bool</code> <p>If True, use BigQuery directly (default: True, recommended for mentions)</p> <code>True</code> <p>Returns:</p> Type Description <code>FetchResult[Mention]</code> <p>FetchResult[Mention]: Container with list of Mention objects and failure tracking</p> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If BigQuery not configured but required</p> <code>ValueError</code> <p>If date range is invalid or too large</p> Example <p>filter_obj = EventFilter( ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7)) ... ) result = await endpoint.query(123456789, filter_obj) print(f\"Complete: {result.complete}, Count: {len(result)}\") for mention in result: ...     print(f\"{mention.source_name}: {mention.confidence}%\")</p> Source code in <code>src/py_gdelt/endpoints/mentions.py</code> <pre><code>async def query(\n    self,\n    global_event_id: int,\n    filter_obj: EventFilter,\n    *,\n    use_bigquery: bool = True,\n) -&gt; FetchResult[Mention]:\n    \"\"\"Query mentions for a specific event and return all results.\n\n    This method collects all mentions into memory and returns them as a FetchResult.\n    For large result sets or memory-constrained environments, use stream() instead.\n\n    Args:\n        global_event_id: Global event ID to fetch mentions for (integer)\n        filter_obj: Filter with date range for the query window\n        use_bigquery: If True, use BigQuery directly (default: True, recommended for mentions)\n\n    Returns:\n        FetchResult[Mention]: Container with list of Mention objects and failure tracking\n\n    Raises:\n        ConfigurationError: If BigQuery not configured but required\n        ValueError: If date range is invalid or too large\n\n    Example:\n        &gt;&gt;&gt; filter_obj = EventFilter(\n        ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7))\n        ... )\n        &gt;&gt;&gt; result = await endpoint.query(123456789, filter_obj)\n        &gt;&gt;&gt; print(f\"Complete: {result.complete}, Count: {len(result)}\")\n        &gt;&gt;&gt; for mention in result:\n        ...     print(f\"{mention.source_name}: {mention.confidence}%\")\n    \"\"\"\n    logger.info(\n        \"Querying mentions for event %s (date_range=%s to %s, use_bigquery=%s)\",\n        global_event_id,\n        filter_obj.date_range.start,\n        filter_obj.date_range.end or filter_obj.date_range.start,\n        use_bigquery,\n    )\n\n    # Collect all mentions\n    mentions: list[Mention] = [\n        mention\n        async for mention in self.stream(\n            global_event_id=global_event_id,\n            filter_obj=filter_obj,\n            use_bigquery=use_bigquery,\n        )\n    ]\n\n    logger.info(\n        \"Query complete: fetched %d mentions for event %s\",\n        len(mentions),\n        global_event_id,\n    )\n\n    # For now, return FetchResult with no failures\n    # In future, we could track file-level failures if using file source\n    return FetchResult(data=mentions, failed=[])\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.mentions.MentionsEndpoint.stream","title":"<code>stream(global_event_id, filter_obj, *, use_bigquery=True)</code>  <code>async</code>","text":"<p>Stream mentions for a specific event.</p> <p>This method yields mentions one at a time, converting from internal _RawMention to public Mention model at the yield boundary. Memory-efficient for large result sets.</p> <p>Parameters:</p> Name Type Description Default <code>global_event_id</code> <code>int</code> <p>Global event ID to fetch mentions for (integer)</p> required <code>filter_obj</code> <code>EventFilter</code> <p>Filter with date range for the query window</p> required <code>use_bigquery</code> <code>bool</code> <p>If True, use BigQuery directly (default: True, recommended for mentions)</p> <code>True</code> <p>Yields:</p> Name Type Description <code>Mention</code> <code>AsyncIterator[Mention]</code> <p>Individual mention records with full type safety</p> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If BigQuery not configured but required</p> <code>ValueError</code> <p>If date range is invalid or too large</p> Example <p>filter_obj = EventFilter( ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7)) ... ) async for mention in endpoint.stream(123456789, filter_obj): ...     if mention.confidence &gt;= 80: ...         print(f\"High confidence: {mention.source_name}\")</p> Source code in <code>src/py_gdelt/endpoints/mentions.py</code> <pre><code>async def stream(\n    self,\n    global_event_id: int,\n    filter_obj: EventFilter,\n    *,\n    use_bigquery: bool = True,\n) -&gt; AsyncIterator[Mention]:\n    \"\"\"Stream mentions for a specific event.\n\n    This method yields mentions one at a time, converting from internal _RawMention\n    to public Mention model at the yield boundary. Memory-efficient for large result sets.\n\n    Args:\n        global_event_id: Global event ID to fetch mentions for (integer)\n        filter_obj: Filter with date range for the query window\n        use_bigquery: If True, use BigQuery directly (default: True, recommended for mentions)\n\n    Yields:\n        Mention: Individual mention records with full type safety\n\n    Raises:\n        ConfigurationError: If BigQuery not configured but required\n        ValueError: If date range is invalid or too large\n\n    Example:\n        &gt;&gt;&gt; filter_obj = EventFilter(\n        ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7))\n        ... )\n        &gt;&gt;&gt; async for mention in endpoint.stream(123456789, filter_obj):\n        ...     if mention.confidence &gt;= 80:\n        ...         print(f\"High confidence: {mention.source_name}\")\n    \"\"\"\n    logger.debug(\n        \"Streaming mentions for event %s (date_range=%s to %s)\",\n        global_event_id,\n        filter_obj.date_range.start,\n        filter_obj.date_range.end or filter_obj.date_range.start,\n    )\n\n    mentions_count = 0\n\n    # Use DataFetcher to query mentions\n    # Note: fetch_mentions() returns AsyncIterator[_RawMention] (or dict from BigQuery)\n    raw_mentions: AsyncIterator[_RawMention | dict[str, Any]] = self._fetcher.fetch_mentions(\n        global_event_id=global_event_id,\n        filter_obj=filter_obj,\n        use_bigquery=use_bigquery,\n    )\n\n    # Convert _RawMention to Mention at yield boundary\n    async for raw_mention in raw_mentions:\n        # DataFetcher.fetch_mentions() returns dicts from BigQuery\n        # We need to convert them to Mention\n        # For now, assume BigQuery returns compatible dict structure\n        if isinstance(raw_mention, dict):\n            # BigQuery returns dict - convert to Mention directly\n            # This is a simplified implementation - in production, we'd need proper BigQuery row mapping\n            mention = self._dict_to_mention(raw_mention)\n        else:\n            # File source would return _RawMention (though mentions don't come from files typically)\n            mention = Mention.from_raw(raw_mention)\n\n        mentions_count += 1\n        yield mention\n\n    logger.debug(\"Streamed %d mentions for event %s\", mentions_count, global_event_id)\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.mentions.MentionsEndpoint.query_sync","title":"<code>query_sync(global_event_id, filter_obj, *, use_bigquery=True)</code>","text":"<p>Synchronous wrapper for query().</p> <p>This is a convenience method for synchronous code. It runs the async query() method in a new event loop. For better performance, use the async version directly.</p> <p>Parameters:</p> Name Type Description Default <code>global_event_id</code> <code>int</code> <p>Global event ID to fetch mentions for (integer)</p> required <code>filter_obj</code> <code>EventFilter</code> <p>Filter with date range for the query window</p> required <code>use_bigquery</code> <code>bool</code> <p>If True, use BigQuery directly (default: True)</p> <code>True</code> <p>Returns:</p> Type Description <code>FetchResult[Mention]</code> <p>FetchResult[Mention]: Container with list of Mention objects</p> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If BigQuery not configured but required</p> <code>ValueError</code> <p>If date range is invalid</p> Example <p>filter_obj = EventFilter( ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7)) ... ) result = endpoint.query_sync(123456789, filter_obj) for mention in result: ...     print(mention.source_name)</p> Source code in <code>src/py_gdelt/endpoints/mentions.py</code> <pre><code>def query_sync(\n    self,\n    global_event_id: int,\n    filter_obj: EventFilter,\n    *,\n    use_bigquery: bool = True,\n) -&gt; FetchResult[Mention]:\n    \"\"\"Synchronous wrapper for query().\n\n    This is a convenience method for synchronous code. It runs the async query()\n    method in a new event loop. For better performance, use the async version directly.\n\n    Args:\n        global_event_id: Global event ID to fetch mentions for (integer)\n        filter_obj: Filter with date range for the query window\n        use_bigquery: If True, use BigQuery directly (default: True)\n\n    Returns:\n        FetchResult[Mention]: Container with list of Mention objects\n\n    Raises:\n        ConfigurationError: If BigQuery not configured but required\n        ValueError: If date range is invalid\n\n    Example:\n        &gt;&gt;&gt; filter_obj = EventFilter(\n        ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7))\n        ... )\n        &gt;&gt;&gt; result = endpoint.query_sync(123456789, filter_obj)\n        &gt;&gt;&gt; for mention in result:\n        ...     print(mention.source_name)\n    \"\"\"\n    return asyncio.run(\n        self.query(\n            global_event_id=global_event_id,\n            filter_obj=filter_obj,\n            use_bigquery=use_bigquery,\n        ),\n    )\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.mentions.MentionsEndpoint.stream_sync","title":"<code>stream_sync(global_event_id, filter_obj, *, use_bigquery=True)</code>","text":"<p>Synchronous wrapper for stream().</p> <p>This method provides a synchronous iterator interface over async streaming. It internally manages the event loop and yields mentions one at a time, providing true streaming behavior with memory efficiency.</p> <p>Note: This creates a new event loop for each iteration, which has some overhead. For better performance, use the async stream() method directly if possible.</p> <p>Parameters:</p> Name Type Description Default <code>global_event_id</code> <code>int</code> <p>Global event ID to fetch mentions for (integer)</p> required <code>filter_obj</code> <code>EventFilter</code> <p>Filter with date range for the query window</p> required <code>use_bigquery</code> <code>bool</code> <p>If True, use BigQuery directly (default: True)</p> <code>True</code> <p>Returns:</p> Type Description <code>Iterator[Mention]</code> <p>Iterator of individual Mention records</p> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If BigQuery not configured but required</p> <code>ValueError</code> <p>If date range is invalid</p> <code>RuntimeError</code> <p>If called from within an already running event loop</p> Example <p>filter_obj = EventFilter( ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7)) ... ) for mention in endpoint.stream_sync(123456789, filter_obj): ...     print(mention.source_name)</p> Source code in <code>src/py_gdelt/endpoints/mentions.py</code> <pre><code>def stream_sync(\n    self,\n    global_event_id: int,\n    filter_obj: EventFilter,\n    *,\n    use_bigquery: bool = True,\n) -&gt; Iterator[Mention]:\n    \"\"\"Synchronous wrapper for stream().\n\n    This method provides a synchronous iterator interface over async streaming.\n    It internally manages the event loop and yields mentions one at a time,\n    providing true streaming behavior with memory efficiency.\n\n    Note: This creates a new event loop for each iteration, which has some overhead.\n    For better performance, use the async stream() method directly if possible.\n\n    Args:\n        global_event_id: Global event ID to fetch mentions for (integer)\n        filter_obj: Filter with date range for the query window\n        use_bigquery: If True, use BigQuery directly (default: True)\n\n    Returns:\n        Iterator of individual Mention records\n\n    Raises:\n        ConfigurationError: If BigQuery not configured but required\n        ValueError: If date range is invalid\n        RuntimeError: If called from within an already running event loop\n\n    Example:\n        &gt;&gt;&gt; filter_obj = EventFilter(\n        ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7))\n        ... )\n        &gt;&gt;&gt; for mention in endpoint.stream_sync(123456789, filter_obj):\n        ...     print(mention.source_name)\n    \"\"\"\n\n    async def _async_generator() -&gt; AsyncIterator[Mention]:\n        \"\"\"Internal async generator for sync wrapper.\"\"\"\n        async for mention in self.stream(\n            global_event_id=global_event_id,\n            filter_obj=filter_obj,\n            use_bigquery=use_bigquery,\n        ):\n            yield mention\n\n    # Run async generator and yield results synchronously\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    try:\n        async_gen = _async_generator()\n        while True:\n            try:\n                mention = loop.run_until_complete(async_gen.__anext__())\n                yield mention\n            except StopAsyncIteration:\n                break\n    finally:\n        loop.close()\n</code></pre>"},{"location":"api/endpoints/#gkgendpoint","title":"GKGEndpoint","text":""},{"location":"api/endpoints/#py_gdelt.endpoints.gkg.GKGEndpoint","title":"<code>GKGEndpoint</code>","text":"<p>GKG (Global Knowledge Graph) endpoint for querying GDELT enriched content data.</p> <p>The GKGEndpoint provides access to GDELT's Global Knowledge Graph, which contains rich content analysis including themes, people, organizations, locations, counts, tone, and other metadata extracted from news articles.</p> <p>This endpoint uses DataFetcher to orchestrate source selection: - Files are ALWAYS primary (free, no credentials needed) - BigQuery is FALLBACK ONLY (on 429/error, if credentials configured)</p> <p>Parameters:</p> Name Type Description Default <code>file_source</code> <code>FileSource</code> <p>FileSource instance for downloading GDELT files</p> required <code>bigquery_source</code> <code>BigQuerySource | None</code> <p>Optional BigQuerySource instance for fallback queries</p> <code>None</code> <code>settings</code> <code>GDELTSettings | None</code> <p>Optional GDELTSettings for configuration (currently unused but reserved for future features like caching)</p> <code>None</code> <code>fallback_enabled</code> <code>bool</code> <p>Whether to fallback to BigQuery on errors (default: True)</p> <code>True</code> <code>error_policy</code> <code>ErrorPolicy</code> <p>How to handle errors - 'raise', 'warn', or 'skip' (default: 'warn')</p> <code>'warn'</code> Note <p>BigQuery fallback only activates if both fallback_enabled=True AND bigquery_source is provided AND credentials are configured.</p> Example <p>Basic GKG query:</p> <p>from datetime import date from py_gdelt.filters import GKGFilter, DateRange from py_gdelt.endpoints.gkg import GKGEndpoint from py_gdelt.sources.files import FileSource</p> <p>async def main(): ...     async with FileSource() as file_source: ...         endpoint = GKGEndpoint(file_source=file_source) ...         filter_obj = GKGFilter( ...             date_range=DateRange(start=date(2024, 1, 1)), ...             themes=[\"ENV_CLIMATECHANGE\"] ...         ) ...         result = await endpoint.query(filter_obj) ...         for record in result: ...             print(record.record_id, record.source_url)</p> <p>Streaming large result sets:</p> <p>async def stream_example(): ...     async with FileSource() as file_source: ...         endpoint = GKGEndpoint(file_source=file_source) ...         filter_obj = GKGFilter( ...             date_range=DateRange(start=date(2024, 1, 1)), ...             country=\"USA\" ...         ) ...         async for record in endpoint.stream(filter_obj): ...             print(record.record_id, record.primary_theme)</p> <p>Synchronous usage:</p> <p>endpoint = GKGEndpoint(file_source=file_source) result = endpoint.query_sync(filter_obj) for record in result: ...     print(record.record_id)</p> Source code in <code>src/py_gdelt/endpoints/gkg.py</code> <pre><code>class GKGEndpoint:\n    \"\"\"GKG (Global Knowledge Graph) endpoint for querying GDELT enriched content data.\n\n    The GKGEndpoint provides access to GDELT's Global Knowledge Graph, which contains\n    rich content analysis including themes, people, organizations, locations, counts,\n    tone, and other metadata extracted from news articles.\n\n    This endpoint uses DataFetcher to orchestrate source selection:\n    - Files are ALWAYS primary (free, no credentials needed)\n    - BigQuery is FALLBACK ONLY (on 429/error, if credentials configured)\n\n    Args:\n        file_source: FileSource instance for downloading GDELT files\n        bigquery_source: Optional BigQuerySource instance for fallback queries\n        settings: Optional GDELTSettings for configuration (currently unused but\n            reserved for future features like caching)\n        fallback_enabled: Whether to fallback to BigQuery on errors (default: True)\n        error_policy: How to handle errors - 'raise', 'warn', or 'skip' (default: 'warn')\n\n    Note:\n        BigQuery fallback only activates if both fallback_enabled=True AND\n        bigquery_source is provided AND credentials are configured.\n\n    Example:\n        Basic GKG query:\n\n        &gt;&gt;&gt; from datetime import date\n        &gt;&gt;&gt; from py_gdelt.filters import GKGFilter, DateRange\n        &gt;&gt;&gt; from py_gdelt.endpoints.gkg import GKGEndpoint\n        &gt;&gt;&gt; from py_gdelt.sources.files import FileSource\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; async def main():\n        ...     async with FileSource() as file_source:\n        ...         endpoint = GKGEndpoint(file_source=file_source)\n        ...         filter_obj = GKGFilter(\n        ...             date_range=DateRange(start=date(2024, 1, 1)),\n        ...             themes=[\"ENV_CLIMATECHANGE\"]\n        ...         )\n        ...         result = await endpoint.query(filter_obj)\n        ...         for record in result:\n        ...             print(record.record_id, record.source_url)\n\n        Streaming large result sets:\n\n        &gt;&gt;&gt; async def stream_example():\n        ...     async with FileSource() as file_source:\n        ...         endpoint = GKGEndpoint(file_source=file_source)\n        ...         filter_obj = GKGFilter(\n        ...             date_range=DateRange(start=date(2024, 1, 1)),\n        ...             country=\"USA\"\n        ...         )\n        ...         async for record in endpoint.stream(filter_obj):\n        ...             print(record.record_id, record.primary_theme)\n\n        Synchronous usage:\n\n        &gt;&gt;&gt; endpoint = GKGEndpoint(file_source=file_source)\n        &gt;&gt;&gt; result = endpoint.query_sync(filter_obj)\n        &gt;&gt;&gt; for record in result:\n        ...     print(record.record_id)\n    \"\"\"\n\n    def __init__(\n        self,\n        file_source: FileSource,\n        bigquery_source: BigQuerySource | None = None,\n        *,\n        settings: GDELTSettings | None = None,\n        fallback_enabled: bool = True,\n        error_policy: ErrorPolicy = \"warn\",\n    ) -&gt; None:\n        from py_gdelt.sources.fetcher import DataFetcher\n\n        self._settings = settings\n        self._fetcher: Any = DataFetcher(\n            file_source=file_source,\n            bigquery_source=bigquery_source,\n            fallback_enabled=fallback_enabled,\n            error_policy=error_policy,\n        )\n\n        logger.debug(\n            \"GKGEndpoint initialized (fallback_enabled=%s, error_policy=%s)\",\n            fallback_enabled,\n            error_policy,\n        )\n\n    async def query(\n        self,\n        filter_obj: GKGFilter,\n        *,\n        use_bigquery: bool = False,\n    ) -&gt; FetchResult[GKGRecord]:\n        \"\"\"Query GKG data with automatic fallback and return all results.\n\n        This method fetches all matching GKG records and returns them as a FetchResult\n        container. For large result sets, consider using stream() instead to avoid\n        loading everything into memory.\n\n        Files are always tried first (free, no credentials), with automatic fallback\n        to BigQuery on rate limit/error if credentials are configured.\n\n        Args:\n            filter_obj: GKG filter with date range and query parameters\n            use_bigquery: If True, skip files and use BigQuery directly (default: False)\n\n        Returns:\n            FetchResult[GKGRecord] containing all matching records and any failures\n\n        Raises:\n            RateLimitError: If rate limited and fallback not available/enabled\n            APIError: If download fails and fallback not available/enabled\n            ConfigurationError: If BigQuery requested but not configured\n\n        Example:\n            &gt;&gt;&gt; from datetime import date\n            &gt;&gt;&gt; from py_gdelt.filters import GKGFilter, DateRange\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; filter_obj = GKGFilter(\n            ...     date_range=DateRange(start=date(2024, 1, 1)),\n            ...     themes=[\"ECON_STOCKMARKET\"],\n            ...     min_tone=0.0,  # Only positive tone\n            ... )\n            &gt;&gt;&gt; result = await endpoint.query(filter_obj)\n            &gt;&gt;&gt; print(f\"Fetched {len(result)} records\")\n            &gt;&gt;&gt; if not result.complete:\n            ...     print(f\"Warning: {result.total_failed} requests failed\")\n            &gt;&gt;&gt; for record in result:\n            ...     print(record.record_id, record.tone.tone if record.tone else None)\n        \"\"\"\n        records: list[GKGRecord] = [\n            record async for record in self.stream(filter_obj, use_bigquery=use_bigquery)\n        ]\n\n        logger.info(\"GKG query completed: %d records fetched\", len(records))\n\n        # Return FetchResult (failures tracked by DataFetcher error policy)\n        return FetchResult(data=records, failed=[])\n\n    async def stream(\n        self,\n        filter_obj: GKGFilter,\n        *,\n        use_bigquery: bool = False,\n    ) -&gt; AsyncIterator[GKGRecord]:\n        \"\"\"Stream GKG records with automatic fallback.\n\n        This method streams GKG records one at a time, which is memory-efficient for\n        large result sets. Records are converted from internal _RawGKG dataclass to\n        public GKGRecord Pydantic model at the yield boundary.\n\n        Files are always tried first (free, no credentials), with automatic fallback\n        to BigQuery on rate limit/error if credentials are configured.\n\n        Args:\n            filter_obj: GKG filter with date range and query parameters\n            use_bigquery: If True, skip files and use BigQuery directly (default: False)\n\n        Yields:\n            GKGRecord: Individual GKG records matching the filter criteria\n\n        Raises:\n            RateLimitError: If rate limited and fallback not available/enabled\n            APIError: If download fails and fallback not available/enabled\n            ConfigurationError: If BigQuery requested but not configured\n\n        Example:\n            &gt;&gt;&gt; from datetime import date\n            &gt;&gt;&gt; from py_gdelt.filters import GKGFilter, DateRange\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; filter_obj = GKGFilter(\n            ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7)),\n            ...     organizations=[\"United Nations\"],\n            ... )\n            &gt;&gt;&gt; count = 0\n            &gt;&gt;&gt; async for record in endpoint.stream(filter_obj):\n            ...     print(f\"Processing {record.record_id}\")\n            ...     count += 1\n            ...     if count &gt;= 1000:\n            ...         break  # Stop after 1000 records\n        \"\"\"\n        logger.debug(\"Starting GKG stream for filter: %s\", filter_obj)\n\n        # Use DataFetcher to fetch raw GKG records\n        async for raw_gkg in self._fetcher.fetch_gkg(filter_obj, use_bigquery=use_bigquery):\n            # Convert _RawGKG to GKGRecord at yield boundary\n            try:\n                record = GKGRecord.from_raw(raw_gkg)\n                yield record\n            except Exception as e:  # noqa: BLE001\n                # Error boundary: log conversion errors but continue processing other records\n                logger.warning(\"Failed to convert raw GKG record to GKGRecord: %s\", e)\n                continue\n\n    def query_sync(\n        self,\n        filter_obj: GKGFilter,\n        *,\n        use_bigquery: bool = False,\n    ) -&gt; FetchResult[GKGRecord]:\n        \"\"\"Synchronous wrapper for query().\n\n        This is a convenience method for synchronous code that internally uses\n        asyncio.run() to execute the async query() method.\n\n        Args:\n            filter_obj: GKG filter with date range and query parameters\n            use_bigquery: If True, skip files and use BigQuery directly (default: False)\n\n        Returns:\n            FetchResult[GKGRecord] containing all matching records and any failures\n\n        Raises:\n            RateLimitError: If rate limited and fallback not available/enabled\n            APIError: If download fails and fallback not available/enabled\n            ConfigurationError: If BigQuery requested but not configured\n            RuntimeError: If called from within an existing event loop\n\n        Example:\n            &gt;&gt;&gt; from datetime import date\n            &gt;&gt;&gt; from py_gdelt.filters import GKGFilter, DateRange\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Synchronous usage (no async/await needed)\n            &gt;&gt;&gt; endpoint = GKGEndpoint(file_source=file_source)\n            &gt;&gt;&gt; filter_obj = GKGFilter(\n            ...     date_range=DateRange(start=date(2024, 1, 1))\n            ... )\n            &gt;&gt;&gt; result = endpoint.query_sync(filter_obj)\n            &gt;&gt;&gt; for record in result:\n            ...     print(record.record_id)\n        \"\"\"\n        return asyncio.run(self.query(filter_obj, use_bigquery=use_bigquery))\n\n    def stream_sync(\n        self,\n        filter_obj: GKGFilter,\n        *,\n        use_bigquery: bool = False,\n    ) -&gt; Iterator[GKGRecord]:\n        \"\"\"Synchronous wrapper for stream().\n\n        This method provides a synchronous iterator interface over async streaming.\n        It internally manages the event loop and yields records one at a time.\n\n        Note: This creates a new event loop for each iteration, which has some overhead.\n        For better performance, use the async stream() method directly if possible.\n\n        Args:\n            filter_obj: GKG filter with date range and query parameters\n            use_bigquery: If True, skip files and use BigQuery directly (default: False)\n\n        Returns:\n            Iterator of GKGRecord instances for each matching record\n\n        Raises:\n            RateLimitError: If rate limited and fallback not available/enabled\n            APIError: If download fails and fallback not available/enabled\n            ConfigurationError: If BigQuery requested but not configured\n            RuntimeError: If called from within an existing event loop\n\n        Example:\n            &gt;&gt;&gt; from datetime import date\n            &gt;&gt;&gt; from py_gdelt.filters import GKGFilter, DateRange\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Synchronous streaming (no async/await needed)\n            &gt;&gt;&gt; endpoint = GKGEndpoint(file_source=file_source)\n            &gt;&gt;&gt; filter_obj = GKGFilter(\n            ...     date_range=DateRange(start=date(2024, 1, 1))\n            ... )\n            &gt;&gt;&gt; for record in endpoint.stream_sync(filter_obj):\n            ...     print(record.record_id)\n            ...     if record.has_quotations:\n            ...         print(f\"  {len(record.quotations)} quotations found\")\n        \"\"\"\n\n        async def _async_generator() -&gt; AsyncIterator[GKGRecord]:\n            \"\"\"Internal async generator for sync wrapper.\"\"\"\n            async for record in self.stream(filter_obj, use_bigquery=use_bigquery):\n                yield record\n\n        # Run async generator and yield results synchronously\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        try:\n            async_gen = _async_generator()\n            while True:\n                try:\n                    record = loop.run_until_complete(async_gen.__anext__())\n                    yield record\n                except StopAsyncIteration:\n                    break\n        finally:\n            loop.close()\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.gkg.GKGEndpoint.query","title":"<code>query(filter_obj, *, use_bigquery=False)</code>  <code>async</code>","text":"<p>Query GKG data with automatic fallback and return all results.</p> <p>This method fetches all matching GKG records and returns them as a FetchResult container. For large result sets, consider using stream() instead to avoid loading everything into memory.</p> <p>Files are always tried first (free, no credentials), with automatic fallback to BigQuery on rate limit/error if credentials are configured.</p> <p>Parameters:</p> Name Type Description Default <code>filter_obj</code> <code>GKGFilter</code> <p>GKG filter with date range and query parameters</p> required <code>use_bigquery</code> <code>bool</code> <p>If True, skip files and use BigQuery directly (default: False)</p> <code>False</code> <p>Returns:</p> Type Description <code>FetchResult[GKGRecord]</code> <p>FetchResult[GKGRecord] containing all matching records and any failures</p> <p>Raises:</p> Type Description <code>RateLimitError</code> <p>If rate limited and fallback not available/enabled</p> <code>APIError</code> <p>If download fails and fallback not available/enabled</p> <code>ConfigurationError</code> <p>If BigQuery requested but not configured</p> Example <p>from datetime import date from py_gdelt.filters import GKGFilter, DateRange</p> <p>filter_obj = GKGFilter( ...     date_range=DateRange(start=date(2024, 1, 1)), ...     themes=[\"ECON_STOCKMARKET\"], ...     min_tone=0.0,  # Only positive tone ... ) result = await endpoint.query(filter_obj) print(f\"Fetched {len(result)} records\") if not result.complete: ...     print(f\"Warning: {result.total_failed} requests failed\") for record in result: ...     print(record.record_id, record.tone.tone if record.tone else None)</p> Source code in <code>src/py_gdelt/endpoints/gkg.py</code> <pre><code>async def query(\n    self,\n    filter_obj: GKGFilter,\n    *,\n    use_bigquery: bool = False,\n) -&gt; FetchResult[GKGRecord]:\n    \"\"\"Query GKG data with automatic fallback and return all results.\n\n    This method fetches all matching GKG records and returns them as a FetchResult\n    container. For large result sets, consider using stream() instead to avoid\n    loading everything into memory.\n\n    Files are always tried first (free, no credentials), with automatic fallback\n    to BigQuery on rate limit/error if credentials are configured.\n\n    Args:\n        filter_obj: GKG filter with date range and query parameters\n        use_bigquery: If True, skip files and use BigQuery directly (default: False)\n\n    Returns:\n        FetchResult[GKGRecord] containing all matching records and any failures\n\n    Raises:\n        RateLimitError: If rate limited and fallback not available/enabled\n        APIError: If download fails and fallback not available/enabled\n        ConfigurationError: If BigQuery requested but not configured\n\n    Example:\n        &gt;&gt;&gt; from datetime import date\n        &gt;&gt;&gt; from py_gdelt.filters import GKGFilter, DateRange\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; filter_obj = GKGFilter(\n        ...     date_range=DateRange(start=date(2024, 1, 1)),\n        ...     themes=[\"ECON_STOCKMARKET\"],\n        ...     min_tone=0.0,  # Only positive tone\n        ... )\n        &gt;&gt;&gt; result = await endpoint.query(filter_obj)\n        &gt;&gt;&gt; print(f\"Fetched {len(result)} records\")\n        &gt;&gt;&gt; if not result.complete:\n        ...     print(f\"Warning: {result.total_failed} requests failed\")\n        &gt;&gt;&gt; for record in result:\n        ...     print(record.record_id, record.tone.tone if record.tone else None)\n    \"\"\"\n    records: list[GKGRecord] = [\n        record async for record in self.stream(filter_obj, use_bigquery=use_bigquery)\n    ]\n\n    logger.info(\"GKG query completed: %d records fetched\", len(records))\n\n    # Return FetchResult (failures tracked by DataFetcher error policy)\n    return FetchResult(data=records, failed=[])\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.gkg.GKGEndpoint.stream","title":"<code>stream(filter_obj, *, use_bigquery=False)</code>  <code>async</code>","text":"<p>Stream GKG records with automatic fallback.</p> <p>This method streams GKG records one at a time, which is memory-efficient for large result sets. Records are converted from internal _RawGKG dataclass to public GKGRecord Pydantic model at the yield boundary.</p> <p>Files are always tried first (free, no credentials), with automatic fallback to BigQuery on rate limit/error if credentials are configured.</p> <p>Parameters:</p> Name Type Description Default <code>filter_obj</code> <code>GKGFilter</code> <p>GKG filter with date range and query parameters</p> required <code>use_bigquery</code> <code>bool</code> <p>If True, skip files and use BigQuery directly (default: False)</p> <code>False</code> <p>Yields:</p> Name Type Description <code>GKGRecord</code> <code>AsyncIterator[GKGRecord]</code> <p>Individual GKG records matching the filter criteria</p> <p>Raises:</p> Type Description <code>RateLimitError</code> <p>If rate limited and fallback not available/enabled</p> <code>APIError</code> <p>If download fails and fallback not available/enabled</p> <code>ConfigurationError</code> <p>If BigQuery requested but not configured</p> Example <p>from datetime import date from py_gdelt.filters import GKGFilter, DateRange</p> <p>filter_obj = GKGFilter( ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7)), ...     organizations=[\"United Nations\"], ... ) count = 0 async for record in endpoint.stream(filter_obj): ...     print(f\"Processing {record.record_id}\") ...     count += 1 ...     if count &gt;= 1000: ...         break  # Stop after 1000 records</p> Source code in <code>src/py_gdelt/endpoints/gkg.py</code> <pre><code>async def stream(\n    self,\n    filter_obj: GKGFilter,\n    *,\n    use_bigquery: bool = False,\n) -&gt; AsyncIterator[GKGRecord]:\n    \"\"\"Stream GKG records with automatic fallback.\n\n    This method streams GKG records one at a time, which is memory-efficient for\n    large result sets. Records are converted from internal _RawGKG dataclass to\n    public GKGRecord Pydantic model at the yield boundary.\n\n    Files are always tried first (free, no credentials), with automatic fallback\n    to BigQuery on rate limit/error if credentials are configured.\n\n    Args:\n        filter_obj: GKG filter with date range and query parameters\n        use_bigquery: If True, skip files and use BigQuery directly (default: False)\n\n    Yields:\n        GKGRecord: Individual GKG records matching the filter criteria\n\n    Raises:\n        RateLimitError: If rate limited and fallback not available/enabled\n        APIError: If download fails and fallback not available/enabled\n        ConfigurationError: If BigQuery requested but not configured\n\n    Example:\n        &gt;&gt;&gt; from datetime import date\n        &gt;&gt;&gt; from py_gdelt.filters import GKGFilter, DateRange\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; filter_obj = GKGFilter(\n        ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7)),\n        ...     organizations=[\"United Nations\"],\n        ... )\n        &gt;&gt;&gt; count = 0\n        &gt;&gt;&gt; async for record in endpoint.stream(filter_obj):\n        ...     print(f\"Processing {record.record_id}\")\n        ...     count += 1\n        ...     if count &gt;= 1000:\n        ...         break  # Stop after 1000 records\n    \"\"\"\n    logger.debug(\"Starting GKG stream for filter: %s\", filter_obj)\n\n    # Use DataFetcher to fetch raw GKG records\n    async for raw_gkg in self._fetcher.fetch_gkg(filter_obj, use_bigquery=use_bigquery):\n        # Convert _RawGKG to GKGRecord at yield boundary\n        try:\n            record = GKGRecord.from_raw(raw_gkg)\n            yield record\n        except Exception as e:  # noqa: BLE001\n            # Error boundary: log conversion errors but continue processing other records\n            logger.warning(\"Failed to convert raw GKG record to GKGRecord: %s\", e)\n            continue\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.gkg.GKGEndpoint.query_sync","title":"<code>query_sync(filter_obj, *, use_bigquery=False)</code>","text":"<p>Synchronous wrapper for query().</p> <p>This is a convenience method for synchronous code that internally uses asyncio.run() to execute the async query() method.</p> <p>Parameters:</p> Name Type Description Default <code>filter_obj</code> <code>GKGFilter</code> <p>GKG filter with date range and query parameters</p> required <code>use_bigquery</code> <code>bool</code> <p>If True, skip files and use BigQuery directly (default: False)</p> <code>False</code> <p>Returns:</p> Type Description <code>FetchResult[GKGRecord]</code> <p>FetchResult[GKGRecord] containing all matching records and any failures</p> <p>Raises:</p> Type Description <code>RateLimitError</code> <p>If rate limited and fallback not available/enabled</p> <code>APIError</code> <p>If download fails and fallback not available/enabled</p> <code>ConfigurationError</code> <p>If BigQuery requested but not configured</p> <code>RuntimeError</code> <p>If called from within an existing event loop</p> Example <p>from datetime import date from py_gdelt.filters import GKGFilter, DateRange</p> Source code in <code>src/py_gdelt/endpoints/gkg.py</code> <pre><code>def query_sync(\n    self,\n    filter_obj: GKGFilter,\n    *,\n    use_bigquery: bool = False,\n) -&gt; FetchResult[GKGRecord]:\n    \"\"\"Synchronous wrapper for query().\n\n    This is a convenience method for synchronous code that internally uses\n    asyncio.run() to execute the async query() method.\n\n    Args:\n        filter_obj: GKG filter with date range and query parameters\n        use_bigquery: If True, skip files and use BigQuery directly (default: False)\n\n    Returns:\n        FetchResult[GKGRecord] containing all matching records and any failures\n\n    Raises:\n        RateLimitError: If rate limited and fallback not available/enabled\n        APIError: If download fails and fallback not available/enabled\n        ConfigurationError: If BigQuery requested but not configured\n        RuntimeError: If called from within an existing event loop\n\n    Example:\n        &gt;&gt;&gt; from datetime import date\n        &gt;&gt;&gt; from py_gdelt.filters import GKGFilter, DateRange\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Synchronous usage (no async/await needed)\n        &gt;&gt;&gt; endpoint = GKGEndpoint(file_source=file_source)\n        &gt;&gt;&gt; filter_obj = GKGFilter(\n        ...     date_range=DateRange(start=date(2024, 1, 1))\n        ... )\n        &gt;&gt;&gt; result = endpoint.query_sync(filter_obj)\n        &gt;&gt;&gt; for record in result:\n        ...     print(record.record_id)\n    \"\"\"\n    return asyncio.run(self.query(filter_obj, use_bigquery=use_bigquery))\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.gkg.GKGEndpoint.query_sync--synchronous-usage-no-asyncawait-needed","title":"Synchronous usage (no async/await needed)","text":"<p>endpoint = GKGEndpoint(file_source=file_source) filter_obj = GKGFilter( ...     date_range=DateRange(start=date(2024, 1, 1)) ... ) result = endpoint.query_sync(filter_obj) for record in result: ...     print(record.record_id)</p>"},{"location":"api/endpoints/#py_gdelt.endpoints.gkg.GKGEndpoint.stream_sync","title":"<code>stream_sync(filter_obj, *, use_bigquery=False)</code>","text":"<p>Synchronous wrapper for stream().</p> <p>This method provides a synchronous iterator interface over async streaming. It internally manages the event loop and yields records one at a time.</p> <p>Note: This creates a new event loop for each iteration, which has some overhead. For better performance, use the async stream() method directly if possible.</p> <p>Parameters:</p> Name Type Description Default <code>filter_obj</code> <code>GKGFilter</code> <p>GKG filter with date range and query parameters</p> required <code>use_bigquery</code> <code>bool</code> <p>If True, skip files and use BigQuery directly (default: False)</p> <code>False</code> <p>Returns:</p> Type Description <code>Iterator[GKGRecord]</code> <p>Iterator of GKGRecord instances for each matching record</p> <p>Raises:</p> Type Description <code>RateLimitError</code> <p>If rate limited and fallback not available/enabled</p> <code>APIError</code> <p>If download fails and fallback not available/enabled</p> <code>ConfigurationError</code> <p>If BigQuery requested but not configured</p> <code>RuntimeError</code> <p>If called from within an existing event loop</p> Example <p>from datetime import date from py_gdelt.filters import GKGFilter, DateRange</p> Source code in <code>src/py_gdelt/endpoints/gkg.py</code> <pre><code>def stream_sync(\n    self,\n    filter_obj: GKGFilter,\n    *,\n    use_bigquery: bool = False,\n) -&gt; Iterator[GKGRecord]:\n    \"\"\"Synchronous wrapper for stream().\n\n    This method provides a synchronous iterator interface over async streaming.\n    It internally manages the event loop and yields records one at a time.\n\n    Note: This creates a new event loop for each iteration, which has some overhead.\n    For better performance, use the async stream() method directly if possible.\n\n    Args:\n        filter_obj: GKG filter with date range and query parameters\n        use_bigquery: If True, skip files and use BigQuery directly (default: False)\n\n    Returns:\n        Iterator of GKGRecord instances for each matching record\n\n    Raises:\n        RateLimitError: If rate limited and fallback not available/enabled\n        APIError: If download fails and fallback not available/enabled\n        ConfigurationError: If BigQuery requested but not configured\n        RuntimeError: If called from within an existing event loop\n\n    Example:\n        &gt;&gt;&gt; from datetime import date\n        &gt;&gt;&gt; from py_gdelt.filters import GKGFilter, DateRange\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Synchronous streaming (no async/await needed)\n        &gt;&gt;&gt; endpoint = GKGEndpoint(file_source=file_source)\n        &gt;&gt;&gt; filter_obj = GKGFilter(\n        ...     date_range=DateRange(start=date(2024, 1, 1))\n        ... )\n        &gt;&gt;&gt; for record in endpoint.stream_sync(filter_obj):\n        ...     print(record.record_id)\n        ...     if record.has_quotations:\n        ...         print(f\"  {len(record.quotations)} quotations found\")\n    \"\"\"\n\n    async def _async_generator() -&gt; AsyncIterator[GKGRecord]:\n        \"\"\"Internal async generator for sync wrapper.\"\"\"\n        async for record in self.stream(filter_obj, use_bigquery=use_bigquery):\n            yield record\n\n    # Run async generator and yield results synchronously\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    try:\n        async_gen = _async_generator()\n        while True:\n            try:\n                record = loop.run_until_complete(async_gen.__anext__())\n                yield record\n            except StopAsyncIteration:\n                break\n    finally:\n        loop.close()\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.gkg.GKGEndpoint.stream_sync--synchronous-streaming-no-asyncawait-needed","title":"Synchronous streaming (no async/await needed)","text":"<p>endpoint = GKGEndpoint(file_source=file_source) filter_obj = GKGFilter( ...     date_range=DateRange(start=date(2024, 1, 1)) ... ) for record in endpoint.stream_sync(filter_obj): ...     print(record.record_id) ...     if record.has_quotations: ...         print(f\"  {len(record.quotations)} quotations found\")</p>"},{"location":"api/endpoints/#ngramsendpoint","title":"NGramsEndpoint","text":""},{"location":"api/endpoints/#py_gdelt.endpoints.ngrams.NGramsEndpoint","title":"<code>NGramsEndpoint</code>","text":"<p>Endpoint for querying GDELT NGrams 3.0 data.</p> <p>Provides access to GDELT's NGrams dataset, which tracks word and phrase occurrences across global news with contextual information. NGrams are file-based only (no BigQuery support).</p> <p>The endpoint uses DataFetcher for orchestrated file downloads with automatic retry, error handling, and intelligent caching. Internal _RawNGram dataclass instances are converted to Pydantic NGramRecord models at the yield boundary.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>GDELTSettings | None</code> <p>Configuration settings. If None, uses defaults.</p> <code>None</code> <code>file_source</code> <code>FileSource | None</code> <p>Optional shared FileSource. If None, creates owned instance.         When provided, the source lifecycle is managed externally.</p> <code>None</code> Example <p>Batch query with filtering:</p> <p>from py_gdelt.filters import NGramsFilter, DateRange from datetime import date async with NGramsEndpoint() as endpoint: ...     filter_obj = NGramsFilter( ...         date_range=DateRange(start=date(2024, 1, 1)), ...         language=\"en\", ...         ngram=\"climate\", ...     ) ...     result = await endpoint.query(filter_obj) ...     print(f\"Found {len(result)} records\")</p> <p>Streaming for large datasets:</p> <p>async with NGramsEndpoint() as endpoint: ...     filter_obj = NGramsFilter( ...         date_range=DateRange( ...             start=date(2024, 1, 1), ...             end=date(2024, 1, 7) ...         ), ...         language=\"en\", ...     ) ...     async for record in endpoint.stream(filter_obj): ...         if record.is_early_in_article: ...             print(f\"Early: {record.ngram} in {record.url}\")</p> Source code in <code>src/py_gdelt/endpoints/ngrams.py</code> <pre><code>class NGramsEndpoint:\n    \"\"\"Endpoint for querying GDELT NGrams 3.0 data.\n\n    Provides access to GDELT's NGrams dataset, which tracks word and phrase\n    occurrences across global news with contextual information. NGrams are\n    file-based only (no BigQuery support).\n\n    The endpoint uses DataFetcher for orchestrated file downloads with automatic\n    retry, error handling, and intelligent caching. Internal _RawNGram dataclass\n    instances are converted to Pydantic NGramRecord models at the yield boundary.\n\n    Args:\n        settings: Configuration settings. If None, uses defaults.\n        file_source: Optional shared FileSource. If None, creates owned instance.\n                    When provided, the source lifecycle is managed externally.\n\n    Example:\n        Batch query with filtering:\n\n        &gt;&gt;&gt; from py_gdelt.filters import NGramsFilter, DateRange\n        &gt;&gt;&gt; from datetime import date\n        &gt;&gt;&gt; async with NGramsEndpoint() as endpoint:\n        ...     filter_obj = NGramsFilter(\n        ...         date_range=DateRange(start=date(2024, 1, 1)),\n        ...         language=\"en\",\n        ...         ngram=\"climate\",\n        ...     )\n        ...     result = await endpoint.query(filter_obj)\n        ...     print(f\"Found {len(result)} records\")\n\n        Streaming for large datasets:\n\n        &gt;&gt;&gt; async with NGramsEndpoint() as endpoint:\n        ...     filter_obj = NGramsFilter(\n        ...         date_range=DateRange(\n        ...             start=date(2024, 1, 1),\n        ...             end=date(2024, 1, 7)\n        ...         ),\n        ...         language=\"en\",\n        ...     )\n        ...     async for record in endpoint.stream(filter_obj):\n        ...         if record.is_early_in_article:\n        ...             print(f\"Early: {record.ngram} in {record.url}\")\n    \"\"\"\n\n    def __init__(\n        self,\n        settings: GDELTSettings | None = None,\n        file_source: FileSource | None = None,\n    ) -&gt; None:\n        self.settings = settings or GDELTSettings()\n\n        if file_source is not None:\n            self._file_source = file_source\n            self._owns_sources = False\n        else:\n            self._file_source = FileSource(settings=self.settings)\n            self._owns_sources = True\n\n        # Create DataFetcher with file source only (NGrams don't support BigQuery)\n        self._fetcher = DataFetcher(\n            file_source=self._file_source,\n            bigquery_source=None,\n            fallback_enabled=False,\n            error_policy=\"warn\",\n        )\n\n        # Create parser instance\n        self._parser = NGramsParser()\n\n    async def close(self) -&gt; None:\n        \"\"\"Close resources if we own them.\n\n        Only closes resources that were created by this instance.\n        Shared resources are not closed to allow reuse.\n        \"\"\"\n        if self._owns_sources:\n            # FileSource uses context manager protocol, manually call __aexit__\n            await self._file_source.__aexit__(None, None, None)\n\n    async def __aenter__(self) -&gt; NGramsEndpoint:\n        \"\"\"Async context manager entry.\n\n        Returns:\n            Self for use in async with statement.\n        \"\"\"\n        return self\n\n    async def __aexit__(self, *args: object) -&gt; None:\n        \"\"\"Async context manager exit - close resources.\n\n        Args:\n            *args: Exception info (unused, but required by protocol).\n        \"\"\"\n        await self.close()\n\n    async def query(self, filter_obj: NGramsFilter) -&gt; FetchResult[NGramRecord]:\n        \"\"\"Query NGrams data and return all results.\n\n        Fetches all NGram records matching the filter criteria and returns them\n        as a FetchResult. This method collects all records in memory before returning,\n        so use stream() for large result sets to avoid memory issues.\n\n        Args:\n            filter_obj: Filter with date range and optional ngram/language constraints\n\n        Returns:\n            FetchResult containing list of NGramRecord instances and any failed requests\n\n        Raises:\n            RateLimitError: If rate limited and retries exhausted\n            APIError: If downloads fail\n            DataError: If file parsing fails\n\n        Example:\n            &gt;&gt;&gt; filter_obj = NGramsFilter(\n            ...     date_range=DateRange(start=date(2024, 1, 1)),\n            ...     language=\"en\",\n            ...     min_position=0,\n            ...     max_position=20,\n            ... )\n            &gt;&gt;&gt; result = await endpoint.query(filter_obj)\n            &gt;&gt;&gt; print(f\"Found {len(result)} records in article headlines\")\n        \"\"\"\n        # Stream all records and collect them\n        records: list[NGramRecord] = [record async for record in self.stream(filter_obj)]\n\n        logger.info(\"Collected %d NGram records from query\", len(records))\n\n        # Return FetchResult (no failed tracking for now - DataFetcher handles errors)\n        return FetchResult(data=records, failed=[])\n\n    async def stream(self, filter_obj: NGramsFilter) -&gt; AsyncIterator[NGramRecord]:\n        \"\"\"Stream NGrams data record by record.\n\n        Yields NGram records one at a time, converting internal _RawNGram dataclass\n        instances to Pydantic NGramRecord models at the yield boundary. This method\n        is memory-efficient for large result sets.\n\n        Client-side filtering is applied for ngram text, language, and position\n        constraints since file downloads provide all records for a date range.\n\n        Args:\n            filter_obj: Filter with date range and optional ngram/language constraints\n\n        Yields:\n            NGramRecord: Individual NGram records matching the filter criteria\n\n        Raises:\n            RateLimitError: If rate limited and retries exhausted\n            APIError: If downloads fail\n            DataError: If file parsing fails\n\n        Example:\n            &gt;&gt;&gt; filter_obj = NGramsFilter(\n            ...     date_range=DateRange(start=date(2024, 1, 1)),\n            ...     ngram=\"climate\",\n            ...     language=\"en\",\n            ... )\n            &gt;&gt;&gt; async for record in endpoint.stream(filter_obj):\n            ...     print(f\"{record.ngram}: {record.context}\")\n        \"\"\"\n        # Use DataFetcher's fetch_ngrams method to get raw records\n        async for raw_record in self._fetcher.fetch_ngrams(filter_obj):\n            # Convert _RawNGram to NGramRecord (type conversion happens here)\n            try:\n                record = NGramRecord.from_raw(raw_record)\n            except Exception as e:  # noqa: BLE001\n                logger.warning(\n                    \"Failed to convert raw ngram to NGramRecord: %s - Skipping\",\n                    e,\n                )\n                continue\n\n            # Apply client-side filtering\n            if not self._matches_filter(record, filter_obj):\n                continue\n\n            yield record\n\n    def _matches_filter(self, record: NGramRecord, filter_obj: NGramsFilter) -&gt; bool:\n        \"\"\"Check if record matches filter criteria.\n\n        Applies client-side filtering for ngram text, language, and position\n        constraints. Date filtering is handled by DataFetcher (file selection).\n\n        Args:\n            record: NGramRecord to check\n            filter_obj: Filter criteria\n\n        Returns:\n            True if record matches all filter criteria, False otherwise\n        \"\"\"\n        # Filter by ngram text (case-insensitive substring match)\n        if filter_obj.ngram is not None and filter_obj.ngram.lower() not in record.ngram.lower():\n            return False\n\n        # Filter by language (exact match)\n        if filter_obj.language is not None and record.language != filter_obj.language:\n            return False\n\n        # Filter by position (article decile)\n        if filter_obj.min_position is not None and record.position &lt; filter_obj.min_position:\n            return False\n\n        return not (\n            filter_obj.max_position is not None and record.position &gt; filter_obj.max_position\n        )\n\n    def query_sync(self, filter_obj: NGramsFilter) -&gt; FetchResult[NGramRecord]:\n        \"\"\"Synchronous wrapper for query().\n\n        Runs the async query() method in a new event loop. This is a convenience\n        method for synchronous code, but async methods are preferred when possible.\n\n        Args:\n            filter_obj: Filter with date range and optional constraints\n\n        Returns:\n            FetchResult containing list of NGramRecord instances\n\n        Raises:\n            RateLimitError: If rate limited and retries exhausted\n            APIError: If downloads fail\n            DataError: If file parsing fails\n\n        Example:\n            &gt;&gt;&gt; filter_obj = NGramsFilter(\n            ...     date_range=DateRange(start=date(2024, 1, 1)),\n            ...     language=\"en\",\n            ... )\n            &gt;&gt;&gt; result = endpoint.query_sync(filter_obj)\n            &gt;&gt;&gt; print(f\"Found {len(result)} records\")\n        \"\"\"\n        return asyncio.run(self.query(filter_obj))\n\n    def stream_sync(self, filter_obj: NGramsFilter) -&gt; Iterator[NGramRecord]:\n        \"\"\"Synchronous wrapper for stream().\n\n        Yields NGram records from the async stream() method in a blocking manner.\n        This is a convenience method for synchronous code, but async methods are\n        preferred when possible.\n\n        Args:\n            filter_obj: Filter with date range and optional constraints\n\n        Yields:\n            NGramRecord: Individual NGram records matching the filter criteria\n\n        Raises:\n            RuntimeError: If called from within a running event loop.\n            RateLimitError: If rate limited and retries exhausted.\n            APIError: If downloads fail.\n            DataError: If file parsing fails.\n\n        Note:\n            This method cannot be called from within an async context (e.g., inside\n            an async function or running event loop). Doing so will raise RuntimeError.\n            Use the async stream() method instead. This method creates its own event\n            loop internally.\n\n        Example:\n            &gt;&gt;&gt; filter_obj = NGramsFilter(\n            ...     date_range=DateRange(start=date(2024, 1, 1)),\n            ...     ngram=\"climate\",\n            ... )\n            &gt;&gt;&gt; for record in endpoint.stream_sync(filter_obj):\n            ...     print(f\"{record.ngram}: {record.url}\")\n        \"\"\"\n        # Manual event loop management is required for async generators.\n        # Unlike query_sync() which uses asyncio.run() for a single coroutine,\n        # stream_sync() must iterate through an async generator step-by-step.\n        # asyncio.run() cannot handle async generators - it expects a coroutine\n        # that returns a value, not one that yields multiple values.\n\n        # Check if we're already in an async context - this would cause issues\n        try:\n            asyncio.get_running_loop()\n            has_running_loop = True\n        except RuntimeError:\n            has_running_loop = False\n\n        if has_running_loop:\n            msg = \"stream_sync() cannot be called from a running event loop. Use stream() instead.\"\n            raise RuntimeError(msg)\n\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        try:\n            async_gen = self.stream(filter_obj)\n            while True:\n                try:\n                    record = loop.run_until_complete(async_gen.__anext__())\n                    yield record\n                except StopAsyncIteration:\n                    break\n        finally:\n            loop.close()\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.ngrams.NGramsEndpoint.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close resources if we own them.</p> <p>Only closes resources that were created by this instance. Shared resources are not closed to allow reuse.</p> Source code in <code>src/py_gdelt/endpoints/ngrams.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close resources if we own them.\n\n    Only closes resources that were created by this instance.\n    Shared resources are not closed to allow reuse.\n    \"\"\"\n    if self._owns_sources:\n        # FileSource uses context manager protocol, manually call __aexit__\n        await self._file_source.__aexit__(None, None, None)\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.ngrams.NGramsEndpoint.__aenter__","title":"<code>__aenter__()</code>  <code>async</code>","text":"<p>Async context manager entry.</p> <p>Returns:</p> Type Description <code>NGramsEndpoint</code> <p>Self for use in async with statement.</p> Source code in <code>src/py_gdelt/endpoints/ngrams.py</code> <pre><code>async def __aenter__(self) -&gt; NGramsEndpoint:\n    \"\"\"Async context manager entry.\n\n    Returns:\n        Self for use in async with statement.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.ngrams.NGramsEndpoint.__aexit__","title":"<code>__aexit__(*args)</code>  <code>async</code>","text":"<p>Async context manager exit - close resources.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>object</code> <p>Exception info (unused, but required by protocol).</p> <code>()</code> Source code in <code>src/py_gdelt/endpoints/ngrams.py</code> <pre><code>async def __aexit__(self, *args: object) -&gt; None:\n    \"\"\"Async context manager exit - close resources.\n\n    Args:\n        *args: Exception info (unused, but required by protocol).\n    \"\"\"\n    await self.close()\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.ngrams.NGramsEndpoint.query","title":"<code>query(filter_obj)</code>  <code>async</code>","text":"<p>Query NGrams data and return all results.</p> <p>Fetches all NGram records matching the filter criteria and returns them as a FetchResult. This method collects all records in memory before returning, so use stream() for large result sets to avoid memory issues.</p> <p>Parameters:</p> Name Type Description Default <code>filter_obj</code> <code>NGramsFilter</code> <p>Filter with date range and optional ngram/language constraints</p> required <p>Returns:</p> Type Description <code>FetchResult[NGramRecord]</code> <p>FetchResult containing list of NGramRecord instances and any failed requests</p> <p>Raises:</p> Type Description <code>RateLimitError</code> <p>If rate limited and retries exhausted</p> <code>APIError</code> <p>If downloads fail</p> <code>DataError</code> <p>If file parsing fails</p> Example <p>filter_obj = NGramsFilter( ...     date_range=DateRange(start=date(2024, 1, 1)), ...     language=\"en\", ...     min_position=0, ...     max_position=20, ... ) result = await endpoint.query(filter_obj) print(f\"Found {len(result)} records in article headlines\")</p> Source code in <code>src/py_gdelt/endpoints/ngrams.py</code> <pre><code>async def query(self, filter_obj: NGramsFilter) -&gt; FetchResult[NGramRecord]:\n    \"\"\"Query NGrams data and return all results.\n\n    Fetches all NGram records matching the filter criteria and returns them\n    as a FetchResult. This method collects all records in memory before returning,\n    so use stream() for large result sets to avoid memory issues.\n\n    Args:\n        filter_obj: Filter with date range and optional ngram/language constraints\n\n    Returns:\n        FetchResult containing list of NGramRecord instances and any failed requests\n\n    Raises:\n        RateLimitError: If rate limited and retries exhausted\n        APIError: If downloads fail\n        DataError: If file parsing fails\n\n    Example:\n        &gt;&gt;&gt; filter_obj = NGramsFilter(\n        ...     date_range=DateRange(start=date(2024, 1, 1)),\n        ...     language=\"en\",\n        ...     min_position=0,\n        ...     max_position=20,\n        ... )\n        &gt;&gt;&gt; result = await endpoint.query(filter_obj)\n        &gt;&gt;&gt; print(f\"Found {len(result)} records in article headlines\")\n    \"\"\"\n    # Stream all records and collect them\n    records: list[NGramRecord] = [record async for record in self.stream(filter_obj)]\n\n    logger.info(\"Collected %d NGram records from query\", len(records))\n\n    # Return FetchResult (no failed tracking for now - DataFetcher handles errors)\n    return FetchResult(data=records, failed=[])\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.ngrams.NGramsEndpoint.stream","title":"<code>stream(filter_obj)</code>  <code>async</code>","text":"<p>Stream NGrams data record by record.</p> <p>Yields NGram records one at a time, converting internal _RawNGram dataclass instances to Pydantic NGramRecord models at the yield boundary. This method is memory-efficient for large result sets.</p> <p>Client-side filtering is applied for ngram text, language, and position constraints since file downloads provide all records for a date range.</p> <p>Parameters:</p> Name Type Description Default <code>filter_obj</code> <code>NGramsFilter</code> <p>Filter with date range and optional ngram/language constraints</p> required <p>Yields:</p> Name Type Description <code>NGramRecord</code> <code>AsyncIterator[NGramRecord]</code> <p>Individual NGram records matching the filter criteria</p> <p>Raises:</p> Type Description <code>RateLimitError</code> <p>If rate limited and retries exhausted</p> <code>APIError</code> <p>If downloads fail</p> <code>DataError</code> <p>If file parsing fails</p> Example <p>filter_obj = NGramsFilter( ...     date_range=DateRange(start=date(2024, 1, 1)), ...     ngram=\"climate\", ...     language=\"en\", ... ) async for record in endpoint.stream(filter_obj): ...     print(f\"{record.ngram}: {record.context}\")</p> Source code in <code>src/py_gdelt/endpoints/ngrams.py</code> <pre><code>async def stream(self, filter_obj: NGramsFilter) -&gt; AsyncIterator[NGramRecord]:\n    \"\"\"Stream NGrams data record by record.\n\n    Yields NGram records one at a time, converting internal _RawNGram dataclass\n    instances to Pydantic NGramRecord models at the yield boundary. This method\n    is memory-efficient for large result sets.\n\n    Client-side filtering is applied for ngram text, language, and position\n    constraints since file downloads provide all records for a date range.\n\n    Args:\n        filter_obj: Filter with date range and optional ngram/language constraints\n\n    Yields:\n        NGramRecord: Individual NGram records matching the filter criteria\n\n    Raises:\n        RateLimitError: If rate limited and retries exhausted\n        APIError: If downloads fail\n        DataError: If file parsing fails\n\n    Example:\n        &gt;&gt;&gt; filter_obj = NGramsFilter(\n        ...     date_range=DateRange(start=date(2024, 1, 1)),\n        ...     ngram=\"climate\",\n        ...     language=\"en\",\n        ... )\n        &gt;&gt;&gt; async for record in endpoint.stream(filter_obj):\n        ...     print(f\"{record.ngram}: {record.context}\")\n    \"\"\"\n    # Use DataFetcher's fetch_ngrams method to get raw records\n    async for raw_record in self._fetcher.fetch_ngrams(filter_obj):\n        # Convert _RawNGram to NGramRecord (type conversion happens here)\n        try:\n            record = NGramRecord.from_raw(raw_record)\n        except Exception as e:  # noqa: BLE001\n            logger.warning(\n                \"Failed to convert raw ngram to NGramRecord: %s - Skipping\",\n                e,\n            )\n            continue\n\n        # Apply client-side filtering\n        if not self._matches_filter(record, filter_obj):\n            continue\n\n        yield record\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.ngrams.NGramsEndpoint.query_sync","title":"<code>query_sync(filter_obj)</code>","text":"<p>Synchronous wrapper for query().</p> <p>Runs the async query() method in a new event loop. This is a convenience method for synchronous code, but async methods are preferred when possible.</p> <p>Parameters:</p> Name Type Description Default <code>filter_obj</code> <code>NGramsFilter</code> <p>Filter with date range and optional constraints</p> required <p>Returns:</p> Type Description <code>FetchResult[NGramRecord]</code> <p>FetchResult containing list of NGramRecord instances</p> <p>Raises:</p> Type Description <code>RateLimitError</code> <p>If rate limited and retries exhausted</p> <code>APIError</code> <p>If downloads fail</p> <code>DataError</code> <p>If file parsing fails</p> Example <p>filter_obj = NGramsFilter( ...     date_range=DateRange(start=date(2024, 1, 1)), ...     language=\"en\", ... ) result = endpoint.query_sync(filter_obj) print(f\"Found {len(result)} records\")</p> Source code in <code>src/py_gdelt/endpoints/ngrams.py</code> <pre><code>def query_sync(self, filter_obj: NGramsFilter) -&gt; FetchResult[NGramRecord]:\n    \"\"\"Synchronous wrapper for query().\n\n    Runs the async query() method in a new event loop. This is a convenience\n    method for synchronous code, but async methods are preferred when possible.\n\n    Args:\n        filter_obj: Filter with date range and optional constraints\n\n    Returns:\n        FetchResult containing list of NGramRecord instances\n\n    Raises:\n        RateLimitError: If rate limited and retries exhausted\n        APIError: If downloads fail\n        DataError: If file parsing fails\n\n    Example:\n        &gt;&gt;&gt; filter_obj = NGramsFilter(\n        ...     date_range=DateRange(start=date(2024, 1, 1)),\n        ...     language=\"en\",\n        ... )\n        &gt;&gt;&gt; result = endpoint.query_sync(filter_obj)\n        &gt;&gt;&gt; print(f\"Found {len(result)} records\")\n    \"\"\"\n    return asyncio.run(self.query(filter_obj))\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.ngrams.NGramsEndpoint.stream_sync","title":"<code>stream_sync(filter_obj)</code>","text":"<p>Synchronous wrapper for stream().</p> <p>Yields NGram records from the async stream() method in a blocking manner. This is a convenience method for synchronous code, but async methods are preferred when possible.</p> <p>Parameters:</p> Name Type Description Default <code>filter_obj</code> <code>NGramsFilter</code> <p>Filter with date range and optional constraints</p> required <p>Yields:</p> Name Type Description <code>NGramRecord</code> <code>NGramRecord</code> <p>Individual NGram records matching the filter criteria</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called from within a running event loop.</p> <code>RateLimitError</code> <p>If rate limited and retries exhausted.</p> <code>APIError</code> <p>If downloads fail.</p> <code>DataError</code> <p>If file parsing fails.</p> Note <p>This method cannot be called from within an async context (e.g., inside an async function or running event loop). Doing so will raise RuntimeError. Use the async stream() method instead. This method creates its own event loop internally.</p> Example <p>filter_obj = NGramsFilter( ...     date_range=DateRange(start=date(2024, 1, 1)), ...     ngram=\"climate\", ... ) for record in endpoint.stream_sync(filter_obj): ...     print(f\"{record.ngram}: {record.url}\")</p> Source code in <code>src/py_gdelt/endpoints/ngrams.py</code> <pre><code>def stream_sync(self, filter_obj: NGramsFilter) -&gt; Iterator[NGramRecord]:\n    \"\"\"Synchronous wrapper for stream().\n\n    Yields NGram records from the async stream() method in a blocking manner.\n    This is a convenience method for synchronous code, but async methods are\n    preferred when possible.\n\n    Args:\n        filter_obj: Filter with date range and optional constraints\n\n    Yields:\n        NGramRecord: Individual NGram records matching the filter criteria\n\n    Raises:\n        RuntimeError: If called from within a running event loop.\n        RateLimitError: If rate limited and retries exhausted.\n        APIError: If downloads fail.\n        DataError: If file parsing fails.\n\n    Note:\n        This method cannot be called from within an async context (e.g., inside\n        an async function or running event loop). Doing so will raise RuntimeError.\n        Use the async stream() method instead. This method creates its own event\n        loop internally.\n\n    Example:\n        &gt;&gt;&gt; filter_obj = NGramsFilter(\n        ...     date_range=DateRange(start=date(2024, 1, 1)),\n        ...     ngram=\"climate\",\n        ... )\n        &gt;&gt;&gt; for record in endpoint.stream_sync(filter_obj):\n        ...     print(f\"{record.ngram}: {record.url}\")\n    \"\"\"\n    # Manual event loop management is required for async generators.\n    # Unlike query_sync() which uses asyncio.run() for a single coroutine,\n    # stream_sync() must iterate through an async generator step-by-step.\n    # asyncio.run() cannot handle async generators - it expects a coroutine\n    # that returns a value, not one that yields multiple values.\n\n    # Check if we're already in an async context - this would cause issues\n    try:\n        asyncio.get_running_loop()\n        has_running_loop = True\n    except RuntimeError:\n        has_running_loop = False\n\n    if has_running_loop:\n        msg = \"stream_sync() cannot be called from a running event loop. Use stream() instead.\"\n        raise RuntimeError(msg)\n\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    try:\n        async_gen = self.stream(filter_obj)\n        while True:\n            try:\n                record = loop.run_until_complete(async_gen.__anext__())\n                yield record\n            except StopAsyncIteration:\n                break\n    finally:\n        loop.close()\n</code></pre>"},{"location":"api/endpoints/#rest-api-endpoints","title":"REST API Endpoints","text":""},{"location":"api/endpoints/#docendpoint","title":"DocEndpoint","text":""},{"location":"api/endpoints/#py_gdelt.endpoints.doc.DocEndpoint","title":"<code>DocEndpoint</code>","text":"<p>               Bases: <code>BaseEndpoint</code></p> <p>DOC 2.0 API endpoint for searching GDELT articles.</p> <p>The DOC API provides full-text search across GDELT's monitored news sources with support for various output modes (article lists, timelines, galleries) and flexible filtering by time, source, language, and relevance.</p> <p>Attributes:</p> Name Type Description <code>BASE_URL</code> <p>Base URL for the DOC API endpoint</p> Example <p>Basic article search:</p> <p>async with DocEndpoint() as doc: ...     articles = await doc.search(\"climate change\", max_results=100) ...     for article in articles: ...         print(article.title, article.url)</p> <p>Using filters for advanced queries:</p> <p>from py_gdelt.filters import DocFilter async with DocEndpoint() as doc: ...     filter = DocFilter( ...         query=\"elections\", ...         timespan=\"7d\", ...         source_country=\"US\", ...         sort_by=\"relevance\" ...     ) ...     articles = await doc.query(filter)</p> <p>Getting timeline data:</p> <p>async with DocEndpoint() as doc: ...     timeline = await doc.timeline(\"protests\", timespan=\"30d\") ...     for point in timeline.points: ...         print(point.date, point.value)</p> Source code in <code>src/py_gdelt/endpoints/doc.py</code> <pre><code>class DocEndpoint(BaseEndpoint):\n    \"\"\"\n    DOC 2.0 API endpoint for searching GDELT articles.\n\n    The DOC API provides full-text search across GDELT's monitored news sources\n    with support for various output modes (article lists, timelines, galleries)\n    and flexible filtering by time, source, language, and relevance.\n\n    Attributes:\n        BASE_URL: Base URL for the DOC API endpoint\n\n    Example:\n        Basic article search:\n\n        &gt;&gt;&gt; async with DocEndpoint() as doc:\n        ...     articles = await doc.search(\"climate change\", max_results=100)\n        ...     for article in articles:\n        ...         print(article.title, article.url)\n\n        Using filters for advanced queries:\n\n        &gt;&gt;&gt; from py_gdelt.filters import DocFilter\n        &gt;&gt;&gt; async with DocEndpoint() as doc:\n        ...     filter = DocFilter(\n        ...         query=\"elections\",\n        ...         timespan=\"7d\",\n        ...         source_country=\"US\",\n        ...         sort_by=\"relevance\"\n        ...     )\n        ...     articles = await doc.query(filter)\n\n        Getting timeline data:\n\n        &gt;&gt;&gt; async with DocEndpoint() as doc:\n        ...     timeline = await doc.timeline(\"protests\", timespan=\"30d\")\n        ...     for point in timeline.points:\n        ...         print(point.date, point.value)\n    \"\"\"\n\n    BASE_URL = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n\n    async def _build_url(self, **kwargs: Any) -&gt; str:\n        \"\"\"Build DOC API URL.\n\n        The DOC API uses a fixed URL with all parameters passed as query strings.\n\n        Args:\n            **kwargs: Unused, but required by BaseEndpoint interface.\n\n        Returns:\n            Base URL for DOC API requests.\n        \"\"\"\n        return self.BASE_URL\n\n    def _build_params(self, query_filter: DocFilter) -&gt; dict[str, str]:\n        \"\"\"\n        Build query parameters from DocFilter.\n\n        Converts the DocFilter model into URL query parameters expected by\n        the DOC API, including proper format conversion for dates and\n        mapping of sort values to API parameter names.\n\n        Args:\n            query_filter: Query filter with search parameters.\n\n        Returns:\n            Dict of query parameters ready for API request.\n\n        Example:\n            &gt;&gt;&gt; filter = DocFilter(query=\"test\", timespan=\"24h\", sort_by=\"relevance\")\n            &gt;&gt;&gt; endpoint = DocEndpoint()\n            &gt;&gt;&gt; params = endpoint._build_params(filter)\n            &gt;&gt;&gt; params[\"query\"]\n            'test'\n            &gt;&gt;&gt; params[\"sort\"]\n            'rel'\n        \"\"\"\n        params: dict[str, str] = {\n            \"query\": query_filter.query,\n            \"format\": \"json\",\n            \"mode\": query_filter.mode,\n            \"maxrecords\": str(query_filter.max_results),\n        }\n\n        # Map sort values to API parameter names\n        sort_map = {\n            \"date\": \"date\",\n            \"relevance\": \"rel\",\n            \"tone\": \"tonedesc\",\n        }\n        params[\"sort\"] = sort_map[query_filter.sort_by]\n\n        # Time constraints - timespan takes precedence over datetime range\n        if query_filter.timespan:\n            params[\"timespan\"] = query_filter.timespan\n        elif query_filter.start_datetime:\n            params[\"startdatetime\"] = query_filter.start_datetime.strftime(\"%Y%m%d%H%M%S\")\n            if query_filter.end_datetime:\n                params[\"enddatetime\"] = query_filter.end_datetime.strftime(\"%Y%m%d%H%M%S\")\n\n        # Source filters\n        if query_filter.source_language:\n            params[\"sourcelang\"] = query_filter.source_language\n        if query_filter.source_country:\n            params[\"sourcecountry\"] = query_filter.source_country\n\n        return params\n\n    async def search(\n        self,\n        query: str,\n        *,\n        timespan: str | None = None,\n        max_results: int = 250,\n        sort_by: Literal[\"date\", \"relevance\", \"tone\"] = \"date\",\n        source_language: str | None = None,\n        source_country: str | None = None,\n    ) -&gt; list[Article]:\n        \"\"\"\n        Search for articles matching a query.\n\n        This is a convenience method that constructs a DocFilter internally.\n        For more control over query parameters, use query() with a DocFilter directly.\n\n        Args:\n            query: Search query string (supports boolean operators, phrases).\n            timespan: Time range like \"24h\", \"7d\", \"30d\". If None, searches all time.\n            max_results: Maximum results to return (1-250, default: 250).\n            sort_by: Sort order - \"date\", \"relevance\", or \"tone\" (default: \"date\").\n            source_language: Filter by source language (ISO 639 code).\n            source_country: Filter by source country (FIPS country code).\n\n        Returns:\n            List of Article objects matching the query.\n\n        Raises:\n            APIError: On HTTP errors or invalid responses.\n            APIUnavailableError: When API is down or unreachable.\n            RateLimitError: When rate limited by the API.\n\n        Example:\n            &gt;&gt;&gt; async with DocEndpoint() as doc:\n            ...     # Search recent articles about climate\n            ...     articles = await doc.search(\n            ...         \"climate change\",\n            ...         timespan=\"7d\",\n            ...         max_results=50,\n            ...         sort_by=\"relevance\"\n            ...     )\n            ...     # Filter by country\n            ...     us_articles = await doc.search(\n            ...         \"elections\",\n            ...         source_country=\"US\",\n            ...         timespan=\"24h\"\n            ...     )\n        \"\"\"\n        query_filter = DocFilter(\n            query=query,\n            timespan=timespan,\n            max_results=max_results,\n            sort_by=sort_by,\n            source_language=source_language,\n            source_country=source_country,\n        )\n        return await self.query(query_filter)\n\n    async def query(self, query_filter: DocFilter) -&gt; list[Article]:\n        \"\"\"\n        Query the DOC API with a filter.\n\n        Executes a search using a pre-configured DocFilter object, providing\n        full control over all query parameters.\n\n        Args:\n            query_filter: DocFilter with query parameters and constraints.\n\n        Returns:\n            List of Article objects matching the filter criteria.\n\n        Raises:\n            APIError: On HTTP errors or invalid responses.\n            APIUnavailableError: When API is down or unreachable.\n            RateLimitError: When rate limited by the API.\n\n        Example:\n            &gt;&gt;&gt; from py_gdelt.filters import DocFilter\n            &gt;&gt;&gt; from datetime import datetime\n            &gt;&gt;&gt; async with DocEndpoint() as doc:\n            ...     # Complex query with datetime range\n            ...     doc_filter = DocFilter(\n            ...         query='\"machine learning\" AND python',\n            ...         start_datetime=datetime(2024, 1, 1),\n            ...         end_datetime=datetime(2024, 1, 31),\n            ...         source_country=\"US\",\n            ...         max_results=100,\n            ...         sort_by=\"relevance\"\n            ...     )\n            ...     articles = await doc.query(doc_filter)\n        \"\"\"\n        from py_gdelt.models.articles import Article\n\n        params = self._build_params(query_filter)\n        url = await self._build_url()\n\n        data = await self._get_json(url, params=params)\n\n        # Parse response - handle both empty and populated responses\n        return [Article.model_validate(item) for item in data.get(\"articles\", [])]\n\n    async def timeline(\n        self,\n        query: str,\n        *,\n        timespan: str | None = \"7d\",\n    ) -&gt; Timeline:\n        \"\"\"\n        Get timeline data for a query.\n\n        Returns time series data showing article volume over time for a given\n        search query. Useful for visualizing trends and tracking story evolution.\n\n        Args:\n            query: Search query string.\n            timespan: Time range to analyze (default: \"7d\" - 7 days).\n                     Common values: \"24h\", \"7d\", \"30d\", \"3mon\".\n\n        Returns:\n            Timeline object with time series data points.\n\n        Raises:\n            APIError: On HTTP errors or invalid responses.\n            APIUnavailableError: When API is down or unreachable.\n            RateLimitError: When rate limited by the API.\n\n        Example:\n            &gt;&gt;&gt; async with DocEndpoint() as doc:\n            ...     # Get article volume over last month\n            ...     timeline = await doc.timeline(\"protests\", timespan=\"30d\")\n            ...     for point in timeline.points:\n            ...         print(f\"{point.date}: {point.value} articles\")\n        \"\"\"\n        from py_gdelt.models.articles import Timeline\n\n        query_filter = DocFilter(\n            query=query,\n            timespan=timespan,\n            mode=\"timelinevol\",  # GDELT API uses 'timelinevol', not 'timeline'\n        )\n\n        params = self._build_params(query_filter)\n        url = await self._build_url()\n\n        data = await self._get_json(url, params=params)\n        return Timeline.model_validate(data)\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.doc.DocEndpoint.search","title":"<code>search(query, *, timespan=None, max_results=250, sort_by='date', source_language=None, source_country=None)</code>  <code>async</code>","text":"<p>Search for articles matching a query.</p> <p>This is a convenience method that constructs a DocFilter internally. For more control over query parameters, use query() with a DocFilter directly.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query string (supports boolean operators, phrases).</p> required <code>timespan</code> <code>str | None</code> <p>Time range like \"24h\", \"7d\", \"30d\". If None, searches all time.</p> <code>None</code> <code>max_results</code> <code>int</code> <p>Maximum results to return (1-250, default: 250).</p> <code>250</code> <code>sort_by</code> <code>Literal['date', 'relevance', 'tone']</code> <p>Sort order - \"date\", \"relevance\", or \"tone\" (default: \"date\").</p> <code>'date'</code> <code>source_language</code> <code>str | None</code> <p>Filter by source language (ISO 639 code).</p> <code>None</code> <code>source_country</code> <code>str | None</code> <p>Filter by source country (FIPS country code).</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Article]</code> <p>List of Article objects matching the query.</p> <p>Raises:</p> Type Description <code>APIError</code> <p>On HTTP errors or invalid responses.</p> <code>APIUnavailableError</code> <p>When API is down or unreachable.</p> <code>RateLimitError</code> <p>When rate limited by the API.</p> Example <p>async with DocEndpoint() as doc: ...     # Search recent articles about climate ...     articles = await doc.search( ...         \"climate change\", ...         timespan=\"7d\", ...         max_results=50, ...         sort_by=\"relevance\" ...     ) ...     # Filter by country ...     us_articles = await doc.search( ...         \"elections\", ...         source_country=\"US\", ...         timespan=\"24h\" ...     )</p> Source code in <code>src/py_gdelt/endpoints/doc.py</code> <pre><code>async def search(\n    self,\n    query: str,\n    *,\n    timespan: str | None = None,\n    max_results: int = 250,\n    sort_by: Literal[\"date\", \"relevance\", \"tone\"] = \"date\",\n    source_language: str | None = None,\n    source_country: str | None = None,\n) -&gt; list[Article]:\n    \"\"\"\n    Search for articles matching a query.\n\n    This is a convenience method that constructs a DocFilter internally.\n    For more control over query parameters, use query() with a DocFilter directly.\n\n    Args:\n        query: Search query string (supports boolean operators, phrases).\n        timespan: Time range like \"24h\", \"7d\", \"30d\". If None, searches all time.\n        max_results: Maximum results to return (1-250, default: 250).\n        sort_by: Sort order - \"date\", \"relevance\", or \"tone\" (default: \"date\").\n        source_language: Filter by source language (ISO 639 code).\n        source_country: Filter by source country (FIPS country code).\n\n    Returns:\n        List of Article objects matching the query.\n\n    Raises:\n        APIError: On HTTP errors or invalid responses.\n        APIUnavailableError: When API is down or unreachable.\n        RateLimitError: When rate limited by the API.\n\n    Example:\n        &gt;&gt;&gt; async with DocEndpoint() as doc:\n        ...     # Search recent articles about climate\n        ...     articles = await doc.search(\n        ...         \"climate change\",\n        ...         timespan=\"7d\",\n        ...         max_results=50,\n        ...         sort_by=\"relevance\"\n        ...     )\n        ...     # Filter by country\n        ...     us_articles = await doc.search(\n        ...         \"elections\",\n        ...         source_country=\"US\",\n        ...         timespan=\"24h\"\n        ...     )\n    \"\"\"\n    query_filter = DocFilter(\n        query=query,\n        timespan=timespan,\n        max_results=max_results,\n        sort_by=sort_by,\n        source_language=source_language,\n        source_country=source_country,\n    )\n    return await self.query(query_filter)\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.doc.DocEndpoint.query","title":"<code>query(query_filter)</code>  <code>async</code>","text":"<p>Query the DOC API with a filter.</p> <p>Executes a search using a pre-configured DocFilter object, providing full control over all query parameters.</p> <p>Parameters:</p> Name Type Description Default <code>query_filter</code> <code>DocFilter</code> <p>DocFilter with query parameters and constraints.</p> required <p>Returns:</p> Type Description <code>list[Article]</code> <p>List of Article objects matching the filter criteria.</p> <p>Raises:</p> Type Description <code>APIError</code> <p>On HTTP errors or invalid responses.</p> <code>APIUnavailableError</code> <p>When API is down or unreachable.</p> <code>RateLimitError</code> <p>When rate limited by the API.</p> Example <p>from py_gdelt.filters import DocFilter from datetime import datetime async with DocEndpoint() as doc: ...     # Complex query with datetime range ...     doc_filter = DocFilter( ...         query='\"machine learning\" AND python', ...         start_datetime=datetime(2024, 1, 1), ...         end_datetime=datetime(2024, 1, 31), ...         source_country=\"US\", ...         max_results=100, ...         sort_by=\"relevance\" ...     ) ...     articles = await doc.query(doc_filter)</p> Source code in <code>src/py_gdelt/endpoints/doc.py</code> <pre><code>async def query(self, query_filter: DocFilter) -&gt; list[Article]:\n    \"\"\"\n    Query the DOC API with a filter.\n\n    Executes a search using a pre-configured DocFilter object, providing\n    full control over all query parameters.\n\n    Args:\n        query_filter: DocFilter with query parameters and constraints.\n\n    Returns:\n        List of Article objects matching the filter criteria.\n\n    Raises:\n        APIError: On HTTP errors or invalid responses.\n        APIUnavailableError: When API is down or unreachable.\n        RateLimitError: When rate limited by the API.\n\n    Example:\n        &gt;&gt;&gt; from py_gdelt.filters import DocFilter\n        &gt;&gt;&gt; from datetime import datetime\n        &gt;&gt;&gt; async with DocEndpoint() as doc:\n        ...     # Complex query with datetime range\n        ...     doc_filter = DocFilter(\n        ...         query='\"machine learning\" AND python',\n        ...         start_datetime=datetime(2024, 1, 1),\n        ...         end_datetime=datetime(2024, 1, 31),\n        ...         source_country=\"US\",\n        ...         max_results=100,\n        ...         sort_by=\"relevance\"\n        ...     )\n        ...     articles = await doc.query(doc_filter)\n    \"\"\"\n    from py_gdelt.models.articles import Article\n\n    params = self._build_params(query_filter)\n    url = await self._build_url()\n\n    data = await self._get_json(url, params=params)\n\n    # Parse response - handle both empty and populated responses\n    return [Article.model_validate(item) for item in data.get(\"articles\", [])]\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.doc.DocEndpoint.timeline","title":"<code>timeline(query, *, timespan='7d')</code>  <code>async</code>","text":"<p>Get timeline data for a query.</p> <p>Returns time series data showing article volume over time for a given search query. Useful for visualizing trends and tracking story evolution.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query string.</p> required <code>timespan</code> <code>str | None</code> <p>Time range to analyze (default: \"7d\" - 7 days).      Common values: \"24h\", \"7d\", \"30d\", \"3mon\".</p> <code>'7d'</code> <p>Returns:</p> Type Description <code>Timeline</code> <p>Timeline object with time series data points.</p> <p>Raises:</p> Type Description <code>APIError</code> <p>On HTTP errors or invalid responses.</p> <code>APIUnavailableError</code> <p>When API is down or unreachable.</p> <code>RateLimitError</code> <p>When rate limited by the API.</p> Example <p>async with DocEndpoint() as doc: ...     # Get article volume over last month ...     timeline = await doc.timeline(\"protests\", timespan=\"30d\") ...     for point in timeline.points: ...         print(f\"{point.date}: {point.value} articles\")</p> Source code in <code>src/py_gdelt/endpoints/doc.py</code> <pre><code>async def timeline(\n    self,\n    query: str,\n    *,\n    timespan: str | None = \"7d\",\n) -&gt; Timeline:\n    \"\"\"\n    Get timeline data for a query.\n\n    Returns time series data showing article volume over time for a given\n    search query. Useful for visualizing trends and tracking story evolution.\n\n    Args:\n        query: Search query string.\n        timespan: Time range to analyze (default: \"7d\" - 7 days).\n                 Common values: \"24h\", \"7d\", \"30d\", \"3mon\".\n\n    Returns:\n        Timeline object with time series data points.\n\n    Raises:\n        APIError: On HTTP errors or invalid responses.\n        APIUnavailableError: When API is down or unreachable.\n        RateLimitError: When rate limited by the API.\n\n    Example:\n        &gt;&gt;&gt; async with DocEndpoint() as doc:\n        ...     # Get article volume over last month\n        ...     timeline = await doc.timeline(\"protests\", timespan=\"30d\")\n        ...     for point in timeline.points:\n        ...         print(f\"{point.date}: {point.value} articles\")\n    \"\"\"\n    from py_gdelt.models.articles import Timeline\n\n    query_filter = DocFilter(\n        query=query,\n        timespan=timespan,\n        mode=\"timelinevol\",  # GDELT API uses 'timelinevol', not 'timeline'\n    )\n\n    params = self._build_params(query_filter)\n    url = await self._build_url()\n\n    data = await self._get_json(url, params=params)\n    return Timeline.model_validate(data)\n</code></pre>"},{"location":"api/endpoints/#geoendpoint","title":"GeoEndpoint","text":""},{"location":"api/endpoints/#py_gdelt.endpoints.geo.GeoEndpoint","title":"<code>GeoEndpoint</code>","text":"<p>               Bases: <code>BaseEndpoint</code></p> <p>GEO 2.0 API endpoint for geographic article data.</p> <p>Returns locations mentioned in news articles matching a query. Supports time-based filtering and geographic bounds.</p> Example <p>async with GeoEndpoint() as geo:     result = await geo.search(\"earthquake\", max_points=100)     for point in result.points:         print(f\"{point.name}: {point.count} articles\")</p> <p>Attributes:</p> Name Type Description <code>BASE_URL</code> <p>GEO API base URL</p> Source code in <code>src/py_gdelt/endpoints/geo.py</code> <pre><code>class GeoEndpoint(BaseEndpoint):\n    \"\"\"GEO 2.0 API endpoint for geographic article data.\n\n    Returns locations mentioned in news articles matching a query.\n    Supports time-based filtering and geographic bounds.\n\n    Example:\n        async with GeoEndpoint() as geo:\n            result = await geo.search(\"earthquake\", max_points=100)\n            for point in result.points:\n                print(f\"{point.name}: {point.count} articles\")\n\n    Attributes:\n        BASE_URL: GEO API base URL\n    \"\"\"\n\n    BASE_URL = \"https://api.gdeltproject.org/api/v2/geo/geo\"\n\n    async def _build_url(self, **kwargs: Any) -&gt; str:\n        \"\"\"Build GEO API URL.\n\n        The GEO API uses a fixed base URL with query parameters.\n\n        Args:\n            **kwargs: Unused, kept for BaseEndpoint compatibility\n\n        Returns:\n            Base URL for GEO API\n        \"\"\"\n        return self.BASE_URL\n\n    def _build_params(self, query_filter: GeoFilter) -&gt; dict[str, str]:\n        \"\"\"Build query parameters from GeoFilter.\n\n        Args:\n            query_filter: GeoFilter with query parameters\n\n        Returns:\n            Dict of URL query parameters\n        \"\"\"\n        params: dict[str, str] = {\n            \"query\": query_filter.query,\n            \"format\": \"GeoJSON\",  # GDELT GEO API requires exact case\n            \"maxpoints\": str(query_filter.max_results),\n        }\n\n        if query_filter.timespan:\n            params[\"timespan\"] = query_filter.timespan\n\n        # Add bounding box if provided (format: lon1,lat1,lon2,lat2)\n        if query_filter.bounding_box:\n            min_lat, min_lon, max_lat, max_lon = query_filter.bounding_box\n            params[\"BBOX\"] = f\"{min_lon},{min_lat},{max_lon},{max_lat}\"\n\n        return params\n\n    async def search(\n        self,\n        query: str,\n        *,\n        timespan: str | None = None,\n        max_points: int = 250,\n        bounding_box: tuple[float, float, float, float] | None = None,\n    ) -&gt; GeoResult:\n        \"\"\"Search for geographic locations in news.\n\n        Args:\n            query: Search query (full text search)\n            timespan: Time range (e.g., \"24h\", \"7d\", \"1m\")\n            max_points: Maximum points to return (1-250)\n            bounding_box: Optional (min_lat, min_lon, max_lat, max_lon)\n\n        Returns:\n            GeoResult with list of GeoPoints\n\n        Example:\n            async with GeoEndpoint() as geo:\n                result = await geo.search(\n                    \"earthquake\",\n                    timespan=\"7d\",\n                    max_points=50\n                )\n                print(f\"Found {len(result.points)} locations\")\n        \"\"\"\n        query_filter = GeoFilter(\n            query=query,\n            timespan=timespan,\n            max_results=min(max_points, 250),  # Cap at filter max\n            bounding_box=bounding_box,\n        )\n        return await self.query(query_filter)\n\n    async def query(self, query_filter: GeoFilter) -&gt; GeoResult:\n        \"\"\"Query the GEO API with a filter.\n\n        Args:\n            query_filter: GeoFilter with query parameters\n\n        Returns:\n            GeoResult containing geographic points\n\n        Raises:\n            APIError: On request failure\n            RateLimitError: On rate limit\n            APIUnavailableError: On server error\n        \"\"\"\n        params = self._build_params(query_filter)\n        url = await self._build_url()\n\n        data = await self._get_json(url, params=params)\n\n        # Parse GeoJSON features or raw points\n        points: list[GeoPoint] = []\n\n        if \"features\" in data:\n            # GeoJSON format\n            for feature in data[\"features\"]:\n                coords = feature.get(\"geometry\", {}).get(\"coordinates\", [])\n                props = feature.get(\"properties\", {})\n                if len(coords) &gt;= 2:\n                    points.append(\n                        GeoPoint(\n                            lon=coords[0],\n                            lat=coords[1],\n                            name=props.get(\"name\"),\n                            count=props.get(\"count\", 1),\n                            url=props.get(\"url\"),\n                        ),\n                    )\n        elif \"points\" in data:\n            # Plain JSON format\n            points.extend([GeoPoint.model_validate(item) for item in data[\"points\"]])\n\n        return GeoResult(\n            points=points,\n            total_count=data.get(\"count\", len(points)),\n        )\n\n    async def to_geojson(\n        self,\n        query: str,\n        *,\n        timespan: str | None = None,\n        max_points: int = 250,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Get raw GeoJSON response.\n\n        Useful for direct use with mapping libraries (Leaflet, Folium, etc).\n\n        Args:\n            query: Search query\n            timespan: Time range (e.g., \"24h\", \"7d\")\n            max_points: Maximum points (1-250)\n\n        Returns:\n            Raw GeoJSON dict (FeatureCollection)\n\n        Example:\n            async with GeoEndpoint() as geo:\n                geojson = await geo.to_geojson(\"climate change\", timespan=\"30d\")\n                # Pass directly to mapping library\n                folium.GeoJson(geojson).add_to(map)\n        \"\"\"\n        query_filter = GeoFilter(\n            query=query,\n            timespan=timespan,\n            max_results=min(max_points, 250),\n        )\n\n        params = self._build_params(query_filter)\n        params[\"format\"] = \"geojson\"\n        url = await self._build_url()\n\n        result = await self._get_json(url, params=params)\n        return cast(\"dict[str, Any]\", result)\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.geo.GeoEndpoint.search","title":"<code>search(query, *, timespan=None, max_points=250, bounding_box=None)</code>  <code>async</code>","text":"<p>Search for geographic locations in news.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query (full text search)</p> required <code>timespan</code> <code>str | None</code> <p>Time range (e.g., \"24h\", \"7d\", \"1m\")</p> <code>None</code> <code>max_points</code> <code>int</code> <p>Maximum points to return (1-250)</p> <code>250</code> <code>bounding_box</code> <code>tuple[float, float, float, float] | None</code> <p>Optional (min_lat, min_lon, max_lat, max_lon)</p> <code>None</code> <p>Returns:</p> Type Description <code>GeoResult</code> <p>GeoResult with list of GeoPoints</p> Example <p>async with GeoEndpoint() as geo:     result = await geo.search(         \"earthquake\",         timespan=\"7d\",         max_points=50     )     print(f\"Found {len(result.points)} locations\")</p> Source code in <code>src/py_gdelt/endpoints/geo.py</code> <pre><code>async def search(\n    self,\n    query: str,\n    *,\n    timespan: str | None = None,\n    max_points: int = 250,\n    bounding_box: tuple[float, float, float, float] | None = None,\n) -&gt; GeoResult:\n    \"\"\"Search for geographic locations in news.\n\n    Args:\n        query: Search query (full text search)\n        timespan: Time range (e.g., \"24h\", \"7d\", \"1m\")\n        max_points: Maximum points to return (1-250)\n        bounding_box: Optional (min_lat, min_lon, max_lat, max_lon)\n\n    Returns:\n        GeoResult with list of GeoPoints\n\n    Example:\n        async with GeoEndpoint() as geo:\n            result = await geo.search(\n                \"earthquake\",\n                timespan=\"7d\",\n                max_points=50\n            )\n            print(f\"Found {len(result.points)} locations\")\n    \"\"\"\n    query_filter = GeoFilter(\n        query=query,\n        timespan=timespan,\n        max_results=min(max_points, 250),  # Cap at filter max\n        bounding_box=bounding_box,\n    )\n    return await self.query(query_filter)\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.geo.GeoEndpoint.query","title":"<code>query(query_filter)</code>  <code>async</code>","text":"<p>Query the GEO API with a filter.</p> <p>Parameters:</p> Name Type Description Default <code>query_filter</code> <code>GeoFilter</code> <p>GeoFilter with query parameters</p> required <p>Returns:</p> Type Description <code>GeoResult</code> <p>GeoResult containing geographic points</p> <p>Raises:</p> Type Description <code>APIError</code> <p>On request failure</p> <code>RateLimitError</code> <p>On rate limit</p> <code>APIUnavailableError</code> <p>On server error</p> Source code in <code>src/py_gdelt/endpoints/geo.py</code> <pre><code>async def query(self, query_filter: GeoFilter) -&gt; GeoResult:\n    \"\"\"Query the GEO API with a filter.\n\n    Args:\n        query_filter: GeoFilter with query parameters\n\n    Returns:\n        GeoResult containing geographic points\n\n    Raises:\n        APIError: On request failure\n        RateLimitError: On rate limit\n        APIUnavailableError: On server error\n    \"\"\"\n    params = self._build_params(query_filter)\n    url = await self._build_url()\n\n    data = await self._get_json(url, params=params)\n\n    # Parse GeoJSON features or raw points\n    points: list[GeoPoint] = []\n\n    if \"features\" in data:\n        # GeoJSON format\n        for feature in data[\"features\"]:\n            coords = feature.get(\"geometry\", {}).get(\"coordinates\", [])\n            props = feature.get(\"properties\", {})\n            if len(coords) &gt;= 2:\n                points.append(\n                    GeoPoint(\n                        lon=coords[0],\n                        lat=coords[1],\n                        name=props.get(\"name\"),\n                        count=props.get(\"count\", 1),\n                        url=props.get(\"url\"),\n                    ),\n                )\n    elif \"points\" in data:\n        # Plain JSON format\n        points.extend([GeoPoint.model_validate(item) for item in data[\"points\"]])\n\n    return GeoResult(\n        points=points,\n        total_count=data.get(\"count\", len(points)),\n    )\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.geo.GeoEndpoint.to_geojson","title":"<code>to_geojson(query, *, timespan=None, max_points=250)</code>  <code>async</code>","text":"<p>Get raw GeoJSON response.</p> <p>Useful for direct use with mapping libraries (Leaflet, Folium, etc).</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query</p> required <code>timespan</code> <code>str | None</code> <p>Time range (e.g., \"24h\", \"7d\")</p> <code>None</code> <code>max_points</code> <code>int</code> <p>Maximum points (1-250)</p> <code>250</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Raw GeoJSON dict (FeatureCollection)</p> Example <p>async with GeoEndpoint() as geo:     geojson = await geo.to_geojson(\"climate change\", timespan=\"30d\")     # Pass directly to mapping library     folium.GeoJson(geojson).add_to(map)</p> Source code in <code>src/py_gdelt/endpoints/geo.py</code> <pre><code>async def to_geojson(\n    self,\n    query: str,\n    *,\n    timespan: str | None = None,\n    max_points: int = 250,\n) -&gt; dict[str, Any]:\n    \"\"\"Get raw GeoJSON response.\n\n    Useful for direct use with mapping libraries (Leaflet, Folium, etc).\n\n    Args:\n        query: Search query\n        timespan: Time range (e.g., \"24h\", \"7d\")\n        max_points: Maximum points (1-250)\n\n    Returns:\n        Raw GeoJSON dict (FeatureCollection)\n\n    Example:\n        async with GeoEndpoint() as geo:\n            geojson = await geo.to_geojson(\"climate change\", timespan=\"30d\")\n            # Pass directly to mapping library\n            folium.GeoJson(geojson).add_to(map)\n    \"\"\"\n    query_filter = GeoFilter(\n        query=query,\n        timespan=timespan,\n        max_results=min(max_points, 250),\n    )\n\n    params = self._build_params(query_filter)\n    params[\"format\"] = \"geojson\"\n    url = await self._build_url()\n\n    result = await self._get_json(url, params=params)\n    return cast(\"dict[str, Any]\", result)\n</code></pre>"},{"location":"api/endpoints/#contextendpoint","title":"ContextEndpoint","text":""},{"location":"api/endpoints/#py_gdelt.endpoints.context.ContextEndpoint","title":"<code>ContextEndpoint</code>","text":"<p>               Bases: <code>BaseEndpoint</code></p> <p>Context 2.0 API endpoint for contextual analysis.</p> <p>Provides contextual information about search terms including related themes, entities, and sentiment analysis.</p> <p>Attributes:</p> Name Type Description <code>BASE_URL</code> <p>Base URL for the Context API endpoint</p> Example <p>async with ContextEndpoint() as ctx:     result = await ctx.analyze(\"climate change\")     for theme in result.themes[:5]:         print(f\"{theme.theme}: {theme.count} mentions\")</p> Source code in <code>src/py_gdelt/endpoints/context.py</code> <pre><code>class ContextEndpoint(BaseEndpoint):\n    \"\"\"Context 2.0 API endpoint for contextual analysis.\n\n    Provides contextual information about search terms including\n    related themes, entities, and sentiment analysis.\n\n    Attributes:\n        BASE_URL: Base URL for the Context API endpoint\n\n    Example:\n        async with ContextEndpoint() as ctx:\n            result = await ctx.analyze(\"climate change\")\n            for theme in result.themes[:5]:\n                print(f\"{theme.theme}: {theme.count} mentions\")\n    \"\"\"\n\n    BASE_URL = \"https://api.gdeltproject.org/api/v2/context/context\"\n\n    async def __aenter__(self) -&gt; ContextEndpoint:\n        \"\"\"Async context manager entry.\n\n        Returns:\n            Self for use in async with statement.\n        \"\"\"\n        await super().__aenter__()\n        return self\n\n    async def _build_url(self, **kwargs: Any) -&gt; str:\n        \"\"\"Build Context API URL.\n\n        The Context API uses a single endpoint URL with query parameters.\n\n        Args:\n            **kwargs: Unused, included for BaseEndpoint compatibility.\n\n        Returns:\n            Base URL for Context API.\n        \"\"\"\n        return self.BASE_URL\n\n    def _build_params(\n        self,\n        query: str,\n        timespan: str | None = None,\n    ) -&gt; dict[str, str]:\n        \"\"\"Build query parameters for Context API request.\n\n        Args:\n            query: Search term to analyze\n            timespan: Time range (e.g., \"24h\", \"7d\", \"30d\")\n\n        Returns:\n            Dictionary of query parameters for the API request.\n        \"\"\"\n        params: dict[str, str] = {\n            \"query\": query,\n            \"format\": \"json\",\n            \"mode\": \"artlist\",  # GDELT Context API only supports 'artlist' mode\n        }\n\n        if timespan:\n            params[\"timespan\"] = timespan\n\n        return params\n\n    async def analyze(\n        self,\n        query: str,\n        *,\n        timespan: str | None = None,\n    ) -&gt; ContextResult:\n        \"\"\"Get contextual analysis for a search term.\n\n        Retrieves comprehensive contextual information including themes, entities,\n        tone analysis, and related queries for the specified search term.\n\n        Args:\n            query: Search term to analyze\n            timespan: Time range (e.g., \"24h\", \"7d\", \"30d\")\n\n        Returns:\n            ContextResult with themes, entities, and tone analysis\n\n        Raises:\n            RateLimitError: On 429 response\n            APIUnavailableError: On 5xx response or connection error\n            APIError: On other HTTP errors or invalid JSON\n        \"\"\"\n        params = self._build_params(query, timespan)\n        url = await self._build_url()\n\n        data = await self._get_json(url, params=params)\n\n        # Parse themes\n        themes: list[ContextTheme] = [\n            ContextTheme(\n                theme=item.get(\"theme\", \"\"),\n                count=item.get(\"count\", 0),\n                score=item.get(\"score\"),\n            )\n            for item in data.get(\"themes\", [])\n        ]\n\n        # Parse entities\n        entities: list[ContextEntity] = [\n            ContextEntity(\n                name=item.get(\"name\", \"\"),\n                entity_type=item.get(\"type\", \"UNKNOWN\"),\n                count=item.get(\"count\", 0),\n            )\n            for item in data.get(\"entities\", [])\n        ]\n\n        # Parse tone\n        tone: ContextTone | None = None\n        if \"tone\" in data:\n            t = data[\"tone\"]\n            tone = ContextTone(\n                average_tone=t.get(\"average\", 0.0),\n                positive_count=t.get(\"positive\", 0),\n                negative_count=t.get(\"negative\", 0),\n                neutral_count=t.get(\"neutral\", 0),\n            )\n\n        # Parse related queries\n        related = data.get(\"related_queries\", [])\n        related_queries = [str(q) for q in related] if isinstance(related, list) else []\n\n        return ContextResult(\n            query=query,\n            article_count=data.get(\"article_count\", 0),\n            themes=themes,\n            entities=entities,\n            tone=tone,\n            related_queries=related_queries,\n        )\n\n    async def get_themes(\n        self,\n        query: str,\n        *,\n        timespan: str | None = None,\n        limit: int = 10,\n    ) -&gt; list[ContextTheme]:\n        \"\"\"Get top themes for a search term.\n\n        Convenience method that returns just themes sorted by count.\n\n        Args:\n            query: Search term\n            timespan: Time range\n            limit: Max themes to return\n\n        Returns:\n            List of top themes sorted by count (descending)\n\n        Raises:\n            RateLimitError: On 429 response\n            APIUnavailableError: On 5xx response or connection error\n            APIError: On other HTTP errors or invalid JSON\n        \"\"\"\n        result = await self.analyze(query, timespan=timespan)\n        sorted_themes = sorted(result.themes, key=lambda t: t.count, reverse=True)\n        return sorted_themes[:limit]\n\n    async def get_entities(\n        self,\n        query: str,\n        *,\n        timespan: str | None = None,\n        entity_type: str | None = None,\n        limit: int = 10,\n    ) -&gt; list[ContextEntity]:\n        \"\"\"Get top entities for a search term.\n\n        Convenience method that returns entities, optionally filtered by type\n        and sorted by count.\n\n        Args:\n            query: Search term\n            timespan: Time range\n            entity_type: Filter by type (PERSON, ORG, LOCATION)\n            limit: Max entities to return\n\n        Returns:\n            List of top entities sorted by count (descending)\n\n        Raises:\n            RateLimitError: On 429 response\n            APIUnavailableError: On 5xx response or connection error\n            APIError: On other HTTP errors or invalid JSON\n        \"\"\"\n        result = await self.analyze(query, timespan=timespan)\n\n        entities = result.entities\n        if entity_type:\n            entities = [e for e in entities if e.entity_type == entity_type]\n\n        sorted_entities = sorted(entities, key=lambda e: e.count, reverse=True)\n        return sorted_entities[:limit]\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.context.ContextEndpoint.__aenter__","title":"<code>__aenter__()</code>  <code>async</code>","text":"<p>Async context manager entry.</p> <p>Returns:</p> Type Description <code>ContextEndpoint</code> <p>Self for use in async with statement.</p> Source code in <code>src/py_gdelt/endpoints/context.py</code> <pre><code>async def __aenter__(self) -&gt; ContextEndpoint:\n    \"\"\"Async context manager entry.\n\n    Returns:\n        Self for use in async with statement.\n    \"\"\"\n    await super().__aenter__()\n    return self\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.context.ContextEndpoint.analyze","title":"<code>analyze(query, *, timespan=None)</code>  <code>async</code>","text":"<p>Get contextual analysis for a search term.</p> <p>Retrieves comprehensive contextual information including themes, entities, tone analysis, and related queries for the specified search term.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search term to analyze</p> required <code>timespan</code> <code>str | None</code> <p>Time range (e.g., \"24h\", \"7d\", \"30d\")</p> <code>None</code> <p>Returns:</p> Type Description <code>ContextResult</code> <p>ContextResult with themes, entities, and tone analysis</p> <p>Raises:</p> Type Description <code>RateLimitError</code> <p>On 429 response</p> <code>APIUnavailableError</code> <p>On 5xx response or connection error</p> <code>APIError</code> <p>On other HTTP errors or invalid JSON</p> Source code in <code>src/py_gdelt/endpoints/context.py</code> <pre><code>async def analyze(\n    self,\n    query: str,\n    *,\n    timespan: str | None = None,\n) -&gt; ContextResult:\n    \"\"\"Get contextual analysis for a search term.\n\n    Retrieves comprehensive contextual information including themes, entities,\n    tone analysis, and related queries for the specified search term.\n\n    Args:\n        query: Search term to analyze\n        timespan: Time range (e.g., \"24h\", \"7d\", \"30d\")\n\n    Returns:\n        ContextResult with themes, entities, and tone analysis\n\n    Raises:\n        RateLimitError: On 429 response\n        APIUnavailableError: On 5xx response or connection error\n        APIError: On other HTTP errors or invalid JSON\n    \"\"\"\n    params = self._build_params(query, timespan)\n    url = await self._build_url()\n\n    data = await self._get_json(url, params=params)\n\n    # Parse themes\n    themes: list[ContextTheme] = [\n        ContextTheme(\n            theme=item.get(\"theme\", \"\"),\n            count=item.get(\"count\", 0),\n            score=item.get(\"score\"),\n        )\n        for item in data.get(\"themes\", [])\n    ]\n\n    # Parse entities\n    entities: list[ContextEntity] = [\n        ContextEntity(\n            name=item.get(\"name\", \"\"),\n            entity_type=item.get(\"type\", \"UNKNOWN\"),\n            count=item.get(\"count\", 0),\n        )\n        for item in data.get(\"entities\", [])\n    ]\n\n    # Parse tone\n    tone: ContextTone | None = None\n    if \"tone\" in data:\n        t = data[\"tone\"]\n        tone = ContextTone(\n            average_tone=t.get(\"average\", 0.0),\n            positive_count=t.get(\"positive\", 0),\n            negative_count=t.get(\"negative\", 0),\n            neutral_count=t.get(\"neutral\", 0),\n        )\n\n    # Parse related queries\n    related = data.get(\"related_queries\", [])\n    related_queries = [str(q) for q in related] if isinstance(related, list) else []\n\n    return ContextResult(\n        query=query,\n        article_count=data.get(\"article_count\", 0),\n        themes=themes,\n        entities=entities,\n        tone=tone,\n        related_queries=related_queries,\n    )\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.context.ContextEndpoint.get_themes","title":"<code>get_themes(query, *, timespan=None, limit=10)</code>  <code>async</code>","text":"<p>Get top themes for a search term.</p> <p>Convenience method that returns just themes sorted by count.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search term</p> required <code>timespan</code> <code>str | None</code> <p>Time range</p> <code>None</code> <code>limit</code> <code>int</code> <p>Max themes to return</p> <code>10</code> <p>Returns:</p> Type Description <code>list[ContextTheme]</code> <p>List of top themes sorted by count (descending)</p> <p>Raises:</p> Type Description <code>RateLimitError</code> <p>On 429 response</p> <code>APIUnavailableError</code> <p>On 5xx response or connection error</p> <code>APIError</code> <p>On other HTTP errors or invalid JSON</p> Source code in <code>src/py_gdelt/endpoints/context.py</code> <pre><code>async def get_themes(\n    self,\n    query: str,\n    *,\n    timespan: str | None = None,\n    limit: int = 10,\n) -&gt; list[ContextTheme]:\n    \"\"\"Get top themes for a search term.\n\n    Convenience method that returns just themes sorted by count.\n\n    Args:\n        query: Search term\n        timespan: Time range\n        limit: Max themes to return\n\n    Returns:\n        List of top themes sorted by count (descending)\n\n    Raises:\n        RateLimitError: On 429 response\n        APIUnavailableError: On 5xx response or connection error\n        APIError: On other HTTP errors or invalid JSON\n    \"\"\"\n    result = await self.analyze(query, timespan=timespan)\n    sorted_themes = sorted(result.themes, key=lambda t: t.count, reverse=True)\n    return sorted_themes[:limit]\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.context.ContextEndpoint.get_entities","title":"<code>get_entities(query, *, timespan=None, entity_type=None, limit=10)</code>  <code>async</code>","text":"<p>Get top entities for a search term.</p> <p>Convenience method that returns entities, optionally filtered by type and sorted by count.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search term</p> required <code>timespan</code> <code>str | None</code> <p>Time range</p> <code>None</code> <code>entity_type</code> <code>str | None</code> <p>Filter by type (PERSON, ORG, LOCATION)</p> <code>None</code> <code>limit</code> <code>int</code> <p>Max entities to return</p> <code>10</code> <p>Returns:</p> Type Description <code>list[ContextEntity]</code> <p>List of top entities sorted by count (descending)</p> <p>Raises:</p> Type Description <code>RateLimitError</code> <p>On 429 response</p> <code>APIUnavailableError</code> <p>On 5xx response or connection error</p> <code>APIError</code> <p>On other HTTP errors or invalid JSON</p> Source code in <code>src/py_gdelt/endpoints/context.py</code> <pre><code>async def get_entities(\n    self,\n    query: str,\n    *,\n    timespan: str | None = None,\n    entity_type: str | None = None,\n    limit: int = 10,\n) -&gt; list[ContextEntity]:\n    \"\"\"Get top entities for a search term.\n\n    Convenience method that returns entities, optionally filtered by type\n    and sorted by count.\n\n    Args:\n        query: Search term\n        timespan: Time range\n        entity_type: Filter by type (PERSON, ORG, LOCATION)\n        limit: Max entities to return\n\n    Returns:\n        List of top entities sorted by count (descending)\n\n    Raises:\n        RateLimitError: On 429 response\n        APIUnavailableError: On 5xx response or connection error\n        APIError: On other HTTP errors or invalid JSON\n    \"\"\"\n    result = await self.analyze(query, timespan=timespan)\n\n    entities = result.entities\n    if entity_type:\n        entities = [e for e in entities if e.entity_type == entity_type]\n\n    sorted_entities = sorted(entities, key=lambda e: e.count, reverse=True)\n    return sorted_entities[:limit]\n</code></pre>"},{"location":"api/endpoints/#tvendpoint","title":"TVEndpoint","text":""},{"location":"api/endpoints/#py_gdelt.endpoints.tv.TVEndpoint","title":"<code>TVEndpoint</code>","text":"<p>               Bases: <code>BaseEndpoint</code></p> <p>TV API endpoint for television news monitoring.</p> <p>Searches transcripts from major US television networks including CNN, Fox News, MSNBC, and others. Provides three query modes: - Clip gallery: Individual video clips matching query - Timeline: Time series of mention frequency - Station chart: Breakdown by network</p> <p>The endpoint handles date formatting, parameter building, and response parsing automatically.</p> <p>Attributes:</p> Name Type Description <code>BASE_URL</code> <p>API endpoint URL for TV queries</p> Example <p>async with TVEndpoint() as tv:     clips = await tv.search(\"election\", station=\"CNN\")     for clip in clips:         print(f\"{clip.show_name}: {clip.snippet}\")</p> Source code in <code>src/py_gdelt/endpoints/tv.py</code> <pre><code>class TVEndpoint(BaseEndpoint):\n    \"\"\"TV API endpoint for television news monitoring.\n\n    Searches transcripts from major US television networks including CNN,\n    Fox News, MSNBC, and others. Provides three query modes:\n    - Clip gallery: Individual video clips matching query\n    - Timeline: Time series of mention frequency\n    - Station chart: Breakdown by network\n\n    The endpoint handles date formatting, parameter building, and response\n    parsing automatically.\n\n    Attributes:\n        BASE_URL: API endpoint URL for TV queries\n\n    Example:\n        async with TVEndpoint() as tv:\n            clips = await tv.search(\"election\", station=\"CNN\")\n            for clip in clips:\n                print(f\"{clip.show_name}: {clip.snippet}\")\n    \"\"\"\n\n    BASE_URL = \"https://api.gdeltproject.org/api/v2/tv/tv\"\n\n    async def _build_url(self, **kwargs: Any) -&gt; str:\n        \"\"\"Build the request URL.\n\n        TV API uses a fixed URL with query parameters.\n\n        Args:\n            **kwargs: Unused, but required by BaseEndpoint interface.\n\n        Returns:\n            The base TV API URL.\n        \"\"\"\n        return self.BASE_URL\n\n    def _build_params(self, query_filter: TVFilter) -&gt; dict[str, str]:\n        \"\"\"Build query parameters from TVFilter.\n\n        Constructs query parameters for the TV API from a TVFilter object.\n        Handles both timespan and datetime range parameters, station/market\n        filtering, and output mode selection.\n\n        Note: GDELT TV API requires station to be in the query string itself\n        (e.g., \"election station:CNN\") rather than as a separate parameter.\n\n        Args:\n            query_filter: Validated TV filter object\n\n        Returns:\n            Dictionary of query parameters ready for HTTP request\n        \"\"\"\n        # Build query string - GDELT TV API requires station in query\n        query = query_filter.query\n        if query_filter.station:\n            query = f\"{query} station:{query_filter.station}\"\n        if query_filter.market:\n            query = f\"{query} market:{query_filter.market}\"\n\n        params: dict[str, str] = {\n            \"query\": query,\n            \"format\": \"json\",\n            \"mode\": query_filter.mode,\n            \"maxrecords\": str(query_filter.max_results),\n        }\n\n        # Convert timespan to explicit datetime range (GDELT TV API TIMESPAN is unreliable)\n        if query_filter.timespan:\n            delta = _parse_timespan(query_filter.timespan)\n            if delta:\n                end_dt = datetime.now(UTC)\n                start_dt = end_dt - delta\n                params[\"STARTDATETIME\"] = start_dt.strftime(\"%Y%m%d%H%M%S\")\n                params[\"ENDDATETIME\"] = end_dt.strftime(\"%Y%m%d%H%M%S\")\n        elif query_filter.start_datetime:\n            params[\"STARTDATETIME\"] = query_filter.start_datetime.strftime(\"%Y%m%d%H%M%S\")\n            if query_filter.end_datetime:\n                params[\"ENDDATETIME\"] = query_filter.end_datetime.strftime(\"%Y%m%d%H%M%S\")\n\n        return params\n\n    async def search(\n        self,\n        query: str,\n        *,\n        timespan: str | None = None,\n        start_datetime: datetime | None = None,\n        end_datetime: datetime | None = None,\n        station: str | None = None,\n        market: str | None = None,\n        max_results: int = 250,\n    ) -&gt; list[TVClip]:\n        \"\"\"Search TV transcripts for clips.\n\n        Searches television news transcripts and returns matching video clips\n        with metadata and text excerpts.\n\n        Args:\n            query: Search query (keywords, phrases, or boolean expressions)\n            timespan: Time range (e.g., \"24h\", \"7d\", \"30d\")\n            start_datetime: Start of date range (alternative to timespan)\n            end_datetime: End of date range (alternative to timespan)\n            station: Filter by station (CNN, FOXNEWS, MSNBC, etc.)\n            market: Filter by market (National, Philadelphia, etc.)\n            max_results: Maximum clips to return (1-250)\n\n        Returns:\n            List of TVClip objects matching the query\n\n        Raises:\n            APIError: If the API returns an error\n            RateLimitError: If rate limit is exceeded\n            APIUnavailableError: If the API is unavailable\n\n        Example:\n            clips = await tv.search(\"climate change\", station=\"CNN\", timespan=\"7d\")\n        \"\"\"\n        query_filter = TVFilter(\n            query=query,\n            timespan=timespan,\n            start_datetime=start_datetime,\n            end_datetime=end_datetime,\n            station=station,\n            market=market,\n            max_results=max_results,\n            mode=\"ClipGallery\",\n        )\n        return await self.query_clips(query_filter)\n\n    async def query_clips(self, query_filter: TVFilter) -&gt; list[TVClip]:\n        \"\"\"Query for TV clips with a filter.\n\n        Lower-level method that accepts a TVFilter object for more control\n        over query parameters.\n\n        Args:\n            query_filter: TVFilter object with query parameters\n\n        Returns:\n            List of TVClip objects\n\n        Raises:\n            APIError: If the API returns an error\n            RateLimitError: If rate limit is exceeded\n            APIUnavailableError: If the API is unavailable\n        \"\"\"\n        params = self._build_params(query_filter)\n        params[\"mode\"] = \"ClipGallery\"\n        url = await self._build_url()\n\n        data = await self._get_json(url, params=params)\n\n        clips: list[TVClip] = [\n            TVClip(\n                station=item.get(\"station\", \"\"),\n                show_name=item.get(\"show\"),\n                clip_url=item.get(\"url\"),\n                preview_url=item.get(\"preview\"),\n                date=try_parse_gdelt_datetime(item.get(\"date\")),\n                duration_seconds=item.get(\"duration\"),\n                snippet=item.get(\"snippet\"),\n            )\n            for item in data.get(\"clips\", [])\n        ]\n\n        return clips\n\n    async def timeline(\n        self,\n        query: str,\n        *,\n        timespan: str | None = \"7d\",\n        start_datetime: datetime | None = None,\n        end_datetime: datetime | None = None,\n        station: str | None = None,\n    ) -&gt; TVTimeline:\n        \"\"\"Get timeline of TV mentions.\n\n        Returns a time series showing when a topic was mentioned on television,\n        useful for tracking coverage patterns over time.\n\n        Args:\n            query: Search query\n            timespan: Time range (default: \"7d\")\n            start_datetime: Start of date range (alternative to timespan)\n            end_datetime: End of date range (alternative to timespan)\n            station: Optional station filter\n\n        Returns:\n            TVTimeline with time series data\n\n        Raises:\n            APIError: If the API returns an error\n            RateLimitError: If rate limit is exceeded\n            APIUnavailableError: If the API is unavailable\n\n        Example:\n            timeline = await tv.timeline(\"election\", timespan=\"30d\")\n            for point in timeline.points:\n                print(f\"{point.date}: {point.count} mentions\")\n        \"\"\"\n        query_filter = TVFilter(\n            query=query,\n            timespan=timespan if not start_datetime else None,\n            start_datetime=start_datetime,\n            end_datetime=end_datetime,\n            station=station,\n            mode=\"TimelineVol\",\n        )\n\n        params = self._build_params(query_filter)\n        url = await self._build_url()\n\n        data = await self._get_json(url, params=params)\n\n        points: list[TVTimelinePoint] = [\n            TVTimelinePoint(\n                date=item.get(\"date\", \"\"),\n                station=item.get(\"station\"),\n                count=item.get(\"count\", 0),\n            )\n            for item in data.get(\"timeline\", [])\n        ]\n\n        return TVTimeline(points=points)\n\n    async def station_chart(\n        self,\n        query: str,\n        *,\n        timespan: str | None = \"7d\",\n        start_datetime: datetime | None = None,\n        end_datetime: datetime | None = None,\n    ) -&gt; TVStationChart:\n        \"\"\"Get station comparison chart.\n\n        Shows which stations covered a topic the most, useful for understanding\n        which networks are focusing on particular stories.\n\n        Args:\n            query: Search query\n            timespan: Time range (default: \"7d\")\n            start_datetime: Start of date range (alternative to timespan)\n            end_datetime: End of date range (alternative to timespan)\n\n        Returns:\n            TVStationChart with station breakdown\n\n        Raises:\n            APIError: If the API returns an error\n            RateLimitError: If rate limit is exceeded\n            APIUnavailableError: If the API is unavailable\n\n        Example:\n            chart = await tv.station_chart(\"healthcare\")\n            for station in chart.stations:\n                print(f\"{station.station}: {station.percentage}%\")\n        \"\"\"\n        query_filter = TVFilter(\n            query=query,\n            timespan=timespan if not start_datetime else None,\n            start_datetime=start_datetime,\n            end_datetime=end_datetime,\n            mode=\"StationChart\",\n        )\n\n        params = self._build_params(query_filter)\n        url = await self._build_url()\n\n        data = await self._get_json(url, params=params)\n\n        stations: list[TVStationData] = []\n        if \"stations\" in data:\n            total = sum(s.get(\"count\", 0) for s in data[\"stations\"])\n            for item in data[\"stations\"]:\n                count = item.get(\"count\", 0)\n                stations.append(\n                    TVStationData(\n                        station=item.get(\"station\", \"\"),\n                        count=count,\n                        percentage=count / total * 100 if total &gt; 0 else None,\n                    ),\n                )\n\n        return TVStationChart(stations=stations)\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.tv.TVEndpoint.search","title":"<code>search(query, *, timespan=None, start_datetime=None, end_datetime=None, station=None, market=None, max_results=250)</code>  <code>async</code>","text":"<p>Search TV transcripts for clips.</p> <p>Searches television news transcripts and returns matching video clips with metadata and text excerpts.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query (keywords, phrases, or boolean expressions)</p> required <code>timespan</code> <code>str | None</code> <p>Time range (e.g., \"24h\", \"7d\", \"30d\")</p> <code>None</code> <code>start_datetime</code> <code>datetime | None</code> <p>Start of date range (alternative to timespan)</p> <code>None</code> <code>end_datetime</code> <code>datetime | None</code> <p>End of date range (alternative to timespan)</p> <code>None</code> <code>station</code> <code>str | None</code> <p>Filter by station (CNN, FOXNEWS, MSNBC, etc.)</p> <code>None</code> <code>market</code> <code>str | None</code> <p>Filter by market (National, Philadelphia, etc.)</p> <code>None</code> <code>max_results</code> <code>int</code> <p>Maximum clips to return (1-250)</p> <code>250</code> <p>Returns:</p> Type Description <code>list[TVClip]</code> <p>List of TVClip objects matching the query</p> <p>Raises:</p> Type Description <code>APIError</code> <p>If the API returns an error</p> <code>RateLimitError</code> <p>If rate limit is exceeded</p> <code>APIUnavailableError</code> <p>If the API is unavailable</p> Example <p>clips = await tv.search(\"climate change\", station=\"CNN\", timespan=\"7d\")</p> Source code in <code>src/py_gdelt/endpoints/tv.py</code> <pre><code>async def search(\n    self,\n    query: str,\n    *,\n    timespan: str | None = None,\n    start_datetime: datetime | None = None,\n    end_datetime: datetime | None = None,\n    station: str | None = None,\n    market: str | None = None,\n    max_results: int = 250,\n) -&gt; list[TVClip]:\n    \"\"\"Search TV transcripts for clips.\n\n    Searches television news transcripts and returns matching video clips\n    with metadata and text excerpts.\n\n    Args:\n        query: Search query (keywords, phrases, or boolean expressions)\n        timespan: Time range (e.g., \"24h\", \"7d\", \"30d\")\n        start_datetime: Start of date range (alternative to timespan)\n        end_datetime: End of date range (alternative to timespan)\n        station: Filter by station (CNN, FOXNEWS, MSNBC, etc.)\n        market: Filter by market (National, Philadelphia, etc.)\n        max_results: Maximum clips to return (1-250)\n\n    Returns:\n        List of TVClip objects matching the query\n\n    Raises:\n        APIError: If the API returns an error\n        RateLimitError: If rate limit is exceeded\n        APIUnavailableError: If the API is unavailable\n\n    Example:\n        clips = await tv.search(\"climate change\", station=\"CNN\", timespan=\"7d\")\n    \"\"\"\n    query_filter = TVFilter(\n        query=query,\n        timespan=timespan,\n        start_datetime=start_datetime,\n        end_datetime=end_datetime,\n        station=station,\n        market=market,\n        max_results=max_results,\n        mode=\"ClipGallery\",\n    )\n    return await self.query_clips(query_filter)\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.tv.TVEndpoint.query_clips","title":"<code>query_clips(query_filter)</code>  <code>async</code>","text":"<p>Query for TV clips with a filter.</p> <p>Lower-level method that accepts a TVFilter object for more control over query parameters.</p> <p>Parameters:</p> Name Type Description Default <code>query_filter</code> <code>TVFilter</code> <p>TVFilter object with query parameters</p> required <p>Returns:</p> Type Description <code>list[TVClip]</code> <p>List of TVClip objects</p> <p>Raises:</p> Type Description <code>APIError</code> <p>If the API returns an error</p> <code>RateLimitError</code> <p>If rate limit is exceeded</p> <code>APIUnavailableError</code> <p>If the API is unavailable</p> Source code in <code>src/py_gdelt/endpoints/tv.py</code> <pre><code>async def query_clips(self, query_filter: TVFilter) -&gt; list[TVClip]:\n    \"\"\"Query for TV clips with a filter.\n\n    Lower-level method that accepts a TVFilter object for more control\n    over query parameters.\n\n    Args:\n        query_filter: TVFilter object with query parameters\n\n    Returns:\n        List of TVClip objects\n\n    Raises:\n        APIError: If the API returns an error\n        RateLimitError: If rate limit is exceeded\n        APIUnavailableError: If the API is unavailable\n    \"\"\"\n    params = self._build_params(query_filter)\n    params[\"mode\"] = \"ClipGallery\"\n    url = await self._build_url()\n\n    data = await self._get_json(url, params=params)\n\n    clips: list[TVClip] = [\n        TVClip(\n            station=item.get(\"station\", \"\"),\n            show_name=item.get(\"show\"),\n            clip_url=item.get(\"url\"),\n            preview_url=item.get(\"preview\"),\n            date=try_parse_gdelt_datetime(item.get(\"date\")),\n            duration_seconds=item.get(\"duration\"),\n            snippet=item.get(\"snippet\"),\n        )\n        for item in data.get(\"clips\", [])\n    ]\n\n    return clips\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.tv.TVEndpoint.timeline","title":"<code>timeline(query, *, timespan='7d', start_datetime=None, end_datetime=None, station=None)</code>  <code>async</code>","text":"<p>Get timeline of TV mentions.</p> <p>Returns a time series showing when a topic was mentioned on television, useful for tracking coverage patterns over time.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query</p> required <code>timespan</code> <code>str | None</code> <p>Time range (default: \"7d\")</p> <code>'7d'</code> <code>start_datetime</code> <code>datetime | None</code> <p>Start of date range (alternative to timespan)</p> <code>None</code> <code>end_datetime</code> <code>datetime | None</code> <p>End of date range (alternative to timespan)</p> <code>None</code> <code>station</code> <code>str | None</code> <p>Optional station filter</p> <code>None</code> <p>Returns:</p> Type Description <code>TVTimeline</code> <p>TVTimeline with time series data</p> <p>Raises:</p> Type Description <code>APIError</code> <p>If the API returns an error</p> <code>RateLimitError</code> <p>If rate limit is exceeded</p> <code>APIUnavailableError</code> <p>If the API is unavailable</p> Example <p>timeline = await tv.timeline(\"election\", timespan=\"30d\") for point in timeline.points:     print(f\"{point.date}: {point.count} mentions\")</p> Source code in <code>src/py_gdelt/endpoints/tv.py</code> <pre><code>async def timeline(\n    self,\n    query: str,\n    *,\n    timespan: str | None = \"7d\",\n    start_datetime: datetime | None = None,\n    end_datetime: datetime | None = None,\n    station: str | None = None,\n) -&gt; TVTimeline:\n    \"\"\"Get timeline of TV mentions.\n\n    Returns a time series showing when a topic was mentioned on television,\n    useful for tracking coverage patterns over time.\n\n    Args:\n        query: Search query\n        timespan: Time range (default: \"7d\")\n        start_datetime: Start of date range (alternative to timespan)\n        end_datetime: End of date range (alternative to timespan)\n        station: Optional station filter\n\n    Returns:\n        TVTimeline with time series data\n\n    Raises:\n        APIError: If the API returns an error\n        RateLimitError: If rate limit is exceeded\n        APIUnavailableError: If the API is unavailable\n\n    Example:\n        timeline = await tv.timeline(\"election\", timespan=\"30d\")\n        for point in timeline.points:\n            print(f\"{point.date}: {point.count} mentions\")\n    \"\"\"\n    query_filter = TVFilter(\n        query=query,\n        timespan=timespan if not start_datetime else None,\n        start_datetime=start_datetime,\n        end_datetime=end_datetime,\n        station=station,\n        mode=\"TimelineVol\",\n    )\n\n    params = self._build_params(query_filter)\n    url = await self._build_url()\n\n    data = await self._get_json(url, params=params)\n\n    points: list[TVTimelinePoint] = [\n        TVTimelinePoint(\n            date=item.get(\"date\", \"\"),\n            station=item.get(\"station\"),\n            count=item.get(\"count\", 0),\n        )\n        for item in data.get(\"timeline\", [])\n    ]\n\n    return TVTimeline(points=points)\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.tv.TVEndpoint.station_chart","title":"<code>station_chart(query, *, timespan='7d', start_datetime=None, end_datetime=None)</code>  <code>async</code>","text":"<p>Get station comparison chart.</p> <p>Shows which stations covered a topic the most, useful for understanding which networks are focusing on particular stories.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query</p> required <code>timespan</code> <code>str | None</code> <p>Time range (default: \"7d\")</p> <code>'7d'</code> <code>start_datetime</code> <code>datetime | None</code> <p>Start of date range (alternative to timespan)</p> <code>None</code> <code>end_datetime</code> <code>datetime | None</code> <p>End of date range (alternative to timespan)</p> <code>None</code> <p>Returns:</p> Type Description <code>TVStationChart</code> <p>TVStationChart with station breakdown</p> <p>Raises:</p> Type Description <code>APIError</code> <p>If the API returns an error</p> <code>RateLimitError</code> <p>If rate limit is exceeded</p> <code>APIUnavailableError</code> <p>If the API is unavailable</p> Example <p>chart = await tv.station_chart(\"healthcare\") for station in chart.stations:     print(f\"{station.station}: {station.percentage}%\")</p> Source code in <code>src/py_gdelt/endpoints/tv.py</code> <pre><code>async def station_chart(\n    self,\n    query: str,\n    *,\n    timespan: str | None = \"7d\",\n    start_datetime: datetime | None = None,\n    end_datetime: datetime | None = None,\n) -&gt; TVStationChart:\n    \"\"\"Get station comparison chart.\n\n    Shows which stations covered a topic the most, useful for understanding\n    which networks are focusing on particular stories.\n\n    Args:\n        query: Search query\n        timespan: Time range (default: \"7d\")\n        start_datetime: Start of date range (alternative to timespan)\n        end_datetime: End of date range (alternative to timespan)\n\n    Returns:\n        TVStationChart with station breakdown\n\n    Raises:\n        APIError: If the API returns an error\n        RateLimitError: If rate limit is exceeded\n        APIUnavailableError: If the API is unavailable\n\n    Example:\n        chart = await tv.station_chart(\"healthcare\")\n        for station in chart.stations:\n            print(f\"{station.station}: {station.percentage}%\")\n    \"\"\"\n    query_filter = TVFilter(\n        query=query,\n        timespan=timespan if not start_datetime else None,\n        start_datetime=start_datetime,\n        end_datetime=end_datetime,\n        mode=\"StationChart\",\n    )\n\n    params = self._build_params(query_filter)\n    url = await self._build_url()\n\n    data = await self._get_json(url, params=params)\n\n    stations: list[TVStationData] = []\n    if \"stations\" in data:\n        total = sum(s.get(\"count\", 0) for s in data[\"stations\"])\n        for item in data[\"stations\"]:\n            count = item.get(\"count\", 0)\n            stations.append(\n                TVStationData(\n                    station=item.get(\"station\", \"\"),\n                    count=count,\n                    percentage=count / total * 100 if total &gt; 0 else None,\n                ),\n            )\n\n    return TVStationChart(stations=stations)\n</code></pre>"},{"location":"api/endpoints/#tvaiendpoint","title":"TVAIEndpoint","text":""},{"location":"api/endpoints/#py_gdelt.endpoints.tv.TVAIEndpoint","title":"<code>TVAIEndpoint</code>","text":"<p>               Bases: <code>BaseEndpoint</code></p> <p>TVAI API endpoint for AI-enhanced TV analysis.</p> <p>Similar to TVEndpoint but uses AI-powered features for enhanced analysis. Uses the same data models and similar interface as TVEndpoint.</p> <p>Attributes:</p> Name Type Description <code>BASE_URL</code> <p>API endpoint URL for TVAI queries</p> Example <p>async with TVAIEndpoint() as tvai:     clips = await tvai.search(\"artificial intelligence\")</p> Source code in <code>src/py_gdelt/endpoints/tv.py</code> <pre><code>class TVAIEndpoint(BaseEndpoint):\n    \"\"\"TVAI API endpoint for AI-enhanced TV analysis.\n\n    Similar to TVEndpoint but uses AI-powered features for enhanced analysis.\n    Uses the same data models and similar interface as TVEndpoint.\n\n    Attributes:\n        BASE_URL: API endpoint URL for TVAI queries\n\n    Example:\n        async with TVAIEndpoint() as tvai:\n            clips = await tvai.search(\"artificial intelligence\")\n    \"\"\"\n\n    BASE_URL = \"https://api.gdeltproject.org/api/v2/tvai/tvai\"\n\n    async def _build_url(self, **kwargs: Any) -&gt; str:\n        \"\"\"Build the request URL.\n\n        TVAI API uses a fixed URL with query parameters.\n\n        Args:\n            **kwargs: Unused, but required by BaseEndpoint interface.\n\n        Returns:\n            The base TVAI API URL.\n        \"\"\"\n        return self.BASE_URL\n\n    async def search(\n        self,\n        query: str,\n        *,\n        timespan: str | None = None,\n        start_datetime: datetime | None = None,\n        end_datetime: datetime | None = None,\n        station: str | None = None,\n        max_results: int = 250,\n    ) -&gt; list[TVClip]:\n        \"\"\"Search using AI-enhanced analysis.\n\n        Searches television transcripts using AI-powered analysis for potentially\n        better semantic matching and relevance.\n\n        Args:\n            query: Search query\n            timespan: Time range (e.g., \"24h\", \"7d\")\n            start_datetime: Start of date range (alternative to timespan)\n            end_datetime: End of date range (alternative to timespan)\n            station: Filter by station\n            max_results: Maximum clips to return (1-250)\n\n        Returns:\n            List of TVClip objects\n\n        Raises:\n            APIError: If the API returns an error\n            RateLimitError: If rate limit is exceeded\n            APIUnavailableError: If the API is unavailable\n\n        Example:\n            clips = await tvai.search(\"machine learning\", timespan=\"7d\")\n        \"\"\"\n        # Build query string - GDELT TV API requires station in query\n        query_str = query\n        if station:\n            query_str = f\"{query} station:{station}\"\n\n        params: dict[str, str] = {\n            \"query\": query_str,\n            \"format\": \"json\",\n            \"mode\": \"ClipGallery\",\n            \"maxrecords\": str(max_results),\n        }\n\n        # Use explicit datetime range if provided, otherwise convert timespan\n        if start_datetime:\n            params[\"STARTDATETIME\"] = start_datetime.strftime(\"%Y%m%d%H%M%S\")\n            if end_datetime:\n                params[\"ENDDATETIME\"] = end_datetime.strftime(\"%Y%m%d%H%M%S\")\n        elif timespan:\n            delta = _parse_timespan(timespan)\n            if delta:\n                end_dt = datetime.now(UTC)\n                start_dt = end_dt - delta\n                params[\"STARTDATETIME\"] = start_dt.strftime(\"%Y%m%d%H%M%S\")\n                params[\"ENDDATETIME\"] = end_dt.strftime(\"%Y%m%d%H%M%S\")\n\n        url = await self._build_url()\n        data = await self._get_json(url, params=params)\n\n        clips: list[TVClip] = [\n            TVClip(\n                station=item.get(\"station\", \"\"),\n                show_name=item.get(\"show\"),\n                clip_url=item.get(\"url\"),\n                preview_url=item.get(\"preview\"),\n                date=try_parse_gdelt_datetime(item.get(\"date\")),\n                duration_seconds=item.get(\"duration\"),\n                snippet=item.get(\"snippet\"),\n            )\n            for item in data.get(\"clips\", [])\n        ]\n\n        return clips\n</code></pre>"},{"location":"api/endpoints/#py_gdelt.endpoints.tv.TVAIEndpoint.search","title":"<code>search(query, *, timespan=None, start_datetime=None, end_datetime=None, station=None, max_results=250)</code>  <code>async</code>","text":"<p>Search using AI-enhanced analysis.</p> <p>Searches television transcripts using AI-powered analysis for potentially better semantic matching and relevance.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query</p> required <code>timespan</code> <code>str | None</code> <p>Time range (e.g., \"24h\", \"7d\")</p> <code>None</code> <code>start_datetime</code> <code>datetime | None</code> <p>Start of date range (alternative to timespan)</p> <code>None</code> <code>end_datetime</code> <code>datetime | None</code> <p>End of date range (alternative to timespan)</p> <code>None</code> <code>station</code> <code>str | None</code> <p>Filter by station</p> <code>None</code> <code>max_results</code> <code>int</code> <p>Maximum clips to return (1-250)</p> <code>250</code> <p>Returns:</p> Type Description <code>list[TVClip]</code> <p>List of TVClip objects</p> <p>Raises:</p> Type Description <code>APIError</code> <p>If the API returns an error</p> <code>RateLimitError</code> <p>If rate limit is exceeded</p> <code>APIUnavailableError</code> <p>If the API is unavailable</p> Example <p>clips = await tvai.search(\"machine learning\", timespan=\"7d\")</p> Source code in <code>src/py_gdelt/endpoints/tv.py</code> <pre><code>async def search(\n    self,\n    query: str,\n    *,\n    timespan: str | None = None,\n    start_datetime: datetime | None = None,\n    end_datetime: datetime | None = None,\n    station: str | None = None,\n    max_results: int = 250,\n) -&gt; list[TVClip]:\n    \"\"\"Search using AI-enhanced analysis.\n\n    Searches television transcripts using AI-powered analysis for potentially\n    better semantic matching and relevance.\n\n    Args:\n        query: Search query\n        timespan: Time range (e.g., \"24h\", \"7d\")\n        start_datetime: Start of date range (alternative to timespan)\n        end_datetime: End of date range (alternative to timespan)\n        station: Filter by station\n        max_results: Maximum clips to return (1-250)\n\n    Returns:\n        List of TVClip objects\n\n    Raises:\n        APIError: If the API returns an error\n        RateLimitError: If rate limit is exceeded\n        APIUnavailableError: If the API is unavailable\n\n    Example:\n        clips = await tvai.search(\"machine learning\", timespan=\"7d\")\n    \"\"\"\n    # Build query string - GDELT TV API requires station in query\n    query_str = query\n    if station:\n        query_str = f\"{query} station:{station}\"\n\n    params: dict[str, str] = {\n        \"query\": query_str,\n        \"format\": \"json\",\n        \"mode\": \"ClipGallery\",\n        \"maxrecords\": str(max_results),\n    }\n\n    # Use explicit datetime range if provided, otherwise convert timespan\n    if start_datetime:\n        params[\"STARTDATETIME\"] = start_datetime.strftime(\"%Y%m%d%H%M%S\")\n        if end_datetime:\n            params[\"ENDDATETIME\"] = end_datetime.strftime(\"%Y%m%d%H%M%S\")\n    elif timespan:\n        delta = _parse_timespan(timespan)\n        if delta:\n            end_dt = datetime.now(UTC)\n            start_dt = end_dt - delta\n            params[\"STARTDATETIME\"] = start_dt.strftime(\"%Y%m%d%H%M%S\")\n            params[\"ENDDATETIME\"] = end_dt.strftime(\"%Y%m%d%H%M%S\")\n\n    url = await self._build_url()\n    data = await self._get_json(url, params=params)\n\n    clips: list[TVClip] = [\n        TVClip(\n            station=item.get(\"station\", \"\"),\n            show_name=item.get(\"show\"),\n            clip_url=item.get(\"url\"),\n            preview_url=item.get(\"preview\"),\n            date=try_parse_gdelt_datetime(item.get(\"date\")),\n            duration_seconds=item.get(\"duration\"),\n            snippet=item.get(\"snippet\"),\n        )\n        for item in data.get(\"clips\", [])\n    ]\n\n    return clips\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions API","text":"<p>Exception hierarchy for error handling.</p>"},{"location":"api/exceptions/#exception-hierarchy","title":"Exception Hierarchy","text":"<pre><code>GDELTError (base)\n\u251c\u2500\u2500 APIError\n\u2502   \u251c\u2500\u2500 RateLimitError\n\u2502   \u251c\u2500\u2500 APIUnavailableError\n\u2502   \u2514\u2500\u2500 InvalidQueryError\n\u251c\u2500\u2500 DataError\n\u2502   \u251c\u2500\u2500 ParseError\n\u2502   \u2514\u2500\u2500 ValidationError\n\u2502       \u2514\u2500\u2500 InvalidCodeError\n\u251c\u2500\u2500 ConfigurationError\n\u251c\u2500\u2500 BigQueryError\n\u2514\u2500\u2500 SecurityError\n</code></pre>"},{"location":"api/exceptions/#base-exception","title":"Base Exception","text":""},{"location":"api/exceptions/#py_gdelt.exceptions.GDELTError","title":"<code>GDELTError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all GDELT client errors.</p> <p>All custom exceptions in this library inherit from this class, allowing consumers to catch all library-specific errors with a single handler.</p> Source code in <code>src/py_gdelt/exceptions.py</code> <pre><code>class GDELTError(Exception):\n    \"\"\"\n    Base exception for all GDELT client errors.\n\n    All custom exceptions in this library inherit from this class,\n    allowing consumers to catch all library-specific errors with a single handler.\n    \"\"\"\n</code></pre>"},{"location":"api/exceptions/#api-exceptions","title":"API Exceptions","text":""},{"location":"api/exceptions/#py_gdelt.exceptions.APIError","title":"<code>APIError</code>","text":"<p>               Bases: <code>GDELTError</code></p> <p>Base exception for all API-related errors.</p> <p>Raised when errors occur during communication with GDELT REST APIs.</p> Source code in <code>src/py_gdelt/exceptions.py</code> <pre><code>class APIError(GDELTError):\n    \"\"\"\n    Base exception for all API-related errors.\n\n    Raised when errors occur during communication with GDELT REST APIs.\n    \"\"\"\n</code></pre>"},{"location":"api/exceptions/#py_gdelt.exceptions.RateLimitError","title":"<code>RateLimitError</code>","text":"<p>               Bases: <code>APIError</code></p> <p>Raised when API rate limits are exceeded.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error description</p> required <code>retry_after</code> <code>int | None</code> <p>Optional number of seconds to wait before retrying.         None if the retry duration is unknown.</p> <code>None</code> Source code in <code>src/py_gdelt/exceptions.py</code> <pre><code>class RateLimitError(APIError):\n    \"\"\"\n    Raised when API rate limits are exceeded.\n\n    Args:\n        message: Error description\n        retry_after: Optional number of seconds to wait before retrying.\n                    None if the retry duration is unknown.\n    \"\"\"\n\n    def __init__(self, message: str, retry_after: int | None = None) -&gt; None:\n        super().__init__(message)\n        self.retry_after = retry_after\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return string representation including retry information.\"\"\"\n        base_message = super().__str__()\n        if self.retry_after is not None:\n            return f\"{base_message} (retry after {self.retry_after} seconds)\"\n        return base_message\n</code></pre>"},{"location":"api/exceptions/#py_gdelt.exceptions.RateLimitError.__str__","title":"<code>__str__()</code>","text":"<p>Return string representation including retry information.</p> Source code in <code>src/py_gdelt/exceptions.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return string representation including retry information.\"\"\"\n    base_message = super().__str__()\n    if self.retry_after is not None:\n        return f\"{base_message} (retry after {self.retry_after} seconds)\"\n    return base_message\n</code></pre>"},{"location":"api/exceptions/#py_gdelt.exceptions.APIUnavailableError","title":"<code>APIUnavailableError</code>","text":"<p>               Bases: <code>APIError</code></p> <p>Raised when a GDELT API is temporarily unavailable.</p> <p>This typically indicates server-side issues or maintenance windows. Consider falling back to BigQuery when this occurs.</p> Source code in <code>src/py_gdelt/exceptions.py</code> <pre><code>class APIUnavailableError(APIError):\n    \"\"\"\n    Raised when a GDELT API is temporarily unavailable.\n\n    This typically indicates server-side issues or maintenance windows.\n    Consider falling back to BigQuery when this occurs.\n    \"\"\"\n</code></pre>"},{"location":"api/exceptions/#py_gdelt.exceptions.InvalidQueryError","title":"<code>InvalidQueryError</code>","text":"<p>               Bases: <code>APIError</code></p> <p>Raised when API request parameters are invalid.</p> <p>This indicates a client-side error in query construction or parameters.</p> Source code in <code>src/py_gdelt/exceptions.py</code> <pre><code>class InvalidQueryError(APIError):\n    \"\"\"\n    Raised when API request parameters are invalid.\n\n    This indicates a client-side error in query construction or parameters.\n    \"\"\"\n</code></pre>"},{"location":"api/exceptions/#data-exceptions","title":"Data Exceptions","text":""},{"location":"api/exceptions/#py_gdelt.exceptions.DataError","title":"<code>DataError</code>","text":"<p>               Bases: <code>GDELTError</code></p> <p>Base exception for data processing and validation errors.</p> <p>Raised when errors occur during data parsing, transformation, or validation.</p> Source code in <code>src/py_gdelt/exceptions.py</code> <pre><code>class DataError(GDELTError):\n    \"\"\"\n    Base exception for data processing and validation errors.\n\n    Raised when errors occur during data parsing, transformation, or validation.\n    \"\"\"\n</code></pre>"},{"location":"api/exceptions/#py_gdelt.exceptions.ParseError","title":"<code>ParseError</code>","text":"<p>               Bases: <code>DataError</code>, <code>ValueError</code></p> <p>Raised when data parsing fails.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error description</p> required <code>raw_data</code> <code>str | None</code> <p>Optional raw data that failed to parse, for debugging purposes.</p> <code>None</code> Source code in <code>src/py_gdelt/exceptions.py</code> <pre><code>class ParseError(DataError, ValueError):\n    \"\"\"\n    Raised when data parsing fails.\n\n    Args:\n        message: Error description\n        raw_data: Optional raw data that failed to parse, for debugging purposes.\n    \"\"\"\n\n    def __init__(self, message: str, raw_data: str | None = None) -&gt; None:\n        super().__init__(message)\n        self.raw_data = raw_data\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return string representation with truncated raw data if available.\"\"\"\n        base_message = super().__str__()\n        if self.raw_data is not None:\n            # Truncate raw data to first 100 characters for readability\n            truncated = self.raw_data[:100]\n            if len(self.raw_data) &gt; 100:\n                truncated += \"...\"\n            return f\"{base_message} (raw data: {truncated!r})\"\n        return base_message\n</code></pre>"},{"location":"api/exceptions/#py_gdelt.exceptions.ParseError.__str__","title":"<code>__str__()</code>","text":"<p>Return string representation with truncated raw data if available.</p> Source code in <code>src/py_gdelt/exceptions.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return string representation with truncated raw data if available.\"\"\"\n    base_message = super().__str__()\n    if self.raw_data is not None:\n        # Truncate raw data to first 100 characters for readability\n        truncated = self.raw_data[:100]\n        if len(self.raw_data) &gt; 100:\n            truncated += \"...\"\n        return f\"{base_message} (raw data: {truncated!r})\"\n    return base_message\n</code></pre>"},{"location":"api/exceptions/#py_gdelt.exceptions.ValidationError","title":"<code>ValidationError</code>","text":"<p>               Bases: <code>DataError</code></p> <p>Raised when data validation fails.</p> <p>This includes Pydantic validation errors and custom validation logic.</p> Source code in <code>src/py_gdelt/exceptions.py</code> <pre><code>class ValidationError(DataError):\n    \"\"\"\n    Raised when data validation fails.\n\n    This includes Pydantic validation errors and custom validation logic.\n    \"\"\"\n</code></pre>"},{"location":"api/exceptions/#py_gdelt.exceptions.InvalidCodeError","title":"<code>InvalidCodeError</code>","text":"<p>               Bases: <code>ValidationError</code></p> <p>Raised when an invalid GDELT code is encountered.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error description</p> required <code>code</code> <code>str</code> <p>The invalid code value</p> required <code>code_type</code> <code>str</code> <p>Type of code (e.g., \"cameo\", \"theme\", \"country\", \"fips\")</p> required <code>suggestions</code> <code>list[str] | None</code> <p>Optional list of suggested valid codes</p> <code>None</code> <code>help_url</code> <code>str | None</code> <p>Optional URL to reference documentation</p> <code>None</code> Source code in <code>src/py_gdelt/exceptions.py</code> <pre><code>class InvalidCodeError(ValidationError):\n    \"\"\"\n    Raised when an invalid GDELT code is encountered.\n\n    Args:\n        message: Error description\n        code: The invalid code value\n        code_type: Type of code (e.g., \"cameo\", \"theme\", \"country\", \"fips\")\n        suggestions: Optional list of suggested valid codes\n        help_url: Optional URL to reference documentation\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        code: str,\n        code_type: str,\n        suggestions: list[str] | None = None,\n        help_url: str | None = None,\n    ) -&gt; None:\n        super().__init__(message)\n        self.code = code\n        self.code_type = code_type\n        self.suggestions = suggestions or []\n        self.help_url = help_url\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return string representation including code details and guidance.\"\"\"\n        base_message = super().__str__()\n\n        # Always include code and code_type in the base representation (backward compatibility)\n        lines = [f\"{base_message} (code={self.code!r}, type={self.code_type!r})\"]\n\n        # Add country-specific help\n        if self.code_type == \"country\":\n            lines.extend(\n                [\n                    \"\",\n                    \"Accepted formats:\",\n                    \"  - FIPS (2 chars): US, UK, IR, FR, GM, CH, RS\",\n                    \"  - ISO3 (3 chars): USA, GBR, IRN, FRA, DEU, CHN, RUS\",\n                ]\n            )\n\n        # Add suggestions if available\n        if self.suggestions:\n            lines.extend([\"\", f\"Did you mean: {', '.join(self.suggestions)}?\"])\n\n        # Add reference URL if available\n        if self.help_url:\n            lines.extend([\"\", f\"Reference: {self.help_url}\"])\n\n        return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/exceptions/#py_gdelt.exceptions.InvalidCodeError.__str__","title":"<code>__str__()</code>","text":"<p>Return string representation including code details and guidance.</p> Source code in <code>src/py_gdelt/exceptions.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return string representation including code details and guidance.\"\"\"\n    base_message = super().__str__()\n\n    # Always include code and code_type in the base representation (backward compatibility)\n    lines = [f\"{base_message} (code={self.code!r}, type={self.code_type!r})\"]\n\n    # Add country-specific help\n    if self.code_type == \"country\":\n        lines.extend(\n            [\n                \"\",\n                \"Accepted formats:\",\n                \"  - FIPS (2 chars): US, UK, IR, FR, GM, CH, RS\",\n                \"  - ISO3 (3 chars): USA, GBR, IRN, FRA, DEU, CHN, RUS\",\n            ]\n        )\n\n    # Add suggestions if available\n    if self.suggestions:\n        lines.extend([\"\", f\"Did you mean: {', '.join(self.suggestions)}?\"])\n\n    # Add reference URL if available\n    if self.help_url:\n        lines.extend([\"\", f\"Reference: {self.help_url}\"])\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/exceptions/#other-exceptions","title":"Other Exceptions","text":""},{"location":"api/exceptions/#py_gdelt.exceptions.ConfigurationError","title":"<code>ConfigurationError</code>","text":"<p>               Bases: <code>GDELTError</code></p> <p>Raised when client configuration is invalid or incomplete.</p> <p>This includes missing credentials, invalid settings, or misconfiguration.</p> Source code in <code>src/py_gdelt/exceptions.py</code> <pre><code>class ConfigurationError(GDELTError):\n    \"\"\"\n    Raised when client configuration is invalid or incomplete.\n\n    This includes missing credentials, invalid settings, or misconfiguration.\n    \"\"\"\n</code></pre>"},{"location":"api/exceptions/#py_gdelt.exceptions.BigQueryError","title":"<code>BigQueryError</code>","text":"<p>               Bases: <code>GDELTError</code></p> <p>Raised when BigQuery operations fail.</p> <p>This includes query execution errors, authentication failures, and quota/billing issues.</p> Source code in <code>src/py_gdelt/exceptions.py</code> <pre><code>class BigQueryError(GDELTError):\n    \"\"\"\n    Raised when BigQuery operations fail.\n\n    This includes query execution errors, authentication failures,\n    and quota/billing issues.\n    \"\"\"\n</code></pre>"},{"location":"api/exceptions/#py_gdelt.exceptions.SecurityError","title":"<code>SecurityError</code>","text":"<p>               Bases: <code>GDELTError</code></p> <p>Raised when a security check fails.</p> <p>This includes URL validation failures, path traversal detection, zip bomb detection, and other security-related issues.</p> Source code in <code>src/py_gdelt/exceptions.py</code> <pre><code>class SecurityError(GDELTError):\n    \"\"\"\n    Raised when a security check fails.\n\n    This includes URL validation failures, path traversal detection,\n    zip bomb detection, and other security-related issues.\n    \"\"\"\n</code></pre>"},{"location":"api/exceptions/#usage","title":"Usage","text":"<pre><code>import asyncio\nimport logging\n\nfrom py_gdelt.exceptions import APIError, RateLimitError, DataError\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    result = await client.doc.query(doc_filter)\nexcept RateLimitError as e:\n    # Handle rate limiting with retry info\n    if e.retry_after:\n        await asyncio.sleep(e.retry_after)\nexcept APIError as e:\n    # Handle other API errors (network, unavailable, etc.)\n    logger.error(f\"API error: {e}\")\nexcept DataError as e:\n    # Handle data parsing errors\n    logger.error(f\"Data error: {e}\")\nexcept Exception as e:\n    # Handle unexpected errors\n    logger.error(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"api/filters/","title":"Filters API","text":"<p>Filters define query criteria for GDELT endpoints.</p>"},{"location":"api/filters/#common-filters","title":"Common Filters","text":""},{"location":"api/filters/#daterange","title":"DateRange","text":""},{"location":"api/filters/#py_gdelt.filters.DateRange","title":"<code>DateRange</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Date range filter with validation.</p> Note <p>There is no enforced date range limit. File-based datasets (Events, GKG, Mentions, etc.) can span years of data. Use streaming methods for large date ranges to avoid memory issues.</p> <p>REST APIs have their own limits enforced server-side: - DOC 2.0: 1 year (with timespan=1y) - GEO 2.0: 7 days - Context 2.0: 72 hours</p> Source code in <code>src/py_gdelt/filters.py</code> <pre><code>class DateRange(BaseModel):\n    \"\"\"Date range filter with validation.\n\n    Note:\n        There is no enforced date range limit. File-based datasets (Events, GKG,\n        Mentions, etc.) can span years of data. Use streaming methods for large\n        date ranges to avoid memory issues.\n\n        REST APIs have their own limits enforced server-side:\n        - DOC 2.0: 1 year (with timespan=1y)\n        - GEO 2.0: 7 days\n        - Context 2.0: 72 hours\n    \"\"\"\n\n    start: date\n    end: date | None = None\n\n    @model_validator(mode=\"after\")\n    def validate_range(self) -&gt; DateRange:\n        \"\"\"Ensure start &lt;= end.\"\"\"\n        end = self.end or self.start\n        if end &lt; self.start:\n            msg = \"end date must be &gt;= start date\"\n            raise ValueError(msg)\n        return self\n\n    @property\n    def days(self) -&gt; int:\n        \"\"\"Number of days in range.\"\"\"\n        end = self.end or self.start\n        return (end - self.start).days + 1\n</code></pre>"},{"location":"api/filters/#py_gdelt.filters.DateRange.days","title":"<code>days</code>  <code>property</code>","text":"<p>Number of days in range.</p>"},{"location":"api/filters/#py_gdelt.filters.DateRange.validate_range","title":"<code>validate_range()</code>","text":"<p>Ensure start &lt;= end.</p> Source code in <code>src/py_gdelt/filters.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_range(self) -&gt; DateRange:\n    \"\"\"Ensure start &lt;= end.\"\"\"\n    end = self.end or self.start\n    if end &lt; self.start:\n        msg = \"end date must be &gt;= start date\"\n        raise ValueError(msg)\n    return self\n</code></pre>"},{"location":"api/filters/#file-based-filters","title":"File-Based Filters","text":""},{"location":"api/filters/#eventfilter","title":"EventFilter","text":""},{"location":"api/filters/#py_gdelt.filters.EventFilter","title":"<code>EventFilter</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Filter for Events/Mentions queries.</p> Source code in <code>src/py_gdelt/filters.py</code> <pre><code>class EventFilter(BaseModel):\n    \"\"\"Filter for Events/Mentions queries.\"\"\"\n\n    date_range: DateRange\n\n    # Actor filters (CAMEO country codes validated)\n    actor1_country: str | None = None\n    actor2_country: str | None = None\n\n    # Event type filters (CAMEO event codes validated)\n    event_code: str | None = None\n    event_root_code: str | None = None\n    event_base_code: str | None = None\n\n    # Tone filter\n    min_tone: float | None = None\n    max_tone: float | None = None\n\n    # Location filters\n    action_country: str | None = None\n\n    # Options\n    include_translated: bool = True\n\n    @field_validator(\"actor1_country\", \"actor2_country\", \"action_country\", mode=\"before\")\n    @classmethod\n    def validate_country_code(cls, v: str | None) -&gt; str | None:\n        \"\"\"Validate and normalize country codes (accepts FIPS or ISO3).\"\"\"\n        if v is None:\n            return None\n        from py_gdelt.lookups.countries import Countries\n\n        countries = Countries()\n        return countries.normalize(v)  # Returns FIPS, raises InvalidCodeError if invalid\n\n    @field_validator(\"event_code\", \"event_root_code\", \"event_base_code\", mode=\"before\")\n    @classmethod\n    def validate_cameo_code(cls, v: str | None) -&gt; str | None:\n        \"\"\"Validate CAMEO event codes.\"\"\"\n        if v is None:\n            return None\n        from py_gdelt.lookups.cameo import CAMEOCodes\n\n        cameo = CAMEOCodes()\n        try:\n            cameo.validate(v)\n        except InvalidCodeError:\n            msg = f\"Invalid CAMEO code: {v!r}\"\n            raise InvalidCodeError(msg, code=v, code_type=\"CAMEO\") from None\n        return v\n</code></pre>"},{"location":"api/filters/#py_gdelt.filters.EventFilter.validate_country_code","title":"<code>validate_country_code(v)</code>  <code>classmethod</code>","text":"<p>Validate and normalize country codes (accepts FIPS or ISO3).</p> Source code in <code>src/py_gdelt/filters.py</code> <pre><code>@field_validator(\"actor1_country\", \"actor2_country\", \"action_country\", mode=\"before\")\n@classmethod\ndef validate_country_code(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate and normalize country codes (accepts FIPS or ISO3).\"\"\"\n    if v is None:\n        return None\n    from py_gdelt.lookups.countries import Countries\n\n    countries = Countries()\n    return countries.normalize(v)  # Returns FIPS, raises InvalidCodeError if invalid\n</code></pre>"},{"location":"api/filters/#py_gdelt.filters.EventFilter.validate_cameo_code","title":"<code>validate_cameo_code(v)</code>  <code>classmethod</code>","text":"<p>Validate CAMEO event codes.</p> Source code in <code>src/py_gdelt/filters.py</code> <pre><code>@field_validator(\"event_code\", \"event_root_code\", \"event_base_code\", mode=\"before\")\n@classmethod\ndef validate_cameo_code(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate CAMEO event codes.\"\"\"\n    if v is None:\n        return None\n    from py_gdelt.lookups.cameo import CAMEOCodes\n\n    cameo = CAMEOCodes()\n    try:\n        cameo.validate(v)\n    except InvalidCodeError:\n        msg = f\"Invalid CAMEO code: {v!r}\"\n        raise InvalidCodeError(msg, code=v, code_type=\"CAMEO\") from None\n    return v\n</code></pre>"},{"location":"api/filters/#gkgfilter","title":"GKGFilter","text":""},{"location":"api/filters/#py_gdelt.filters.GKGFilter","title":"<code>GKGFilter</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Filter for GKG queries.</p> Source code in <code>src/py_gdelt/filters.py</code> <pre><code>class GKGFilter(BaseModel):\n    \"\"\"Filter for GKG queries.\"\"\"\n\n    date_range: DateRange\n\n    # Theme filters (validated against GKG themes)\n    themes: list[str] | None = None\n    theme_prefix: str | None = None\n\n    # Entity filters\n    persons: list[str] | None = None\n    organizations: list[str] | None = None\n\n    # Location\n    country: str | None = None\n\n    # Tone\n    min_tone: float | None = None\n    max_tone: float | None = None\n\n    # Options\n    include_translated: bool = True\n\n    @field_validator(\"themes\", mode=\"before\")\n    @classmethod\n    def validate_themes(cls, v: list[str] | None) -&gt; list[str] | None:\n        \"\"\"Validate GKG theme codes.\"\"\"\n        if v is None:\n            return None\n        from py_gdelt.lookups.themes import GKGThemes\n\n        themes = GKGThemes()\n        for theme in v:\n            try:\n                themes.validate(theme)\n            except InvalidCodeError:\n                msg = f\"Invalid GKG theme: {theme!r}\"\n                raise InvalidCodeError(msg, code=theme, code_type=\"GKG theme\") from None\n        return v\n\n    @field_validator(\"country\", mode=\"before\")\n    @classmethod\n    def validate_country(cls, v: str | None) -&gt; str | None:\n        \"\"\"Validate and normalize country code (accepts FIPS or ISO3).\"\"\"\n        if v is None:\n            return None\n        from py_gdelt.lookups.countries import Countries\n\n        countries = Countries()\n        return countries.normalize(v)  # Returns FIPS, raises InvalidCodeError if invalid\n</code></pre>"},{"location":"api/filters/#py_gdelt.filters.GKGFilter.validate_themes","title":"<code>validate_themes(v)</code>  <code>classmethod</code>","text":"<p>Validate GKG theme codes.</p> Source code in <code>src/py_gdelt/filters.py</code> <pre><code>@field_validator(\"themes\", mode=\"before\")\n@classmethod\ndef validate_themes(cls, v: list[str] | None) -&gt; list[str] | None:\n    \"\"\"Validate GKG theme codes.\"\"\"\n    if v is None:\n        return None\n    from py_gdelt.lookups.themes import GKGThemes\n\n    themes = GKGThemes()\n    for theme in v:\n        try:\n            themes.validate(theme)\n        except InvalidCodeError:\n            msg = f\"Invalid GKG theme: {theme!r}\"\n            raise InvalidCodeError(msg, code=theme, code_type=\"GKG theme\") from None\n    return v\n</code></pre>"},{"location":"api/filters/#py_gdelt.filters.GKGFilter.validate_country","title":"<code>validate_country(v)</code>  <code>classmethod</code>","text":"<p>Validate and normalize country code (accepts FIPS or ISO3).</p> Source code in <code>src/py_gdelt/filters.py</code> <pre><code>@field_validator(\"country\", mode=\"before\")\n@classmethod\ndef validate_country(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate and normalize country code (accepts FIPS or ISO3).\"\"\"\n    if v is None:\n        return None\n    from py_gdelt.lookups.countries import Countries\n\n    countries = Countries()\n    return countries.normalize(v)  # Returns FIPS, raises InvalidCodeError if invalid\n</code></pre>"},{"location":"api/filters/#ngramsfilter","title":"NGramsFilter","text":""},{"location":"api/filters/#py_gdelt.filters.NGramsFilter","title":"<code>NGramsFilter</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Filter for NGrams 3.0 queries.</p> Source code in <code>src/py_gdelt/filters.py</code> <pre><code>class NGramsFilter(BaseModel):\n    \"\"\"Filter for NGrams 3.0 queries.\"\"\"\n\n    date_range: DateRange\n\n    # NGram filtering\n    ngram: str | None = None\n    language: str | None = None\n\n    # Position filtering (decile 0-90)\n    min_position: int | None = Field(default=None, ge=0, le=90)\n    max_position: int | None = Field(default=None, ge=0, le=90)\n\n    @model_validator(mode=\"after\")\n    def validate_position_range(self) -&gt; NGramsFilter:\n        \"\"\"Ensure min_position &lt;= max_position.\"\"\"\n        if (\n            self.min_position is not None\n            and self.max_position is not None\n            and self.min_position &gt; self.max_position\n        ):\n            msg = \"min_position must be &lt;= max_position\"\n            raise ValueError(msg)\n        return self\n</code></pre>"},{"location":"api/filters/#py_gdelt.filters.NGramsFilter.validate_position_range","title":"<code>validate_position_range()</code>","text":"<p>Ensure min_position &lt;= max_position.</p> Source code in <code>src/py_gdelt/filters.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_position_range(self) -&gt; NGramsFilter:\n    \"\"\"Ensure min_position &lt;= max_position.\"\"\"\n    if (\n        self.min_position is not None\n        and self.max_position is not None\n        and self.min_position &gt; self.max_position\n    ):\n        msg = \"min_position must be &lt;= max_position\"\n        raise ValueError(msg)\n    return self\n</code></pre>"},{"location":"api/filters/#rest-api-filters","title":"REST API Filters","text":""},{"location":"api/filters/#docfilter","title":"DocFilter","text":""},{"location":"api/filters/#py_gdelt.filters.DocFilter","title":"<code>DocFilter</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Filter for DOC 2.0 API queries.</p> Source code in <code>src/py_gdelt/filters.py</code> <pre><code>class DocFilter(BaseModel):\n    \"\"\"Filter for DOC 2.0 API queries.\"\"\"\n\n    query: str\n\n    # Time constraints\n    timespan: str | None = None\n    start_datetime: datetime | None = None\n    end_datetime: datetime | None = None\n\n    # Source filtering\n    source_country: str | None = None\n    source_language: str | None = None\n\n    # Result options\n    max_results: int = Field(default=250, ge=1, le=250)\n    sort_by: Literal[\"date\", \"relevance\", \"tone\"] = \"date\"\n\n    # Output mode\n    mode: Literal[\"artlist\", \"artgallery\", \"timelinevol\"] = \"artlist\"\n\n    @model_validator(mode=\"after\")\n    def validate_time_constraints(self) -&gt; DocFilter:\n        \"\"\"Ensure timespan XOR datetime range, not both.\"\"\"\n        if self.timespan and (self.start_datetime or self.end_datetime):\n            msg = \"Cannot specify both timespan and datetime range\"\n            raise ValueError(msg)\n        return self\n\n    @field_validator(\"source_country\", mode=\"before\")\n    @classmethod\n    def validate_source_country(cls, v: str | None) -&gt; str | None:\n        \"\"\"Validate and normalize source country code (accepts FIPS or ISO3).\"\"\"\n        if v is None:\n            return None\n        from py_gdelt.lookups.countries import Countries\n\n        countries = Countries()\n        return countries.normalize(v)  # Returns FIPS, raises InvalidCodeError if invalid\n</code></pre>"},{"location":"api/filters/#py_gdelt.filters.DocFilter.validate_time_constraints","title":"<code>validate_time_constraints()</code>","text":"<p>Ensure timespan XOR datetime range, not both.</p> Source code in <code>src/py_gdelt/filters.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_time_constraints(self) -&gt; DocFilter:\n    \"\"\"Ensure timespan XOR datetime range, not both.\"\"\"\n    if self.timespan and (self.start_datetime or self.end_datetime):\n        msg = \"Cannot specify both timespan and datetime range\"\n        raise ValueError(msg)\n    return self\n</code></pre>"},{"location":"api/filters/#py_gdelt.filters.DocFilter.validate_source_country","title":"<code>validate_source_country(v)</code>  <code>classmethod</code>","text":"<p>Validate and normalize source country code (accepts FIPS or ISO3).</p> Source code in <code>src/py_gdelt/filters.py</code> <pre><code>@field_validator(\"source_country\", mode=\"before\")\n@classmethod\ndef validate_source_country(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate and normalize source country code (accepts FIPS or ISO3).\"\"\"\n    if v is None:\n        return None\n    from py_gdelt.lookups.countries import Countries\n\n    countries = Countries()\n    return countries.normalize(v)  # Returns FIPS, raises InvalidCodeError if invalid\n</code></pre>"},{"location":"api/filters/#geofilter","title":"GeoFilter","text":""},{"location":"api/filters/#py_gdelt.filters.GeoFilter","title":"<code>GeoFilter</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Filter for GEO 2.0 API queries.</p> Source code in <code>src/py_gdelt/filters.py</code> <pre><code>class GeoFilter(BaseModel):\n    \"\"\"Filter for GEO 2.0 API queries.\"\"\"\n\n    query: str\n\n    # Geographic bounds (optional)\n    bounding_box: tuple[float, float, float, float] | None = None\n\n    # Time\n    timespan: str | None = None\n\n    # Result options\n    max_results: int = Field(default=250, ge=1, le=250)\n\n    @field_validator(\"bounding_box\", mode=\"before\")\n    @classmethod\n    def validate_bbox(\n        cls,\n        v: tuple[float, float, float, float] | None,\n    ) -&gt; tuple[float, float, float, float] | None:\n        \"\"\"Validate bounding box coordinates.\"\"\"\n        if v is None:\n            return None\n        min_lat, min_lon, max_lat, max_lon = v\n        if not (-90 &lt;= min_lat &lt;= 90 and -90 &lt;= max_lat &lt;= 90):\n            msg = \"Latitude must be between -90 and 90\"\n            raise ValueError(msg)\n        if not (-180 &lt;= min_lon &lt;= 180 and -180 &lt;= max_lon &lt;= 180):\n            msg = \"Longitude must be between -180 and 180\"\n            raise ValueError(msg)\n        if min_lat &gt; max_lat:\n            msg = \"min_lat must be &lt;= max_lat\"\n            raise ValueError(msg)\n        if min_lon &gt; max_lon:\n            msg = \"min_lon must be &lt;= max_lon\"\n            raise ValueError(msg)\n        return v\n</code></pre>"},{"location":"api/filters/#py_gdelt.filters.GeoFilter.validate_bbox","title":"<code>validate_bbox(v)</code>  <code>classmethod</code>","text":"<p>Validate bounding box coordinates.</p> Source code in <code>src/py_gdelt/filters.py</code> <pre><code>@field_validator(\"bounding_box\", mode=\"before\")\n@classmethod\ndef validate_bbox(\n    cls,\n    v: tuple[float, float, float, float] | None,\n) -&gt; tuple[float, float, float, float] | None:\n    \"\"\"Validate bounding box coordinates.\"\"\"\n    if v is None:\n        return None\n    min_lat, min_lon, max_lat, max_lon = v\n    if not (-90 &lt;= min_lat &lt;= 90 and -90 &lt;= max_lat &lt;= 90):\n        msg = \"Latitude must be between -90 and 90\"\n        raise ValueError(msg)\n    if not (-180 &lt;= min_lon &lt;= 180 and -180 &lt;= max_lon &lt;= 180):\n        msg = \"Longitude must be between -180 and 180\"\n        raise ValueError(msg)\n    if min_lat &gt; max_lat:\n        msg = \"min_lat must be &lt;= max_lat\"\n        raise ValueError(msg)\n    if min_lon &gt; max_lon:\n        msg = \"min_lon must be &lt;= max_lon\"\n        raise ValueError(msg)\n    return v\n</code></pre>"},{"location":"api/models/","title":"Data Models API","text":"<p>Data models represent GDELT records and results.</p>"},{"location":"api/models/#event-models","title":"Event Models","text":""},{"location":"api/models/#py_gdelt.models.Event","title":"<code>Event</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>GDELT Event record.</p> <p>Represents a single event in the GDELT Events database. Events capture who did what to whom, when, where, and how it was reported.</p> Source code in <code>src/py_gdelt/models/events.py</code> <pre><code>class Event(BaseModel):\n    \"\"\"GDELT Event record.\n\n    Represents a single event in the GDELT Events database. Events capture\n    who did what to whom, when, where, and how it was reported.\n    \"\"\"\n\n    # Identifiers\n    global_event_id: int\n    date: date  # Event date\n    date_added: datetime | None = None  # When first recorded (UTC)\n    source_url: str | None = None\n\n    # Actors\n    actor1: Actor | None = None\n    actor2: Actor | None = None\n\n    # Action\n    event_code: str  # CAMEO code (string, not int, to preserve leading zeros)\n    event_base_code: str\n    event_root_code: str\n    quad_class: int = Field(..., ge=1, le=4)  # 1-4\n    goldstein_scale: float = Field(..., ge=-10, le=10)  # -10 to +10\n\n    # Metrics\n    num_mentions: int = Field(default=0, ge=0)\n    num_sources: int = Field(default=0, ge=0)\n    num_articles: int = Field(default=0, ge=0)\n    avg_tone: float = 0.0\n    is_root_event: bool = False\n\n    # Geography (use Location from common.py)\n    actor1_geo: Location | None = None\n    actor2_geo: Location | None = None\n    action_geo: Location | None = None\n\n    # Metadata\n    version: int = Field(default=2, ge=1, le=2)  # 1 or 2\n    is_translated: bool = False\n    original_record_id: str | None = None  # For translated records\n\n    @classmethod\n    def from_raw(cls, raw: _RawEvent) -&gt; Event:\n        \"\"\"Convert internal _RawEvent to public Event model.\n\n        Args:\n            raw: Internal _RawEvent dataclass from TAB-delimited parsing.\n\n        Returns:\n            Event: Public Event model with proper type conversion.\n\n        Raises:\n            ValueError: If required fields are missing or invalid.\n        \"\"\"\n\n        # Helper to safely parse int\n        def _parse_int(value: str | None, default: int = 0) -&gt; int:\n            if not value or value == \"\":\n                return default\n            try:\n                return int(value)\n            except (ValueError, TypeError):\n                return default\n\n        # Helper to safely parse float\n        def _parse_float(value: str | None, default: float = 0.0) -&gt; float:\n            if not value or value == \"\":\n                return default\n            try:\n                return float(value)\n            except (ValueError, TypeError):\n                return default\n\n        # Helper to safely parse bool\n        def _parse_bool(value: str | None) -&gt; bool:\n            if not value or value == \"\":\n                return False\n            return value.strip() == \"1\"\n\n        # Helper to create Location from geo fields\n        def _make_location(\n            geo_type: str | None,\n            name: str | None,\n            country_code: str | None,\n            adm1_code: str | None,\n            adm2_code: str | None,\n            lat: str | None,\n            lon: str | None,\n            feature_id: str | None,\n        ) -&gt; Location | None:\n            # If no geo data, return None\n            if not any(\n                [\n                    geo_type,\n                    name,\n                    country_code,\n                    adm1_code,\n                    adm2_code,\n                    lat,\n                    lon,\n                    feature_id,\n                ],\n            ):\n                return None\n\n            return Location(\n                lat=_parse_float(lat) if lat else None,\n                lon=_parse_float(lon) if lon else None,\n                feature_id=feature_id if feature_id else None,\n                name=name if name else None,\n                country_code=country_code if country_code else None,\n                adm1_code=adm1_code if adm1_code else None,\n                adm2_code=adm2_code if adm2_code else None,\n                geo_type=_parse_int(geo_type) if geo_type else None,\n            )\n\n        # Helper to create Actor\n        def _make_actor(\n            code: str | None,\n            name: str | None,\n            country_code: str | None,\n            known_group_code: str | None,\n            ethnic_code: str | None,\n            religion1_code: str | None,\n            religion2_code: str | None,\n            type1_code: str | None,\n            type2_code: str | None,\n            type3_code: str | None,\n        ) -&gt; Actor | None:\n            # If no actor data, return None\n            if not any(\n                [\n                    code,\n                    name,\n                    country_code,\n                    known_group_code,\n                    ethnic_code,\n                    religion1_code,\n                    religion2_code,\n                    type1_code,\n                    type2_code,\n                    type3_code,\n                ],\n            ):\n                return None\n\n            return Actor(\n                code=code if code else None,\n                name=name if name else None,\n                country_code=country_code if country_code else None,\n                known_group_code=known_group_code if known_group_code else None,\n                ethnic_code=ethnic_code if ethnic_code else None,\n                religion1_code=religion1_code if religion1_code else None,\n                religion2_code=religion2_code if religion2_code else None,\n                type1_code=type1_code if type1_code else None,\n                type2_code=type2_code if type2_code else None,\n                type3_code=type3_code if type3_code else None,\n            )\n\n        # Parse date from sql_date (YYYYMMDD)\n        event_date = parse_gdelt_date(raw.sql_date)\n\n        # Parse date_added (YYYYMMDDHHMMSS)\n        date_added_dt: datetime | None = None\n        if raw.date_added:\n            with suppress(ValueError):\n                date_added_dt = parse_gdelt_datetime(raw.date_added)\n\n        # Create actors\n        actor1 = _make_actor(\n            raw.actor1_code,\n            raw.actor1_name,\n            raw.actor1_country_code,\n            raw.actor1_known_group_code,\n            raw.actor1_ethnic_code,\n            raw.actor1_religion1_code,\n            raw.actor1_religion2_code,\n            raw.actor1_type1_code,\n            raw.actor1_type2_code,\n            raw.actor1_type3_code,\n        )\n\n        actor2 = _make_actor(\n            raw.actor2_code,\n            raw.actor2_name,\n            raw.actor2_country_code,\n            raw.actor2_known_group_code,\n            raw.actor2_ethnic_code,\n            raw.actor2_religion1_code,\n            raw.actor2_religion2_code,\n            raw.actor2_type1_code,\n            raw.actor2_type2_code,\n            raw.actor2_type3_code,\n        )\n\n        # Create locations\n        actor1_geo = _make_location(\n            raw.actor1_geo_type,\n            raw.actor1_geo_fullname,\n            raw.actor1_geo_country_code,\n            raw.actor1_geo_adm1_code,\n            raw.actor1_geo_adm2_code,\n            raw.actor1_geo_lat,\n            raw.actor1_geo_lon,\n            raw.actor1_geo_feature_id,\n        )\n\n        actor2_geo = _make_location(\n            raw.actor2_geo_type,\n            raw.actor2_geo_fullname,\n            raw.actor2_geo_country_code,\n            raw.actor2_geo_adm1_code,\n            raw.actor2_geo_adm2_code,\n            raw.actor2_geo_lat,\n            raw.actor2_geo_lon,\n            raw.actor2_geo_feature_id,\n        )\n\n        action_geo = _make_location(\n            raw.action_geo_type,\n            raw.action_geo_fullname,\n            raw.action_geo_country_code,\n            raw.action_geo_adm1_code,\n            raw.action_geo_adm2_code,\n            raw.action_geo_lat,\n            raw.action_geo_lon,\n            raw.action_geo_feature_id,\n        )\n\n        return cls(\n            global_event_id=_parse_int(raw.global_event_id),\n            date=event_date,\n            date_added=date_added_dt,\n            source_url=raw.source_url if raw.source_url else None,\n            actor1=actor1,\n            actor2=actor2,\n            event_code=raw.event_code,  # Keep as string to preserve leading zeros\n            event_base_code=raw.event_base_code,\n            event_root_code=raw.event_root_code,\n            quad_class=_parse_int(raw.quad_class, default=1),\n            goldstein_scale=_parse_float(raw.goldstein_scale, default=0.0),\n            num_mentions=_parse_int(raw.num_mentions, default=0),\n            num_sources=_parse_int(raw.num_sources, default=0),\n            num_articles=_parse_int(raw.num_articles, default=0),\n            avg_tone=_parse_float(raw.avg_tone, default=0.0),\n            is_root_event=_parse_bool(raw.is_root_event),\n            actor1_geo=actor1_geo,\n            actor2_geo=actor2_geo,\n            action_geo=action_geo,\n            version=2,  # Default to v2, can be overridden\n            is_translated=raw.is_translated,\n            original_record_id=None,  # Not in raw data\n        )\n\n    @property\n    def is_conflict(self) -&gt; bool:\n        \"\"\"Check if event is conflict (quad_class 3 or 4).\n\n        Returns:\n            True if event is material conflict (3) or verbal conflict (4).\n        \"\"\"\n        return self.quad_class in (3, 4)\n\n    @property\n    def is_cooperation(self) -&gt; bool:\n        \"\"\"Check if event is cooperation (quad_class 1 or 2).\n\n        Returns:\n            True if event is verbal cooperation (1) or material cooperation (2).\n        \"\"\"\n        return self.quad_class in (1, 2)\n</code></pre>"},{"location":"api/models/#py_gdelt.models.Event.is_conflict","title":"<code>is_conflict</code>  <code>property</code>","text":"<p>Check if event is conflict (quad_class 3 or 4).</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if event is material conflict (3) or verbal conflict (4).</p>"},{"location":"api/models/#py_gdelt.models.Event.is_cooperation","title":"<code>is_cooperation</code>  <code>property</code>","text":"<p>Check if event is cooperation (quad_class 1 or 2).</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if event is verbal cooperation (1) or material cooperation (2).</p>"},{"location":"api/models/#py_gdelt.models.Event.from_raw","title":"<code>from_raw(raw)</code>  <code>classmethod</code>","text":"<p>Convert internal _RawEvent to public Event model.</p> <p>Parameters:</p> Name Type Description Default <code>raw</code> <code>_RawEvent</code> <p>Internal _RawEvent dataclass from TAB-delimited parsing.</p> required <p>Returns:</p> Name Type Description <code>Event</code> <code>Event</code> <p>Public Event model with proper type conversion.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required fields are missing or invalid.</p> Source code in <code>src/py_gdelt/models/events.py</code> <pre><code>@classmethod\ndef from_raw(cls, raw: _RawEvent) -&gt; Event:\n    \"\"\"Convert internal _RawEvent to public Event model.\n\n    Args:\n        raw: Internal _RawEvent dataclass from TAB-delimited parsing.\n\n    Returns:\n        Event: Public Event model with proper type conversion.\n\n    Raises:\n        ValueError: If required fields are missing or invalid.\n    \"\"\"\n\n    # Helper to safely parse int\n    def _parse_int(value: str | None, default: int = 0) -&gt; int:\n        if not value or value == \"\":\n            return default\n        try:\n            return int(value)\n        except (ValueError, TypeError):\n            return default\n\n    # Helper to safely parse float\n    def _parse_float(value: str | None, default: float = 0.0) -&gt; float:\n        if not value or value == \"\":\n            return default\n        try:\n            return float(value)\n        except (ValueError, TypeError):\n            return default\n\n    # Helper to safely parse bool\n    def _parse_bool(value: str | None) -&gt; bool:\n        if not value or value == \"\":\n            return False\n        return value.strip() == \"1\"\n\n    # Helper to create Location from geo fields\n    def _make_location(\n        geo_type: str | None,\n        name: str | None,\n        country_code: str | None,\n        adm1_code: str | None,\n        adm2_code: str | None,\n        lat: str | None,\n        lon: str | None,\n        feature_id: str | None,\n    ) -&gt; Location | None:\n        # If no geo data, return None\n        if not any(\n            [\n                geo_type,\n                name,\n                country_code,\n                adm1_code,\n                adm2_code,\n                lat,\n                lon,\n                feature_id,\n            ],\n        ):\n            return None\n\n        return Location(\n            lat=_parse_float(lat) if lat else None,\n            lon=_parse_float(lon) if lon else None,\n            feature_id=feature_id if feature_id else None,\n            name=name if name else None,\n            country_code=country_code if country_code else None,\n            adm1_code=adm1_code if adm1_code else None,\n            adm2_code=adm2_code if adm2_code else None,\n            geo_type=_parse_int(geo_type) if geo_type else None,\n        )\n\n    # Helper to create Actor\n    def _make_actor(\n        code: str | None,\n        name: str | None,\n        country_code: str | None,\n        known_group_code: str | None,\n        ethnic_code: str | None,\n        religion1_code: str | None,\n        religion2_code: str | None,\n        type1_code: str | None,\n        type2_code: str | None,\n        type3_code: str | None,\n    ) -&gt; Actor | None:\n        # If no actor data, return None\n        if not any(\n            [\n                code,\n                name,\n                country_code,\n                known_group_code,\n                ethnic_code,\n                religion1_code,\n                religion2_code,\n                type1_code,\n                type2_code,\n                type3_code,\n            ],\n        ):\n            return None\n\n        return Actor(\n            code=code if code else None,\n            name=name if name else None,\n            country_code=country_code if country_code else None,\n            known_group_code=known_group_code if known_group_code else None,\n            ethnic_code=ethnic_code if ethnic_code else None,\n            religion1_code=religion1_code if religion1_code else None,\n            religion2_code=religion2_code if religion2_code else None,\n            type1_code=type1_code if type1_code else None,\n            type2_code=type2_code if type2_code else None,\n            type3_code=type3_code if type3_code else None,\n        )\n\n    # Parse date from sql_date (YYYYMMDD)\n    event_date = parse_gdelt_date(raw.sql_date)\n\n    # Parse date_added (YYYYMMDDHHMMSS)\n    date_added_dt: datetime | None = None\n    if raw.date_added:\n        with suppress(ValueError):\n            date_added_dt = parse_gdelt_datetime(raw.date_added)\n\n    # Create actors\n    actor1 = _make_actor(\n        raw.actor1_code,\n        raw.actor1_name,\n        raw.actor1_country_code,\n        raw.actor1_known_group_code,\n        raw.actor1_ethnic_code,\n        raw.actor1_religion1_code,\n        raw.actor1_religion2_code,\n        raw.actor1_type1_code,\n        raw.actor1_type2_code,\n        raw.actor1_type3_code,\n    )\n\n    actor2 = _make_actor(\n        raw.actor2_code,\n        raw.actor2_name,\n        raw.actor2_country_code,\n        raw.actor2_known_group_code,\n        raw.actor2_ethnic_code,\n        raw.actor2_religion1_code,\n        raw.actor2_religion2_code,\n        raw.actor2_type1_code,\n        raw.actor2_type2_code,\n        raw.actor2_type3_code,\n    )\n\n    # Create locations\n    actor1_geo = _make_location(\n        raw.actor1_geo_type,\n        raw.actor1_geo_fullname,\n        raw.actor1_geo_country_code,\n        raw.actor1_geo_adm1_code,\n        raw.actor1_geo_adm2_code,\n        raw.actor1_geo_lat,\n        raw.actor1_geo_lon,\n        raw.actor1_geo_feature_id,\n    )\n\n    actor2_geo = _make_location(\n        raw.actor2_geo_type,\n        raw.actor2_geo_fullname,\n        raw.actor2_geo_country_code,\n        raw.actor2_geo_adm1_code,\n        raw.actor2_geo_adm2_code,\n        raw.actor2_geo_lat,\n        raw.actor2_geo_lon,\n        raw.actor2_geo_feature_id,\n    )\n\n    action_geo = _make_location(\n        raw.action_geo_type,\n        raw.action_geo_fullname,\n        raw.action_geo_country_code,\n        raw.action_geo_adm1_code,\n        raw.action_geo_adm2_code,\n        raw.action_geo_lat,\n        raw.action_geo_lon,\n        raw.action_geo_feature_id,\n    )\n\n    return cls(\n        global_event_id=_parse_int(raw.global_event_id),\n        date=event_date,\n        date_added=date_added_dt,\n        source_url=raw.source_url if raw.source_url else None,\n        actor1=actor1,\n        actor2=actor2,\n        event_code=raw.event_code,  # Keep as string to preserve leading zeros\n        event_base_code=raw.event_base_code,\n        event_root_code=raw.event_root_code,\n        quad_class=_parse_int(raw.quad_class, default=1),\n        goldstein_scale=_parse_float(raw.goldstein_scale, default=0.0),\n        num_mentions=_parse_int(raw.num_mentions, default=0),\n        num_sources=_parse_int(raw.num_sources, default=0),\n        num_articles=_parse_int(raw.num_articles, default=0),\n        avg_tone=_parse_float(raw.avg_tone, default=0.0),\n        is_root_event=_parse_bool(raw.is_root_event),\n        actor1_geo=actor1_geo,\n        actor2_geo=actor2_geo,\n        action_geo=action_geo,\n        version=2,  # Default to v2, can be overridden\n        is_translated=raw.is_translated,\n        original_record_id=None,  # Not in raw data\n    )\n</code></pre>"},{"location":"api/models/#py_gdelt.models.Mention","title":"<code>Mention</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Mention of a GDELT event in a source.</p> <p>Represents a single mention of an event in a news article. Each event can have many mentions across different sources and times.</p> Source code in <code>src/py_gdelt/models/events.py</code> <pre><code>class Mention(BaseModel):\n    \"\"\"Mention of a GDELT event in a source.\n\n    Represents a single mention of an event in a news article. Each event\n    can have many mentions across different sources and times.\n    \"\"\"\n\n    global_event_id: int\n    event_time: datetime\n    mention_time: datetime\n    mention_type: int  # 1=WEB, 2=Citation, 3=CORE, etc.\n    source_name: str\n    identifier: str  # URL, DOI, or citation\n    sentence_id: int\n    actor1_char_offset: int | None = None\n    actor2_char_offset: int | None = None\n    action_char_offset: int | None = None\n    in_raw_text: bool = False\n    confidence: int = Field(..., ge=10, le=100)  # 10-100\n    doc_length: int = Field(default=0, ge=0)\n    doc_tone: float = 0.0\n    translation_info: str | None = None\n\n    @classmethod\n    def from_raw(cls, raw: _RawMention) -&gt; Mention:\n        \"\"\"Convert internal _RawMention to public Mention model.\n\n        Args:\n            raw: Internal _RawMention dataclass from TAB-delimited parsing.\n\n        Returns:\n            Mention: Public Mention model with proper type conversion.\n\n        Raises:\n            ValueError: If required fields are missing or invalid.\n        \"\"\"\n\n        # Helper to safely parse int\n        def _parse_int(value: str | None, default: int = 0) -&gt; int:\n            if not value or value == \"\":\n                return default\n            try:\n                return int(value)\n            except (ValueError, TypeError):\n                return default\n\n        # Helper to safely parse float\n        def _parse_float(value: str | None, default: float = 0.0) -&gt; float:\n            if not value or value == \"\":\n                return default\n            try:\n                return float(value)\n            except (ValueError, TypeError):\n                return default\n\n        # Helper to safely parse bool\n        def _parse_bool(value: str | None) -&gt; bool:\n            if not value or value == \"\":\n                return False\n            return value.strip() == \"1\"\n\n        # Parse event_time (YYYYMMDDHHMMSS)\n        event_time = parse_gdelt_datetime(raw.event_time_full)\n\n        # Parse mention_time (YYYYMMDDHHMMSS)\n        mention_time = parse_gdelt_datetime(raw.mention_time_full)\n\n        # Parse char offsets (can be empty string)\n        actor1_offset = (\n            _parse_int(raw.actor1_char_offset)\n            if raw.actor1_char_offset and raw.actor1_char_offset != \"\"\n            else None\n        )\n        actor2_offset = (\n            _parse_int(raw.actor2_char_offset)\n            if raw.actor2_char_offset and raw.actor2_char_offset != \"\"\n            else None\n        )\n        action_offset = (\n            _parse_int(raw.action_char_offset)\n            if raw.action_char_offset and raw.action_char_offset != \"\"\n            else None\n        )\n\n        return cls(\n            global_event_id=_parse_int(raw.global_event_id),\n            event_time=event_time,\n            mention_time=mention_time,\n            mention_type=_parse_int(raw.mention_type, default=1),\n            source_name=raw.mention_source_name,\n            identifier=raw.mention_identifier,\n            sentence_id=_parse_int(raw.sentence_id, default=0),\n            actor1_char_offset=actor1_offset,\n            actor2_char_offset=actor2_offset,\n            action_char_offset=action_offset,\n            in_raw_text=_parse_bool(raw.in_raw_text),\n            confidence=_parse_int(raw.confidence, default=50),\n            doc_length=_parse_int(raw.mention_doc_length, default=0),\n            doc_tone=_parse_float(raw.mention_doc_tone, default=0.0),\n            translation_info=(\n                raw.mention_doc_translation_info if raw.mention_doc_translation_info else None\n            ),\n        )\n</code></pre>"},{"location":"api/models/#py_gdelt.models.Mention.from_raw","title":"<code>from_raw(raw)</code>  <code>classmethod</code>","text":"<p>Convert internal _RawMention to public Mention model.</p> <p>Parameters:</p> Name Type Description Default <code>raw</code> <code>_RawMention</code> <p>Internal _RawMention dataclass from TAB-delimited parsing.</p> required <p>Returns:</p> Name Type Description <code>Mention</code> <code>Mention</code> <p>Public Mention model with proper type conversion.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required fields are missing or invalid.</p> Source code in <code>src/py_gdelt/models/events.py</code> <pre><code>@classmethod\ndef from_raw(cls, raw: _RawMention) -&gt; Mention:\n    \"\"\"Convert internal _RawMention to public Mention model.\n\n    Args:\n        raw: Internal _RawMention dataclass from TAB-delimited parsing.\n\n    Returns:\n        Mention: Public Mention model with proper type conversion.\n\n    Raises:\n        ValueError: If required fields are missing or invalid.\n    \"\"\"\n\n    # Helper to safely parse int\n    def _parse_int(value: str | None, default: int = 0) -&gt; int:\n        if not value or value == \"\":\n            return default\n        try:\n            return int(value)\n        except (ValueError, TypeError):\n            return default\n\n    # Helper to safely parse float\n    def _parse_float(value: str | None, default: float = 0.0) -&gt; float:\n        if not value or value == \"\":\n            return default\n        try:\n            return float(value)\n        except (ValueError, TypeError):\n            return default\n\n    # Helper to safely parse bool\n    def _parse_bool(value: str | None) -&gt; bool:\n        if not value or value == \"\":\n            return False\n        return value.strip() == \"1\"\n\n    # Parse event_time (YYYYMMDDHHMMSS)\n    event_time = parse_gdelt_datetime(raw.event_time_full)\n\n    # Parse mention_time (YYYYMMDDHHMMSS)\n    mention_time = parse_gdelt_datetime(raw.mention_time_full)\n\n    # Parse char offsets (can be empty string)\n    actor1_offset = (\n        _parse_int(raw.actor1_char_offset)\n        if raw.actor1_char_offset and raw.actor1_char_offset != \"\"\n        else None\n    )\n    actor2_offset = (\n        _parse_int(raw.actor2_char_offset)\n        if raw.actor2_char_offset and raw.actor2_char_offset != \"\"\n        else None\n    )\n    action_offset = (\n        _parse_int(raw.action_char_offset)\n        if raw.action_char_offset and raw.action_char_offset != \"\"\n        else None\n    )\n\n    return cls(\n        global_event_id=_parse_int(raw.global_event_id),\n        event_time=event_time,\n        mention_time=mention_time,\n        mention_type=_parse_int(raw.mention_type, default=1),\n        source_name=raw.mention_source_name,\n        identifier=raw.mention_identifier,\n        sentence_id=_parse_int(raw.sentence_id, default=0),\n        actor1_char_offset=actor1_offset,\n        actor2_char_offset=actor2_offset,\n        action_char_offset=action_offset,\n        in_raw_text=_parse_bool(raw.in_raw_text),\n        confidence=_parse_int(raw.confidence, default=50),\n        doc_length=_parse_int(raw.mention_doc_length, default=0),\n        doc_tone=_parse_float(raw.mention_doc_tone, default=0.0),\n        translation_info=(\n            raw.mention_doc_translation_info if raw.mention_doc_translation_info else None\n        ),\n    )\n</code></pre>"},{"location":"api/models/#py_gdelt.models.Actor","title":"<code>Actor</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Actor in a GDELT event.</p> <p>Represents an entity (person, organization, country, etc.) participating in an event. Uses CAMEO actor codes for classification.</p> Source code in <code>src/py_gdelt/models/events.py</code> <pre><code>class Actor(BaseModel):\n    \"\"\"Actor in a GDELT event.\n\n    Represents an entity (person, organization, country, etc.) participating in an event.\n    Uses CAMEO actor codes for classification.\n    \"\"\"\n\n    code: str | None = None\n    name: str | None = None\n    country_code: str | None = None  # FIPS code\n    known_group_code: str | None = None\n    ethnic_code: str | None = None\n    religion1_code: str | None = None\n    religion2_code: str | None = None\n    type1_code: str | None = None\n    type2_code: str | None = None\n    type3_code: str | None = None\n\n    @property\n    def is_state_actor(self) -&gt; bool:\n        \"\"\"Check if actor is a state/government actor.\n\n        Returns:\n            True if actor has a country code but no known group code,\n            indicating a state-level actor.\n        \"\"\"\n        return self.country_code is not None and self.known_group_code is None\n</code></pre>"},{"location":"api/models/#py_gdelt.models.Actor.is_state_actor","title":"<code>is_state_actor</code>  <code>property</code>","text":"<p>Check if actor is a state/government actor.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if actor has a country code but no known group code,</p> <code>bool</code> <p>indicating a state-level actor.</p>"},{"location":"api/models/#article-models","title":"Article Models","text":""},{"location":"api/models/#py_gdelt.models.Article","title":"<code>Article</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Article from GDELT DOC API.</p> <p>Represents a news article monitored by GDELT.</p> Source code in <code>src/py_gdelt/models/articles.py</code> <pre><code>class Article(BaseModel):\n    \"\"\"\n    Article from GDELT DOC API.\n\n    Represents a news article monitored by GDELT.\n    \"\"\"\n\n    # Core fields (from API)\n    url: str\n    title: str | None = None\n    seendate: str | None = None  # Raw GDELT date string (YYYYMMDDHHMMSS)\n\n    # Source information\n    domain: str | None = None\n    source_country: str | None = Field(default=None, alias=\"sourcecountry\")\n    language: str | None = None\n\n    # Content\n    socialimage: str | None = None  # Preview image URL\n\n    # Tone analysis (optional)\n    tone: float | None = None\n\n    # Sharing metrics (optional)\n    share_count: int | None = Field(default=None, alias=\"sharecount\")\n\n    model_config = {\"populate_by_name\": True}\n\n    @property\n    def seen_datetime(self) -&gt; datetime | None:\n        \"\"\"\n        Parse seendate to datetime.\n\n        Returns:\n            datetime object or None if parsing fails\n        \"\"\"\n        return try_parse_gdelt_datetime(self.seendate)\n\n    @property\n    def is_english(self) -&gt; bool:\n        \"\"\"Check if article is in English.\"\"\"\n        if not self.language:\n            return False\n        return self.language.lower() in (\"english\", \"en\")\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert to dictionary for serialization.\"\"\"\n        return self.model_dump(by_alias=True)\n</code></pre>"},{"location":"api/models/#py_gdelt.models.Article.seen_datetime","title":"<code>seen_datetime</code>  <code>property</code>","text":"<p>Parse seendate to datetime.</p> <p>Returns:</p> Type Description <code>datetime | None</code> <p>datetime object or None if parsing fails</p>"},{"location":"api/models/#py_gdelt.models.Article.is_english","title":"<code>is_english</code>  <code>property</code>","text":"<p>Check if article is in English.</p>"},{"location":"api/models/#py_gdelt.models.Article.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary for serialization.</p> Source code in <code>src/py_gdelt/models/articles.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to dictionary for serialization.\"\"\"\n    return self.model_dump(by_alias=True)\n</code></pre>"},{"location":"api/models/#py_gdelt.models.Timeline","title":"<code>Timeline</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Timeline data from GDELT DOC API.</p> <p>Contains time series data for article volume.</p> Source code in <code>src/py_gdelt/models/articles.py</code> <pre><code>class Timeline(BaseModel):\n    \"\"\"\n    Timeline data from GDELT DOC API.\n\n    Contains time series data for article volume.\n    \"\"\"\n\n    # The timeline data\n    timeline: list[TimelinePoint] = Field(default_factory=list)\n\n    # Metadata\n    query: str | None = None\n    total_articles: int | None = None\n\n    @field_validator(\"timeline\", mode=\"before\")\n    @classmethod\n    def parse_timeline(cls, v: Any) -&gt; list[TimelinePoint]:\n        \"\"\"Parse timeline from various formats.\n\n        Handles both flat format and nested series format from timelinevol API:\n        - Flat: [{\"date\": \"...\", \"value\": ...}, ...]\n        - Nested: [{\"series\": \"...\", \"data\": [{\"date\": \"...\", \"value\": ...}]}]\n        \"\"\"\n        if v is None:\n            return []\n        if isinstance(v, list):\n            points: list[TimelinePoint] = []\n            for item in v:\n                if isinstance(item, TimelinePoint):\n                    points.append(item)\n                elif isinstance(item, dict):\n                    # Check for nested series/data structure from timelinevol API\n                    if \"data\" in item and isinstance(item[\"data\"], list):\n                        for dp in item[\"data\"]:\n                            if isinstance(dp, dict):\n                                points.append(TimelinePoint.model_validate(dp))\n                            else:\n                                logger.warning(\n                                    \"Skipping non-dict timeline data point: %s\", type(dp).__name__\n                                )\n                    else:\n                        # Flat structure with date/value directly\n                        points.append(TimelinePoint.model_validate(item))\n            return points\n        return []\n\n    @property\n    def points(self) -&gt; list[TimelinePoint]:\n        \"\"\"Alias for timeline for cleaner access.\"\"\"\n        return self.timeline\n\n    @property\n    def dates(self) -&gt; list[str]:\n        \"\"\"Get list of dates.\"\"\"\n        return [p.date for p in self.timeline]\n\n    @property\n    def values(self) -&gt; list[float]:\n        \"\"\"Get list of values.\"\"\"\n        return [p.value for p in self.timeline]\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return {\n            \"timeline\": [p.model_dump() for p in self.timeline],\n            \"query\": self.query,\n            \"total_articles\": self.total_articles,\n        }\n\n    def to_series(self) -&gt; dict[str, float]:\n        \"\"\"\n        Convert to date:value mapping.\n\n        Useful for quick lookups and plotting.\n        \"\"\"\n        return {p.date: p.value for p in self.timeline}\n</code></pre>"},{"location":"api/models/#py_gdelt.models.Timeline.points","title":"<code>points</code>  <code>property</code>","text":"<p>Alias for timeline for cleaner access.</p>"},{"location":"api/models/#py_gdelt.models.Timeline.dates","title":"<code>dates</code>  <code>property</code>","text":"<p>Get list of dates.</p>"},{"location":"api/models/#py_gdelt.models.Timeline.values","title":"<code>values</code>  <code>property</code>","text":"<p>Get list of values.</p>"},{"location":"api/models/#py_gdelt.models.Timeline.parse_timeline","title":"<code>parse_timeline(v)</code>  <code>classmethod</code>","text":"<p>Parse timeline from various formats.</p> <p>Handles both flat format and nested series format from timelinevol API: - Flat: [{\"date\": \"...\", \"value\": ...}, ...] - Nested: [{\"series\": \"...\", \"data\": [{\"date\": \"...\", \"value\": ...}]}]</p> Source code in <code>src/py_gdelt/models/articles.py</code> <pre><code>@field_validator(\"timeline\", mode=\"before\")\n@classmethod\ndef parse_timeline(cls, v: Any) -&gt; list[TimelinePoint]:\n    \"\"\"Parse timeline from various formats.\n\n    Handles both flat format and nested series format from timelinevol API:\n    - Flat: [{\"date\": \"...\", \"value\": ...}, ...]\n    - Nested: [{\"series\": \"...\", \"data\": [{\"date\": \"...\", \"value\": ...}]}]\n    \"\"\"\n    if v is None:\n        return []\n    if isinstance(v, list):\n        points: list[TimelinePoint] = []\n        for item in v:\n            if isinstance(item, TimelinePoint):\n                points.append(item)\n            elif isinstance(item, dict):\n                # Check for nested series/data structure from timelinevol API\n                if \"data\" in item and isinstance(item[\"data\"], list):\n                    for dp in item[\"data\"]:\n                        if isinstance(dp, dict):\n                            points.append(TimelinePoint.model_validate(dp))\n                        else:\n                            logger.warning(\n                                \"Skipping non-dict timeline data point: %s\", type(dp).__name__\n                            )\n                else:\n                    # Flat structure with date/value directly\n                    points.append(TimelinePoint.model_validate(item))\n        return points\n    return []\n</code></pre>"},{"location":"api/models/#py_gdelt.models.Timeline.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary.</p> Source code in <code>src/py_gdelt/models/articles.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to dictionary.\"\"\"\n    return {\n        \"timeline\": [p.model_dump() for p in self.timeline],\n        \"query\": self.query,\n        \"total_articles\": self.total_articles,\n    }\n</code></pre>"},{"location":"api/models/#py_gdelt.models.Timeline.to_series","title":"<code>to_series()</code>","text":"<p>Convert to date:value mapping.</p> <p>Useful for quick lookups and plotting.</p> Source code in <code>src/py_gdelt/models/articles.py</code> <pre><code>def to_series(self) -&gt; dict[str, float]:\n    \"\"\"\n    Convert to date:value mapping.\n\n    Useful for quick lookups and plotting.\n    \"\"\"\n    return {p.date: p.value for p in self.timeline}\n</code></pre>"},{"location":"api/models/#py_gdelt.models.TimelinePoint","title":"<code>TimelinePoint</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single data point in a timeline.</p> Source code in <code>src/py_gdelt/models/articles.py</code> <pre><code>class TimelinePoint(BaseModel):\n    \"\"\"Single data point in a timeline.\"\"\"\n\n    date: str\n    value: float = Field(default=0, alias=\"count\")\n\n    # Optional breakdown\n    tone: float | None = None\n\n    model_config = {\"populate_by_name\": True}\n\n    @property\n    def parsed_date(self) -&gt; datetime | None:\n        \"\"\"Parse date string to datetime.\"\"\"\n        return try_parse_gdelt_datetime(self.date)\n</code></pre>"},{"location":"api/models/#py_gdelt.models.TimelinePoint.parsed_date","title":"<code>parsed_date</code>  <code>property</code>","text":"<p>Parse date string to datetime.</p>"},{"location":"api/models/#gkg-models","title":"GKG Models","text":""},{"location":"api/models/#py_gdelt.models.GKGRecord","title":"<code>GKGRecord</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>GDELT Global Knowledge Graph record.</p> <p>Represents enriched content analysis of a news article or document, including extracted themes, entities, locations, tone, and other metadata.</p> <p>Attributes:</p> Name Type Description <code>record_id</code> <code>str</code> <p>Unique identifier in format \"YYYYMMDDHHMMSS-seq\" or with \"-T\" suffix for translated</p> <code>date</code> <code>datetime</code> <p>Publication date/time</p> <code>source_url</code> <code>str</code> <p>URL of the source document</p> <code>source_name</code> <code>str</code> <p>Common name of the source</p> <code>source_collection</code> <code>int</code> <p>Source collection identifier (1=WEB, 2=Citation, etc.)</p> <code>themes</code> <code>list[EntityMention]</code> <p>Extracted themes/topics</p> <code>persons</code> <code>list[EntityMention]</code> <p>Extracted person names</p> <code>organizations</code> <code>list[EntityMention]</code> <p>Extracted organization names</p> <code>locations</code> <code>list[Location]</code> <p>Extracted geographic locations</p> <code>tone</code> <code>ToneScores | None</code> <p>Document tone analysis scores</p> <code>gcam</code> <code>dict[str, float]</code> <p>GCAM emotional dimension scores as dict</p> <code>quotations</code> <code>list[Quotation]</code> <p>Extracted quotations (v2.1+ only)</p> <code>amounts</code> <code>list[Amount]</code> <p>Extracted numerical amounts (v2.1+ only)</p> <code>sharing_image</code> <code>str | None</code> <p>Primary sharing image URL if any</p> <code>all_names</code> <code>list[str]</code> <p>All extracted names as flat list</p> <code>version</code> <code>int</code> <p>GDELT version (1 or 2)</p> <code>is_translated</code> <code>bool</code> <p>Whether this is a translated document</p> <code>original_record_id</code> <code>str | None</code> <p>Original record ID if translated</p> <code>translation_info</code> <code>str | None</code> <p>Translation metadata string</p> Source code in <code>src/py_gdelt/models/gkg.py</code> <pre><code>class GKGRecord(BaseModel):\n    \"\"\"GDELT Global Knowledge Graph record.\n\n    Represents enriched content analysis of a news article or document, including\n    extracted themes, entities, locations, tone, and other metadata.\n\n    Attributes:\n        record_id: Unique identifier in format \"YYYYMMDDHHMMSS-seq\" or with \"-T\" suffix for translated\n        date: Publication date/time\n        source_url: URL of the source document\n        source_name: Common name of the source\n        source_collection: Source collection identifier (1=WEB, 2=Citation, etc.)\n        themes: Extracted themes/topics\n        persons: Extracted person names\n        organizations: Extracted organization names\n        locations: Extracted geographic locations\n        tone: Document tone analysis scores\n        gcam: GCAM emotional dimension scores as dict\n        quotations: Extracted quotations (v2.1+ only)\n        amounts: Extracted numerical amounts (v2.1+ only)\n        sharing_image: Primary sharing image URL if any\n        all_names: All extracted names as flat list\n        version: GDELT version (1 or 2)\n        is_translated: Whether this is a translated document\n        original_record_id: Original record ID if translated\n        translation_info: Translation metadata string\n    \"\"\"\n\n    # Identifiers\n    record_id: str\n    date: datetime\n    source_url: str\n    source_name: str\n    source_collection: int\n\n    # Extracted entities\n    themes: list[EntityMention] = Field(default_factory=list)\n    persons: list[EntityMention] = Field(default_factory=list)\n    organizations: list[EntityMention] = Field(default_factory=list)\n    locations: list[Location] = Field(default_factory=list)\n\n    # Tone\n    tone: ToneScores | None = None\n\n    # GCAM emotional dimensions\n    gcam: dict[str, float] = Field(default_factory=dict)\n\n    # V2.1+ fields\n    quotations: list[Quotation] = Field(default_factory=list)\n    amounts: list[Amount] = Field(default_factory=list)\n\n    # Extra fields\n    sharing_image: str | None = None\n    all_names: list[str] = Field(default_factory=list)\n\n    # Metadata\n    version: int = 2\n    is_translated: bool = False\n    original_record_id: str | None = None\n    translation_info: str | None = None\n\n    @classmethod\n    def from_raw(cls, raw: _RawGKG) -&gt; GKGRecord:\n        \"\"\"Convert internal _RawGKG to public GKGRecord model.\n\n        This method handles parsing the complex delimited fields from GKG v2.1 format:\n        - Themes: semicolon-delimited \"theme,offset\" pairs\n        - GCAM: semicolon-delimited \"key:value\" pairs\n        - Quotations: pipe-delimited records with format \"offset#length#verb#quote\"\n        - Amounts: semicolon-delimited \"amount,object,offset\" triples\n        - Locations: semicolon-delimited with multiple sub-fields\n        - Tone: comma-separated values\n\n        Args:\n            raw: Internal _RawGKG dataclass from TSV parsing\n\n        Returns:\n            Validated GKGRecord with all fields parsed and typed\n        \"\"\"\n        # Detect translation\n        is_translated = raw.gkg_record_id.endswith(\"-T\")\n        original_record_id = None\n        if is_translated:\n            original_record_id = raw.gkg_record_id[:-2]\n\n        # Parse date (format: YYYYMMDDHHMMSS)\n        date = parse_gdelt_datetime(raw.date)\n\n        # Parse themes from V2 enhanced (preferred) or V1\n        themes = _parse_themes(raw.themes_v2_enhanced or raw.themes_v1)\n\n        # Parse persons\n        persons = _parse_entities(raw.persons_v2_enhanced or raw.persons_v1, entity_type=\"PERSON\")\n\n        # Parse organizations\n        organizations = _parse_entities(\n            raw.organizations_v2_enhanced or raw.organizations_v1,\n            entity_type=\"ORG\",\n        )\n\n        # Parse locations\n        locations = _parse_locations(raw.locations_v2_enhanced or raw.locations_v1)\n\n        # Parse tone\n        tone = _parse_tone(raw.tone) if raw.tone else None\n\n        # Parse GCAM\n        gcam = _parse_gcam(raw.gcam) if raw.gcam else {}\n\n        # Parse quotations (v2.1+)\n        quotations = _parse_quotations(raw.quotations) if raw.quotations else []\n\n        # Parse amounts (v2.1+)\n        amounts = _parse_amounts(raw.amounts) if raw.amounts else []\n\n        # Parse all_names\n        all_names = (\n            [name.strip() for name in raw.all_names.split(\";\") if name.strip()]\n            if raw.all_names\n            else []\n        )\n\n        # Determine version (if v2 enhanced fields exist, it's v2)\n        version = 2 if raw.themes_v2_enhanced else 1\n\n        return cls(\n            record_id=raw.gkg_record_id,\n            date=date,\n            source_url=raw.document_identifier,\n            source_name=raw.source_common_name,\n            source_collection=int(raw.source_collection_id),\n            themes=themes,\n            persons=persons,\n            organizations=organizations,\n            locations=locations,\n            tone=tone,\n            gcam=gcam,\n            quotations=quotations,\n            amounts=amounts,\n            sharing_image=raw.sharing_image,\n            all_names=all_names,\n            version=version,\n            is_translated=is_translated,\n            original_record_id=original_record_id,\n            translation_info=raw.translation_info,\n        )\n\n    @property\n    def primary_theme(self) -&gt; str | None:\n        \"\"\"Get the first/primary theme if any.\n\n        Returns:\n            The name of the first theme, or None if no themes exist\n        \"\"\"\n        if not self.themes:\n            return None\n        return self.themes[0].name\n\n    @property\n    def has_quotations(self) -&gt; bool:\n        \"\"\"Check if record has extracted quotations.\n\n        Returns:\n            True if one or more quotations were extracted\n        \"\"\"\n        return len(self.quotations) &gt; 0\n</code></pre>"},{"location":"api/models/#py_gdelt.models.GKGRecord.primary_theme","title":"<code>primary_theme</code>  <code>property</code>","text":"<p>Get the first/primary theme if any.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>The name of the first theme, or None if no themes exist</p>"},{"location":"api/models/#py_gdelt.models.GKGRecord.has_quotations","title":"<code>has_quotations</code>  <code>property</code>","text":"<p>Check if record has extracted quotations.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if one or more quotations were extracted</p>"},{"location":"api/models/#py_gdelt.models.GKGRecord.from_raw","title":"<code>from_raw(raw)</code>  <code>classmethod</code>","text":"<p>Convert internal _RawGKG to public GKGRecord model.</p> <p>This method handles parsing the complex delimited fields from GKG v2.1 format: - Themes: semicolon-delimited \"theme,offset\" pairs - GCAM: semicolon-delimited \"key:value\" pairs - Quotations: pipe-delimited records with format \"offset#length#verb#quote\" - Amounts: semicolon-delimited \"amount,object,offset\" triples - Locations: semicolon-delimited with multiple sub-fields - Tone: comma-separated values</p> <p>Parameters:</p> Name Type Description Default <code>raw</code> <code>_RawGKG</code> <p>Internal _RawGKG dataclass from TSV parsing</p> required <p>Returns:</p> Type Description <code>GKGRecord</code> <p>Validated GKGRecord with all fields parsed and typed</p> Source code in <code>src/py_gdelt/models/gkg.py</code> <pre><code>@classmethod\ndef from_raw(cls, raw: _RawGKG) -&gt; GKGRecord:\n    \"\"\"Convert internal _RawGKG to public GKGRecord model.\n\n    This method handles parsing the complex delimited fields from GKG v2.1 format:\n    - Themes: semicolon-delimited \"theme,offset\" pairs\n    - GCAM: semicolon-delimited \"key:value\" pairs\n    - Quotations: pipe-delimited records with format \"offset#length#verb#quote\"\n    - Amounts: semicolon-delimited \"amount,object,offset\" triples\n    - Locations: semicolon-delimited with multiple sub-fields\n    - Tone: comma-separated values\n\n    Args:\n        raw: Internal _RawGKG dataclass from TSV parsing\n\n    Returns:\n        Validated GKGRecord with all fields parsed and typed\n    \"\"\"\n    # Detect translation\n    is_translated = raw.gkg_record_id.endswith(\"-T\")\n    original_record_id = None\n    if is_translated:\n        original_record_id = raw.gkg_record_id[:-2]\n\n    # Parse date (format: YYYYMMDDHHMMSS)\n    date = parse_gdelt_datetime(raw.date)\n\n    # Parse themes from V2 enhanced (preferred) or V1\n    themes = _parse_themes(raw.themes_v2_enhanced or raw.themes_v1)\n\n    # Parse persons\n    persons = _parse_entities(raw.persons_v2_enhanced or raw.persons_v1, entity_type=\"PERSON\")\n\n    # Parse organizations\n    organizations = _parse_entities(\n        raw.organizations_v2_enhanced or raw.organizations_v1,\n        entity_type=\"ORG\",\n    )\n\n    # Parse locations\n    locations = _parse_locations(raw.locations_v2_enhanced or raw.locations_v1)\n\n    # Parse tone\n    tone = _parse_tone(raw.tone) if raw.tone else None\n\n    # Parse GCAM\n    gcam = _parse_gcam(raw.gcam) if raw.gcam else {}\n\n    # Parse quotations (v2.1+)\n    quotations = _parse_quotations(raw.quotations) if raw.quotations else []\n\n    # Parse amounts (v2.1+)\n    amounts = _parse_amounts(raw.amounts) if raw.amounts else []\n\n    # Parse all_names\n    all_names = (\n        [name.strip() for name in raw.all_names.split(\";\") if name.strip()]\n        if raw.all_names\n        else []\n    )\n\n    # Determine version (if v2 enhanced fields exist, it's v2)\n    version = 2 if raw.themes_v2_enhanced else 1\n\n    return cls(\n        record_id=raw.gkg_record_id,\n        date=date,\n        source_url=raw.document_identifier,\n        source_name=raw.source_common_name,\n        source_collection=int(raw.source_collection_id),\n        themes=themes,\n        persons=persons,\n        organizations=organizations,\n        locations=locations,\n        tone=tone,\n        gcam=gcam,\n        quotations=quotations,\n        amounts=amounts,\n        sharing_image=raw.sharing_image,\n        all_names=all_names,\n        version=version,\n        is_translated=is_translated,\n        original_record_id=original_record_id,\n        translation_info=raw.translation_info,\n    )\n</code></pre>"},{"location":"api/models/#py_gdelt.models.Quotation","title":"<code>Quotation</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Quote extracted from a GKG document.</p> Source code in <code>src/py_gdelt/models/gkg.py</code> <pre><code>class Quotation(BaseModel):\n    \"\"\"Quote extracted from a GKG document.\"\"\"\n\n    offset: int\n    length: int\n    verb: str\n    quote: str\n</code></pre>"},{"location":"api/models/#py_gdelt.models.Amount","title":"<code>Amount</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Numerical amount extracted from a GKG document.</p> Source code in <code>src/py_gdelt/models/gkg.py</code> <pre><code>class Amount(BaseModel):\n    \"\"\"Numerical amount extracted from a GKG document.\"\"\"\n\n    amount: float\n    object: str\n    offset: int\n</code></pre>"},{"location":"api/models/#py_gdelt.models.TVGKGRecord","title":"<code>TVGKGRecord</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>TV-GKG record with timecode mapping support.</p> <p>Uses composition instead of inheritance from GKGRecord. Reuses GKG parsing logic but creates independent model.</p> Note <p>The following GKG fields are always empty in TV-GKG: - sharing_image, related_images, social_image_embeds, social_video_embeds - quotations, all_names, dates, amounts, translation_info</p> Special field <ul> <li>timecode_mappings: Parsed from CHARTIMECODEOFFSETTOC in extras</li> </ul> <p>Attributes:</p> Name Type Description <code>gkg_record_id</code> <code>str</code> <p>Unique record identifier.</p> <code>date</code> <code>datetime</code> <p>Timestamp of the analysis.</p> <code>source_identifier</code> <code>str</code> <p>Source collection identifier.</p> <code>document_identifier</code> <code>str</code> <p>URL or identifier of the source document.</p> <code>themes</code> <code>list[str]</code> <p>List of detected themes.</p> <code>locations</code> <code>list[str]</code> <p>List of detected locations.</p> <code>persons</code> <code>list[str]</code> <p>List of detected persons.</p> <code>organizations</code> <code>list[str]</code> <p>List of detected organizations.</p> <code>tone</code> <code>float | None</code> <p>Average tone score.</p> <code>extras</code> <code>str</code> <p>Raw extras field content.</p> <code>timecode_mappings</code> <code>list[TimecodeMapping]</code> <p>Parsed character offset to timecode mappings.</p> Source code in <code>src/py_gdelt/models/gkg.py</code> <pre><code>class TVGKGRecord(BaseModel):\n    \"\"\"TV-GKG record with timecode mapping support.\n\n    Uses composition instead of inheritance from GKGRecord.\n    Reuses GKG parsing logic but creates independent model.\n\n    Note:\n        The following GKG fields are always empty in TV-GKG:\n        - sharing_image, related_images, social_image_embeds, social_video_embeds\n        - quotations, all_names, dates, amounts, translation_info\n\n    Special field:\n        - timecode_mappings: Parsed from CHARTIMECODEOFFSETTOC in extras\n\n    Attributes:\n        gkg_record_id: Unique record identifier.\n        date: Timestamp of the analysis.\n        source_identifier: Source collection identifier.\n        document_identifier: URL or identifier of the source document.\n        themes: List of detected themes.\n        locations: List of detected locations.\n        persons: List of detected persons.\n        organizations: List of detected organizations.\n        tone: Average tone score.\n        extras: Raw extras field content.\n        timecode_mappings: Parsed character offset to timecode mappings.\n    \"\"\"\n\n    gkg_record_id: str\n    date: datetime\n    source_identifier: str\n    document_identifier: str\n    themes: list[str] = Field(default_factory=list)\n    locations: list[str] = Field(default_factory=list)\n    persons: list[str] = Field(default_factory=list)\n    organizations: list[str] = Field(default_factory=list)\n    tone: float | None = None\n    extras: str = \"\"\n    timecode_mappings: list[TimecodeMapping] = Field(default_factory=list)\n\n    @classmethod\n    def from_raw(cls, raw: _RawGKG) -&gt; TVGKGRecord:\n        \"\"\"Convert raw GKG to TV-GKG with timecode extraction.\n\n        Args:\n            raw: Internal raw GKG representation.\n\n        Returns:\n            Validated TVGKGRecord instance.\n\n        Raises:\n            ValueError: If date parsing fails.\n        \"\"\"\n        timecodes = cls._parse_timecode_toc(raw.extras_xml)\n        return cls(\n            gkg_record_id=raw.gkg_record_id,\n            date=parse_gdelt_datetime(raw.date),\n            source_identifier=raw.source_common_name or \"\",\n            document_identifier=raw.document_identifier or \"\",\n            themes=_parse_semicolon_delimited(raw.themes_v1),\n            locations=_parse_semicolon_delimited(raw.locations_v1),\n            persons=_parse_semicolon_delimited(raw.persons_v1),\n            organizations=_parse_semicolon_delimited(raw.organizations_v1),\n            tone=_parse_tone_simple(raw.tone),\n            extras=raw.extras_xml or \"\",\n            timecode_mappings=timecodes,\n        )\n\n    @staticmethod\n    def _parse_timecode_toc(extras: str | None) -&gt; list[TimecodeMapping]:\n        \"\"\"Parse CHARTIMECODEOFFSETTOC:offset:timecode;offset:timecode;...\n\n        Real format discovered: offset:timecode pairs separated by semicolons.\n\n        Args:\n            extras: Raw extras XML/text field from GKG record.\n\n        Returns:\n            List of TimecodeMapping instances.\n        \"\"\"\n        if not extras:\n            return []\n        # Look for CHARTIMECODEOFFSETTOC block in extras\n        for block in extras.split(\"&lt;SPECIAL&gt;\"):\n            if block.startswith(\"CHARTIMECODEOFFSETTOC:\"):\n                content = block[len(\"CHARTIMECODEOFFSETTOC:\") :]\n                mappings: list[TimecodeMapping] = []\n                for raw_entry in content.split(\";\"):\n                    entry = raw_entry.strip()\n                    if \":\" in entry:\n                        parts = entry.split(\":\", 1)\n                        try:\n                            mappings.append(\n                                TimecodeMapping(\n                                    char_offset=int(parts[0]),\n                                    timecode=parts[1],\n                                ),\n                            )\n                        except (ValueError, IndexError):\n                            logger.debug(\"Failed to parse timecode entry: %r\", entry)\n                            continue\n                logger.debug(\"Parsed %d timecode mappings\", len(mappings))\n                return mappings\n        # No CHARTIMECODEOFFSETTOC block found - this is normal for non-TV records\n        if \"&lt;SPECIAL&gt;\" in extras:\n            logger.debug(\"SPECIAL blocks present but no CHARTIMECODEOFFSETTOC found\")\n        return []\n</code></pre>"},{"location":"api/models/#py_gdelt.models.TVGKGRecord.from_raw","title":"<code>from_raw(raw)</code>  <code>classmethod</code>","text":"<p>Convert raw GKG to TV-GKG with timecode extraction.</p> <p>Parameters:</p> Name Type Description Default <code>raw</code> <code>_RawGKG</code> <p>Internal raw GKG representation.</p> required <p>Returns:</p> Type Description <code>TVGKGRecord</code> <p>Validated TVGKGRecord instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If date parsing fails.</p> Source code in <code>src/py_gdelt/models/gkg.py</code> <pre><code>@classmethod\ndef from_raw(cls, raw: _RawGKG) -&gt; TVGKGRecord:\n    \"\"\"Convert raw GKG to TV-GKG with timecode extraction.\n\n    Args:\n        raw: Internal raw GKG representation.\n\n    Returns:\n        Validated TVGKGRecord instance.\n\n    Raises:\n        ValueError: If date parsing fails.\n    \"\"\"\n    timecodes = cls._parse_timecode_toc(raw.extras_xml)\n    return cls(\n        gkg_record_id=raw.gkg_record_id,\n        date=parse_gdelt_datetime(raw.date),\n        source_identifier=raw.source_common_name or \"\",\n        document_identifier=raw.document_identifier or \"\",\n        themes=_parse_semicolon_delimited(raw.themes_v1),\n        locations=_parse_semicolon_delimited(raw.locations_v1),\n        persons=_parse_semicolon_delimited(raw.persons_v1),\n        organizations=_parse_semicolon_delimited(raw.organizations_v1),\n        tone=_parse_tone_simple(raw.tone),\n        extras=raw.extras_xml or \"\",\n        timecode_mappings=timecodes,\n    )\n</code></pre>"},{"location":"api/models/#py_gdelt.models.TimecodeMapping","title":"<code>TimecodeMapping</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Character offset to video timecode mapping (lightweight).</p> <p>Used in TV-GKG to map text positions to video timestamps.</p> <p>Attributes:</p> Name Type Description <code>char_offset</code> <code>int</code> <p>Character offset in the transcript.</p> <code>timecode</code> <code>str</code> <p>Video timecode (e.g., \"00:01:23\").</p> Source code in <code>src/py_gdelt/models/gkg.py</code> <pre><code>class TimecodeMapping(NamedTuple):\n    \"\"\"Character offset to video timecode mapping (lightweight).\n\n    Used in TV-GKG to map text positions to video timestamps.\n\n    Attributes:\n        char_offset: Character offset in the transcript.\n        timecode: Video timecode (e.g., \"00:01:23\").\n    \"\"\"\n\n    char_offset: int\n    timecode: str\n</code></pre>"},{"location":"api/models/#ngrams-models","title":"NGrams Models","text":""},{"location":"api/models/#py_gdelt.models.NGramRecord","title":"<code>NGramRecord</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>GDELT NGram 3.0 record.</p> <p>Represents an n-gram (word or phrase) occurrence in web content, including context and source information.</p> Source code in <code>src/py_gdelt/models/ngrams.py</code> <pre><code>class NGramRecord(BaseModel):\n    \"\"\"GDELT NGram 3.0 record.\n\n    Represents an n-gram (word or phrase) occurrence in web content,\n    including context and source information.\n    \"\"\"\n\n    date: datetime\n    ngram: str  # Word or character\n    language: str  # ISO 639-1/2\n    segment_type: int  # 1=space-delimited, 2=scriptio continua\n    position: int  # Article decile (0-90, where 0 = first 10% of article)\n    pre_context: str  # ~7 words before\n    post_context: str  # ~7 words after\n    url: str\n\n    @classmethod\n    def from_raw(cls, raw: _RawNGram) -&gt; NGramRecord:\n        \"\"\"Convert internal _RawNGram to public NGramRecord model.\n\n        Args:\n            raw: Internal raw ngram representation with string fields\n\n        Returns:\n            Validated NGramRecord instance\n\n        Raises:\n            ValueError: If date parsing or type conversion fails\n        \"\"\"\n        return cls(\n            date=parse_gdelt_datetime(raw.date),\n            ngram=raw.ngram,\n            language=raw.language,\n            segment_type=int(raw.segment_type),\n            position=int(raw.position),\n            pre_context=raw.pre_context,\n            post_context=raw.post_context,\n            url=raw.url,\n        )\n\n    @property\n    def context(self) -&gt; str:\n        \"\"\"Get full context (pre + ngram + post).\n\n        Returns:\n            Full context string with ngram surrounded by pre and post context\n        \"\"\"\n        return f\"{self.pre_context} {self.ngram} {self.post_context}\"\n\n    @property\n    def is_early_in_article(self) -&gt; bool:\n        \"\"\"Check if ngram appears in first 30% of article.\n\n        Returns:\n            True if position &lt;= 20 (first 30% of article)\n        \"\"\"\n        return self.position &lt;= 20\n\n    @property\n    def is_late_in_article(self) -&gt; bool:\n        \"\"\"Check if ngram appears in last 30% of article.\n\n        Returns:\n            True if position &gt;= 70 (last 30% of article)\n        \"\"\"\n        return self.position &gt;= 70\n</code></pre>"},{"location":"api/models/#py_gdelt.models.NGramRecord.context","title":"<code>context</code>  <code>property</code>","text":"<p>Get full context (pre + ngram + post).</p> <p>Returns:</p> Type Description <code>str</code> <p>Full context string with ngram surrounded by pre and post context</p>"},{"location":"api/models/#py_gdelt.models.NGramRecord.is_early_in_article","title":"<code>is_early_in_article</code>  <code>property</code>","text":"<p>Check if ngram appears in first 30% of article.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if position &lt;= 20 (first 30% of article)</p>"},{"location":"api/models/#py_gdelt.models.NGramRecord.is_late_in_article","title":"<code>is_late_in_article</code>  <code>property</code>","text":"<p>Check if ngram appears in last 30% of article.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if position &gt;= 70 (last 30% of article)</p>"},{"location":"api/models/#py_gdelt.models.NGramRecord.from_raw","title":"<code>from_raw(raw)</code>  <code>classmethod</code>","text":"<p>Convert internal _RawNGram to public NGramRecord model.</p> <p>Parameters:</p> Name Type Description Default <code>raw</code> <code>_RawNGram</code> <p>Internal raw ngram representation with string fields</p> required <p>Returns:</p> Type Description <code>NGramRecord</code> <p>Validated NGramRecord instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If date parsing or type conversion fails</p> Source code in <code>src/py_gdelt/models/ngrams.py</code> <pre><code>@classmethod\ndef from_raw(cls, raw: _RawNGram) -&gt; NGramRecord:\n    \"\"\"Convert internal _RawNGram to public NGramRecord model.\n\n    Args:\n        raw: Internal raw ngram representation with string fields\n\n    Returns:\n        Validated NGramRecord instance\n\n    Raises:\n        ValueError: If date parsing or type conversion fails\n    \"\"\"\n    return cls(\n        date=parse_gdelt_datetime(raw.date),\n        ngram=raw.ngram,\n        language=raw.language,\n        segment_type=int(raw.segment_type),\n        position=int(raw.position),\n        pre_context=raw.pre_context,\n        post_context=raw.post_context,\n        url=raw.url,\n    )\n</code></pre>"},{"location":"api/models/#py_gdelt.models.BroadcastNGramRecord","title":"<code>BroadcastNGramRecord</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Broadcast NGram frequency record (TV or Radio).</p> <p>Unified model for both TV and Radio NGrams since schemas are compatible. TV NGrams: 5 columns (DATE, STATION, HOUR, WORD, COUNT) Radio NGrams: 6 columns (DATE, STATION, HOUR, NGRAM, COUNT, SHOW)</p> <p>Attributes:</p> Name Type Description <code>date</code> <code>date</code> <p>Date of the broadcast.</p> <code>station</code> <code>str</code> <p>Station identifier (e.g., CNN, KQED).</p> <code>hour</code> <code>int</code> <p>Hour of broadcast (0-23).</p> <code>ngram</code> <code>str</code> <p>Word or phrase.</p> <code>count</code> <code>int</code> <p>Frequency count.</p> <code>show</code> <code>str | None</code> <p>Show name (Radio only, None for TV).</p> <code>source</code> <code>BroadcastSource</code> <p>Indicates origin (tv or radio).</p> Source code in <code>src/py_gdelt/models/ngrams.py</code> <pre><code>class BroadcastNGramRecord(BaseModel):\n    \"\"\"Broadcast NGram frequency record (TV or Radio).\n\n    Unified model for both TV and Radio NGrams since schemas are compatible.\n    TV NGrams: 5 columns (DATE, STATION, HOUR, WORD, COUNT)\n    Radio NGrams: 6 columns (DATE, STATION, HOUR, NGRAM, COUNT, SHOW)\n\n    Attributes:\n        date: Date of the broadcast.\n        station: Station identifier (e.g., CNN, KQED).\n        hour: Hour of broadcast (0-23).\n        ngram: Word or phrase.\n        count: Frequency count.\n        show: Show name (Radio only, None for TV).\n        source: Indicates origin (tv or radio).\n    \"\"\"\n\n    date: date_type\n    station: str\n    hour: int = Field(ge=0, le=23)\n    ngram: str\n    count: int = Field(ge=0)\n    show: str | None = None\n    source: BroadcastSource\n\n    @classmethod\n    def from_raw(cls, raw: _RawBroadcastNGram, source: BroadcastSource) -&gt; BroadcastNGramRecord:\n        \"\"\"Convert internal _RawBroadcastNGram to public model.\n\n        Args:\n            raw: Internal raw broadcast ngram representation.\n            source: Source type (TV or Radio).\n\n        Returns:\n            Validated BroadcastNGramRecord instance.\n\n        Raises:\n            ValueError: If date parsing or type conversion fails.\n        \"\"\"\n        return cls(\n            date=parse_gdelt_date(raw.date),\n            station=raw.station,\n            hour=int(raw.hour),\n            ngram=raw.ngram,\n            count=int(raw.count),\n            show=raw.show if raw.show else None,\n            source=source,\n        )\n</code></pre>"},{"location":"api/models/#py_gdelt.models.BroadcastNGramRecord.from_raw","title":"<code>from_raw(raw, source)</code>  <code>classmethod</code>","text":"<p>Convert internal _RawBroadcastNGram to public model.</p> <p>Parameters:</p> Name Type Description Default <code>raw</code> <code>_RawBroadcastNGram</code> <p>Internal raw broadcast ngram representation.</p> required <code>source</code> <code>BroadcastSource</code> <p>Source type (TV or Radio).</p> required <p>Returns:</p> Type Description <code>BroadcastNGramRecord</code> <p>Validated BroadcastNGramRecord instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If date parsing or type conversion fails.</p> Source code in <code>src/py_gdelt/models/ngrams.py</code> <pre><code>@classmethod\ndef from_raw(cls, raw: _RawBroadcastNGram, source: BroadcastSource) -&gt; BroadcastNGramRecord:\n    \"\"\"Convert internal _RawBroadcastNGram to public model.\n\n    Args:\n        raw: Internal raw broadcast ngram representation.\n        source: Source type (TV or Radio).\n\n    Returns:\n        Validated BroadcastNGramRecord instance.\n\n    Raises:\n        ValueError: If date parsing or type conversion fails.\n    \"\"\"\n    return cls(\n        date=parse_gdelt_date(raw.date),\n        station=raw.station,\n        hour=int(raw.hour),\n        ngram=raw.ngram,\n        count=int(raw.count),\n        show=raw.show if raw.show else None,\n        source=source,\n    )\n</code></pre>"},{"location":"api/models/#py_gdelt.models.BroadcastSource","title":"<code>BroadcastSource</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Source type for broadcast NGrams.</p> Source code in <code>src/py_gdelt/models/ngrams.py</code> <pre><code>class BroadcastSource(str, Enum):\n    \"\"\"Source type for broadcast NGrams.\"\"\"\n\n    TV = \"tv\"\n    RADIO = \"radio\"\n</code></pre>"},{"location":"api/models/#py_gdelt.models.TVNGramRecord","title":"<code>TVNGramRecord = BroadcastNGramRecord</code>  <code>module-attribute</code>","text":""},{"location":"api/models/#py_gdelt.models.RadioNGramRecord","title":"<code>RadioNGramRecord = BroadcastNGramRecord</code>  <code>module-attribute</code>","text":""},{"location":"api/models/#vgkg-models","title":"VGKG Models","text":""},{"location":"api/models/#py_gdelt.models.VGKGRecord","title":"<code>VGKGRecord</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Visual GKG record with Cloud Vision annotations.</p> <p>Nested structures (labels, faces, etc.) use TypedDict for performance. See tests/benchmarks/test_bench_vgkg_parsing.py for rationale.</p> <p>Attributes:</p> Name Type Description <code>date</code> <code>datetime</code> <p>Timestamp of the analysis.</p> <code>document_identifier</code> <code>str</code> <p>Source article URL.</p> <code>image_url</code> <code>str</code> <p>URL of the analyzed image.</p> <code>labels</code> <code>list[VisionLabelDict]</code> <p>List of detected labels with confidence scores.</p> <code>logos</code> <code>list[VisionLabelDict]</code> <p>List of detected logos.</p> <code>web_entities</code> <code>list[VisionLabelDict]</code> <p>List of web entities matched to the image.</p> <code>safe_search</code> <code>SafeSearchDict | None</code> <p>SafeSearch annotation scores.</p> <code>faces</code> <code>list[FaceAnnotationDict]</code> <p>List of detected faces with pose information.</p> <code>ocr_text</code> <code>str</code> <p>Text extracted via OCR.</p> <code>landmark_annotations</code> <code>list[VisionLabelDict]</code> <p>List of detected landmarks.</p> <code>domain</code> <code>str</code> <p>Domain of the source article.</p> <code>raw_json</code> <code>str</code> <p>Full Cloud Vision API JSON response.</p> Source code in <code>src/py_gdelt/models/vgkg.py</code> <pre><code>class VGKGRecord(BaseModel):\n    \"\"\"Visual GKG record with Cloud Vision annotations.\n\n    Nested structures (labels, faces, etc.) use TypedDict for performance.\n    See tests/benchmarks/test_bench_vgkg_parsing.py for rationale.\n\n    Attributes:\n        date: Timestamp of the analysis.\n        document_identifier: Source article URL.\n        image_url: URL of the analyzed image.\n        labels: List of detected labels with confidence scores.\n        logos: List of detected logos.\n        web_entities: List of web entities matched to the image.\n        safe_search: SafeSearch annotation scores.\n        faces: List of detected faces with pose information.\n        ocr_text: Text extracted via OCR.\n        landmark_annotations: List of detected landmarks.\n        domain: Domain of the source article.\n        raw_json: Full Cloud Vision API JSON response.\n    \"\"\"\n\n    date: datetime\n    document_identifier: str\n    image_url: str\n    labels: list[VisionLabelDict] = Field(default_factory=list)\n    logos: list[VisionLabelDict] = Field(default_factory=list)\n    web_entities: list[VisionLabelDict] = Field(default_factory=list)\n    safe_search: SafeSearchDict | None = None\n    faces: list[FaceAnnotationDict] = Field(default_factory=list)\n    ocr_text: str = \"\"\n    landmark_annotations: list[VisionLabelDict] = Field(default_factory=list)\n    domain: str = \"\"\n    raw_json: str = \"\"  # Keep as string - users can parse with json.loads() if needed\n\n    @classmethod\n    def from_raw(cls, raw: _RawVGKG) -&gt; VGKGRecord:\n        \"\"\"Convert internal _RawVGKG to validated VGKGRecord model.\n\n        Args:\n            raw: Internal raw VGKG representation with string fields.\n\n        Returns:\n            Validated VGKGRecord instance.\n\n        Raises:\n            ValueError: If date parsing or type conversion fails.\n        \"\"\"\n        return cls(\n            date=parse_gdelt_datetime(raw.date),\n            document_identifier=raw.document_identifier,\n            image_url=raw.image_url,\n            labels=cls._parse_labels(raw.labels),\n            logos=cls._parse_labels(raw.logos),\n            web_entities=cls._parse_labels(raw.web_entities),\n            safe_search=cls._parse_safe_search(raw.safe_search),\n            faces=cls._parse_faces(raw.faces),\n            ocr_text=raw.ocr_text or \"\",\n            landmark_annotations=cls._parse_labels(raw.landmark_annotations),\n            domain=raw.domain or \"\",\n            raw_json=raw.raw_json or \"\",\n        )\n\n    @classmethod\n    def _parse_labels(cls, raw: str) -&gt; list[VisionLabelDict]:\n        \"\"\"Parse Label&lt;FIELD&gt;Confidence&lt;FIELD&gt;MID&lt;RECORD&gt;... format.\n\n        Args:\n            raw: Delimited string with label records.\n\n        Returns:\n            List of VisionLabelDict objects.\n        \"\"\"\n        if not raw:\n            return []\n        labels: list[VisionLabelDict] = []\n        for record in raw.split(_RECORD_DELIM):\n            if not record.strip():\n                continue\n            fields = record.split(_FIELD_DELIM)\n            if len(fields) &gt;= 2:\n                try:\n                    labels.append(\n                        {\n                            \"description\": fields[0],\n                            \"confidence\": float(fields[1]) if fields[1] else 0.0,\n                            \"mid\": fields[2] if len(fields) &gt; 2 and fields[2] else None,\n                        }\n                    )\n                except (ValueError, IndexError):\n                    continue\n        return labels\n\n    @classmethod\n    def _parse_safe_search(cls, raw: str) -&gt; SafeSearchDict | None:\n        \"\"\"Parse safe_search field (4 integers).\n\n        Args:\n            raw: Delimited string with 4 safe search scores.\n\n        Returns:\n            SafeSearchDict or None if parsing fails.\n        \"\"\"\n        if not raw:\n            return None\n        fields = raw.split(_FIELD_DELIM)\n        if len(fields) &lt; 4:\n            return None\n        try:\n            return {\n                \"adult\": int(fields[0]) if fields[0] else -1,\n                \"spoof\": int(fields[1]) if fields[1] else -1,\n                \"medical\": int(fields[2]) if fields[2] else -1,\n                \"violence\": int(fields[3]) if fields[3] else -1,\n            }\n        except (ValueError, IndexError):\n            return None\n\n    @classmethod\n    def _parse_faces(cls, raw: str) -&gt; list[FaceAnnotationDict]:\n        \"\"\"Parse faces field (roll/pan/tilt angles, NOT emotions).\n\n        Args:\n            raw: Delimited string with face records.\n\n        Returns:\n            List of FaceAnnotationDict objects.\n        \"\"\"\n        if not raw:\n            return []\n        faces: list[FaceAnnotationDict] = []\n        for record in raw.split(_RECORD_DELIM):\n            if not record.strip():\n                continue\n            fields = record.split(_FIELD_DELIM)\n            if len(fields) &gt;= 5:\n                try:\n                    faces.append(\n                        {\n                            \"confidence\": float(fields[0]) if fields[0] else 0.0,\n                            \"roll\": float(fields[1]) if fields[1] else 0.0,\n                            \"pan\": float(fields[2]) if fields[2] else 0.0,\n                            \"tilt\": float(fields[3]) if fields[3] else 0.0,\n                            \"detection_confidence\": float(fields[4]) if fields[4] else 0.0,\n                            \"bounding_box\": fields[5] if len(fields) &gt; 5 and fields[5] else None,\n                        }\n                    )\n                except (ValueError, IndexError):\n                    continue\n        return faces\n</code></pre>"},{"location":"api/models/#py_gdelt.models.VGKGRecord.from_raw","title":"<code>from_raw(raw)</code>  <code>classmethod</code>","text":"<p>Convert internal _RawVGKG to validated VGKGRecord model.</p> <p>Parameters:</p> Name Type Description Default <code>raw</code> <code>_RawVGKG</code> <p>Internal raw VGKG representation with string fields.</p> required <p>Returns:</p> Type Description <code>VGKGRecord</code> <p>Validated VGKGRecord instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If date parsing or type conversion fails.</p> Source code in <code>src/py_gdelt/models/vgkg.py</code> <pre><code>@classmethod\ndef from_raw(cls, raw: _RawVGKG) -&gt; VGKGRecord:\n    \"\"\"Convert internal _RawVGKG to validated VGKGRecord model.\n\n    Args:\n        raw: Internal raw VGKG representation with string fields.\n\n    Returns:\n        Validated VGKGRecord instance.\n\n    Raises:\n        ValueError: If date parsing or type conversion fails.\n    \"\"\"\n    return cls(\n        date=parse_gdelt_datetime(raw.date),\n        document_identifier=raw.document_identifier,\n        image_url=raw.image_url,\n        labels=cls._parse_labels(raw.labels),\n        logos=cls._parse_labels(raw.logos),\n        web_entities=cls._parse_labels(raw.web_entities),\n        safe_search=cls._parse_safe_search(raw.safe_search),\n        faces=cls._parse_faces(raw.faces),\n        ocr_text=raw.ocr_text or \"\",\n        landmark_annotations=cls._parse_labels(raw.landmark_annotations),\n        domain=raw.domain or \"\",\n        raw_json=raw.raw_json or \"\",\n    )\n</code></pre>"},{"location":"api/models/#py_gdelt.models.VisionLabelDict","title":"<code>VisionLabelDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Google Cloud Vision label annotation (lightweight).</p> <p>Used for labels, logos, web_entities, and landmark_annotations fields. TypedDict avoids Pydantic validation overhead for nested structures.</p> Source code in <code>src/py_gdelt/models/vgkg.py</code> <pre><code>class VisionLabelDict(TypedDict):\n    \"\"\"Google Cloud Vision label annotation (lightweight).\n\n    Used for labels, logos, web_entities, and landmark_annotations fields.\n    TypedDict avoids Pydantic validation overhead for nested structures.\n    \"\"\"\n\n    description: str\n    confidence: float\n    mid: str | None  # Knowledge Graph MID - useful for entity linking\n</code></pre>"},{"location":"api/models/#py_gdelt.models.SafeSearchDict","title":"<code>SafeSearchDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>SafeSearch detection results.</p> <p>Values are integers from Cloud Vision API: - UNKNOWN = -1 - VERY_UNLIKELY = 0 - UNLIKELY = 1 - POSSIBLE = 2 - LIKELY = 3 - VERY_LIKELY = 4</p> Source code in <code>src/py_gdelt/models/vgkg.py</code> <pre><code>class SafeSearchDict(TypedDict):\n    \"\"\"SafeSearch detection results.\n\n    Values are integers from Cloud Vision API:\n    - UNKNOWN = -1\n    - VERY_UNLIKELY = 0\n    - UNLIKELY = 1\n    - POSSIBLE = 2\n    - LIKELY = 3\n    - VERY_LIKELY = 4\n    \"\"\"\n\n    adult: int\n    spoof: int\n    medical: int\n    violence: int\n</code></pre>"},{"location":"api/models/#py_gdelt.models.FaceAnnotationDict","title":"<code>FaceAnnotationDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Detected face with pose angles.</p> <p>Contains pose information (roll/pan/tilt angles), NOT emotion scores. Angles are in degrees relative to the camera.</p> Source code in <code>src/py_gdelt/models/vgkg.py</code> <pre><code>class FaceAnnotationDict(TypedDict):\n    \"\"\"Detected face with pose angles.\n\n    Contains pose information (roll/pan/tilt angles), NOT emotion scores.\n    Angles are in degrees relative to the camera.\n    \"\"\"\n\n    confidence: float\n    roll: float  # Head roll angle in degrees\n    pan: float  # Head pan angle in degrees\n    tilt: float  # Head tilt angle in degrees\n    detection_confidence: float\n    bounding_box: str | None  # Format: \"x1,y1,x2,y2\"\n</code></pre>"},{"location":"api/models/#graph-models","title":"Graph Models","text":""},{"location":"api/models/#py_gdelt.models.Entity","title":"<code>Entity</code>","text":"<p>               Bases: <code>SchemaEvolutionMixin</code>, <code>BaseModel</code></p> <p>Entity extracted from GEG (Global Entity Graph).</p> <p>Represents a named entity with metadata and knowledge graph links.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Entity name</p> <code>entity_type</code> <code>str</code> <p>Entity type</p> <code>salience</code> <code>float | None</code> <p>Salience score</p> <code>wikipedia_url</code> <code>str | None</code> <p>Wikipedia URL if available</p> <code>knowledge_graph_mid</code> <code>str | None</code> <p>Google Knowledge Graph MID if available</p> Source code in <code>src/py_gdelt/models/graphs.py</code> <pre><code>class Entity(SchemaEvolutionMixin, BaseModel):\n    \"\"\"Entity extracted from GEG (Global Entity Graph).\n\n    Represents a named entity with metadata and knowledge graph links.\n\n    Attributes:\n        name: Entity name\n        entity_type: Entity type\n        salience: Salience score\n        wikipedia_url: Wikipedia URL if available\n        knowledge_graph_mid: Google Knowledge Graph MID if available\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"ignore\", populate_by_name=True)\n\n    name: str\n    entity_type: str = Field(alias=\"type\")\n    salience: float | None = None\n    wikipedia_url: str | None = None\n    knowledge_graph_mid: str | None = Field(default=None, alias=\"mid\")\n</code></pre>"},{"location":"api/models/#py_gdelt.models.GALRecord","title":"<code>GALRecord</code>","text":"<p>               Bases: <code>SchemaEvolutionMixin</code>, <code>BaseModel</code></p> <p>Article List record.</p> <p>Represents a news article with basic metadata.</p> <p>Attributes:</p> Name Type Description <code>date</code> <code>datetime</code> <p>Publication date/time</p> <code>url</code> <code>str</code> <p>Article URL</p> <code>title</code> <code>str | None</code> <p>Article title if available</p> <code>image</code> <code>str | None</code> <p>Primary image URL if available</p> <code>description</code> <code>str | None</code> <p>Article description if available</p> <code>author</code> <code>str | None</code> <p>Article author if available</p> <code>lang</code> <code>str</code> <p>Language code</p> Source code in <code>src/py_gdelt/models/graphs.py</code> <pre><code>class GALRecord(SchemaEvolutionMixin, BaseModel):\n    \"\"\"Article List record.\n\n    Represents a news article with basic metadata.\n\n    Attributes:\n        date: Publication date/time\n        url: Article URL\n        title: Article title if available\n        image: Primary image URL if available\n        description: Article description if available\n        author: Article author if available\n        lang: Language code\n    \"\"\"\n\n    date: datetime\n    url: str\n    title: str | None = None\n    image: str | None = None\n    description: str | None = None\n    author: str | None = None\n    lang: str\n\n    @field_validator(\"date\", mode=\"before\")\n    @classmethod\n    def parse_date(cls, v: Any) -&gt; datetime:\n        \"\"\"Parse date from ISO or GDELT format.\"\"\"\n        return parse_gdelt_datetime(v)\n</code></pre>"},{"location":"api/models/#py_gdelt.models.GALRecord.parse_date","title":"<code>parse_date(v)</code>  <code>classmethod</code>","text":"<p>Parse date from ISO or GDELT format.</p> Source code in <code>src/py_gdelt/models/graphs.py</code> <pre><code>@field_validator(\"date\", mode=\"before\")\n@classmethod\ndef parse_date(cls, v: Any) -&gt; datetime:\n    \"\"\"Parse date from ISO or GDELT format.\"\"\"\n    return parse_gdelt_datetime(v)\n</code></pre>"},{"location":"api/models/#py_gdelt.models.GEGRecord","title":"<code>GEGRecord</code>","text":"<p>               Bases: <code>SchemaEvolutionMixin</code>, <code>BaseModel</code></p> <p>Global Entity Graph record.</p> <p>Represents a document with extracted entities and their metadata.</p> <p>Attributes:</p> Name Type Description <code>date</code> <code>datetime</code> <p>Publication date/time</p> <code>url</code> <code>str</code> <p>Source document URL</p> <code>lang</code> <code>str</code> <p>Language code</p> <code>entities</code> <code>list[Entity]</code> <p>List of extracted entities</p> Source code in <code>src/py_gdelt/models/graphs.py</code> <pre><code>class GEGRecord(SchemaEvolutionMixin, BaseModel):\n    \"\"\"Global Entity Graph record.\n\n    Represents a document with extracted entities and their metadata.\n\n    Attributes:\n        date: Publication date/time\n        url: Source document URL\n        lang: Language code\n        entities: List of extracted entities\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"ignore\", populate_by_name=True)\n\n    date: datetime\n    url: str\n    lang: str\n    entities: list[Entity] = Field(default_factory=list)\n\n    @field_validator(\"date\", mode=\"before\")\n    @classmethod\n    def parse_date(cls, v: Any) -&gt; datetime:\n        \"\"\"Parse date from ISO or GDELT format.\"\"\"\n        return parse_gdelt_datetime(v)\n</code></pre>"},{"location":"api/models/#py_gdelt.models.GEGRecord.parse_date","title":"<code>parse_date(v)</code>  <code>classmethod</code>","text":"<p>Parse date from ISO or GDELT format.</p> Source code in <code>src/py_gdelt/models/graphs.py</code> <pre><code>@field_validator(\"date\", mode=\"before\")\n@classmethod\ndef parse_date(cls, v: Any) -&gt; datetime:\n    \"\"\"Parse date from ISO or GDELT format.\"\"\"\n    return parse_gdelt_datetime(v)\n</code></pre>"},{"location":"api/models/#py_gdelt.models.GEMGRecord","title":"<code>GEMGRecord</code>","text":"<p>               Bases: <code>SchemaEvolutionMixin</code>, <code>BaseModel</code></p> <p>Global Embedded Metadata Graph record.</p> <p>Represents a document with extracted metadata tags and JSON-LD.</p> <p>Attributes:</p> Name Type Description <code>date</code> <code>datetime</code> <p>Publication date/time</p> <code>url</code> <code>str</code> <p>Source document URL</p> <code>title</code> <code>str | None</code> <p>Document title if available</p> <code>lang</code> <code>str</code> <p>Language code</p> <code>metatags</code> <code>list[MetaTag]</code> <p>List of extracted metadata tags</p> <code>jsonld</code> <code>list[str]</code> <p>List of JSON-LD strings</p> Source code in <code>src/py_gdelt/models/graphs.py</code> <pre><code>class GEMGRecord(SchemaEvolutionMixin, BaseModel):\n    \"\"\"Global Embedded Metadata Graph record.\n\n    Represents a document with extracted metadata tags and JSON-LD.\n\n    Attributes:\n        date: Publication date/time\n        url: Source document URL\n        title: Document title if available\n        lang: Language code\n        metatags: List of extracted metadata tags\n        jsonld: List of JSON-LD strings\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"ignore\", populate_by_name=True)\n\n    date: datetime\n    url: str\n    title: str | None = None\n    lang: str\n    metatags: list[MetaTag] = Field(default_factory=list)\n    jsonld: list[str] = Field(default_factory=list)\n\n    @field_validator(\"date\", mode=\"before\")\n    @classmethod\n    def parse_date(cls, v: Any) -&gt; datetime:\n        \"\"\"Parse date from ISO or GDELT format.\"\"\"\n        return parse_gdelt_datetime(v)\n</code></pre>"},{"location":"api/models/#py_gdelt.models.GEMGRecord.parse_date","title":"<code>parse_date(v)</code>  <code>classmethod</code>","text":"<p>Parse date from ISO or GDELT format.</p> Source code in <code>src/py_gdelt/models/graphs.py</code> <pre><code>@field_validator(\"date\", mode=\"before\")\n@classmethod\ndef parse_date(cls, v: Any) -&gt; datetime:\n    \"\"\"Parse date from ISO or GDELT format.\"\"\"\n    return parse_gdelt_datetime(v)\n</code></pre>"},{"location":"api/models/#py_gdelt.models.GFGRecord","title":"<code>GFGRecord</code>","text":"<p>               Bases: <code>SchemaEvolutionMixin</code>, <code>BaseModel</code></p> <p>Global Frontpage Graph record (TSV format).</p> <p>Represents a hyperlink from a frontpage to another URL.</p> <p>Attributes:</p> Name Type Description <code>date</code> <code>datetime</code> <p>Publication date/time</p> <code>from_frontpage_url</code> <code>str</code> <p>Source frontpage URL</p> <code>link_url</code> <code>str</code> <p>Destination link URL</p> <code>link_text</code> <code>str</code> <p>Anchor text of the link</p> <code>page_position</code> <code>int</code> <p>Position of link on page</p> <code>lang</code> <code>str</code> <p>Language code</p> Source code in <code>src/py_gdelt/models/graphs.py</code> <pre><code>class GFGRecord(SchemaEvolutionMixin, BaseModel):\n    \"\"\"Global Frontpage Graph record (TSV format).\n\n    Represents a hyperlink from a frontpage to another URL.\n\n    Attributes:\n        date: Publication date/time\n        from_frontpage_url: Source frontpage URL\n        link_url: Destination link URL\n        link_text: Anchor text of the link\n        page_position: Position of link on page\n        lang: Language code\n    \"\"\"\n\n    date: datetime\n    from_frontpage_url: str\n    link_url: str\n    link_text: str\n    page_position: int\n    lang: str\n\n    @classmethod\n    def from_raw(cls, raw: _RawGFGRecord) -&gt; Self:\n        \"\"\"Convert internal _RawGFGRecord to public GFGRecord model.\n\n        Args:\n            raw: Internal _RawGFGRecord dataclass from TSV parsing.\n\n        Returns:\n            Validated GFGRecord with all fields parsed and typed.\n        \"\"\"\n        date = parse_gdelt_datetime(raw.date)\n\n        # Parse page_position (default to 0 if empty)\n        page_position = 0\n        if raw.page_position:\n            try:\n                page_position = int(raw.page_position)\n            except ValueError:\n                logger.warning(\"Invalid page_position '%s', defaulting to 0\", raw.page_position)\n\n        return cls(\n            date=date,\n            from_frontpage_url=raw.from_frontpage_url,\n            link_url=raw.link_url,\n            link_text=raw.link_text,\n            page_position=page_position,\n            lang=raw.lang,\n        )\n</code></pre>"},{"location":"api/models/#py_gdelt.models.GFGRecord.from_raw","title":"<code>from_raw(raw)</code>  <code>classmethod</code>","text":"<p>Convert internal _RawGFGRecord to public GFGRecord model.</p> <p>Parameters:</p> Name Type Description Default <code>raw</code> <code>_RawGFGRecord</code> <p>Internal _RawGFGRecord dataclass from TSV parsing.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>Validated GFGRecord with all fields parsed and typed.</p> Source code in <code>src/py_gdelt/models/graphs.py</code> <pre><code>@classmethod\ndef from_raw(cls, raw: _RawGFGRecord) -&gt; Self:\n    \"\"\"Convert internal _RawGFGRecord to public GFGRecord model.\n\n    Args:\n        raw: Internal _RawGFGRecord dataclass from TSV parsing.\n\n    Returns:\n        Validated GFGRecord with all fields parsed and typed.\n    \"\"\"\n    date = parse_gdelt_datetime(raw.date)\n\n    # Parse page_position (default to 0 if empty)\n    page_position = 0\n    if raw.page_position:\n        try:\n            page_position = int(raw.page_position)\n        except ValueError:\n            logger.warning(\"Invalid page_position '%s', defaulting to 0\", raw.page_position)\n\n    return cls(\n        date=date,\n        from_frontpage_url=raw.from_frontpage_url,\n        link_url=raw.link_url,\n        link_text=raw.link_text,\n        page_position=page_position,\n        lang=raw.lang,\n    )\n</code></pre>"},{"location":"api/models/#py_gdelt.models.GGGRecord","title":"<code>GGGRecord</code>","text":"<p>               Bases: <code>SchemaEvolutionMixin</code>, <code>BaseModel</code></p> <p>Global Geographic Graph record.</p> <p>Represents a document with an extracted geographic location.</p> <p>Attributes:</p> Name Type Description <code>date</code> <code>datetime</code> <p>Publication date/time</p> <code>url</code> <code>str</code> <p>Source document URL</p> <code>location_name</code> <code>str</code> <p>Name of the location</p> <code>lat</code> <code>float</code> <p>Latitude (-90 to 90)</p> <code>lon</code> <code>float</code> <p>Longitude (-180 to 180)</p> <code>context</code> <code>str</code> <p>Surrounding text context</p> Source code in <code>src/py_gdelt/models/graphs.py</code> <pre><code>class GGGRecord(SchemaEvolutionMixin, BaseModel):\n    \"\"\"Global Geographic Graph record.\n\n    Represents a document with an extracted geographic location.\n\n    Attributes:\n        date: Publication date/time\n        url: Source document URL\n        location_name: Name of the location\n        lat: Latitude (-90 to 90)\n        lon: Longitude (-180 to 180)\n        context: Surrounding text context\n    \"\"\"\n\n    date: datetime\n    url: str\n    location_name: str\n    lat: float\n    lon: float\n    context: str\n\n    @field_validator(\"date\", mode=\"before\")\n    @classmethod\n    def parse_date(cls, v: Any) -&gt; datetime:\n        \"\"\"Parse date from ISO or GDELT format.\"\"\"\n        return parse_gdelt_datetime(v)\n\n    @field_validator(\"lat\")\n    @classmethod\n    def validate_lat(cls, v: float) -&gt; float:\n        \"\"\"Validate latitude is within valid range.\n\n        Args:\n            v: Latitude value.\n\n        Returns:\n            Validated latitude.\n\n        Raises:\n            ValueError: If latitude is outside -90 to 90 range.\n        \"\"\"\n        if not -90 &lt;= v &lt;= 90:\n            msg = f\"Latitude must be between -90 and 90, got {v}\"\n            raise ValueError(msg)\n        return v\n\n    @field_validator(\"lon\")\n    @classmethod\n    def validate_lon(cls, v: float) -&gt; float:\n        \"\"\"Validate longitude is within valid range.\n\n        Args:\n            v: Longitude value.\n\n        Returns:\n            Validated longitude.\n\n        Raises:\n            ValueError: If longitude is outside -180 to 180 range.\n        \"\"\"\n        if not -180 &lt;= v &lt;= 180:\n            msg = f\"Longitude must be between -180 and 180, got {v}\"\n            raise ValueError(msg)\n        return v\n</code></pre>"},{"location":"api/models/#py_gdelt.models.GGGRecord.parse_date","title":"<code>parse_date(v)</code>  <code>classmethod</code>","text":"<p>Parse date from ISO or GDELT format.</p> Source code in <code>src/py_gdelt/models/graphs.py</code> <pre><code>@field_validator(\"date\", mode=\"before\")\n@classmethod\ndef parse_date(cls, v: Any) -&gt; datetime:\n    \"\"\"Parse date from ISO or GDELT format.\"\"\"\n    return parse_gdelt_datetime(v)\n</code></pre>"},{"location":"api/models/#py_gdelt.models.GGGRecord.validate_lat","title":"<code>validate_lat(v)</code>  <code>classmethod</code>","text":"<p>Validate latitude is within valid range.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>float</code> <p>Latitude value.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Validated latitude.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If latitude is outside -90 to 90 range.</p> Source code in <code>src/py_gdelt/models/graphs.py</code> <pre><code>@field_validator(\"lat\")\n@classmethod\ndef validate_lat(cls, v: float) -&gt; float:\n    \"\"\"Validate latitude is within valid range.\n\n    Args:\n        v: Latitude value.\n\n    Returns:\n        Validated latitude.\n\n    Raises:\n        ValueError: If latitude is outside -90 to 90 range.\n    \"\"\"\n    if not -90 &lt;= v &lt;= 90:\n        msg = f\"Latitude must be between -90 and 90, got {v}\"\n        raise ValueError(msg)\n    return v\n</code></pre>"},{"location":"api/models/#py_gdelt.models.GGGRecord.validate_lon","title":"<code>validate_lon(v)</code>  <code>classmethod</code>","text":"<p>Validate longitude is within valid range.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>float</code> <p>Longitude value.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Validated longitude.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If longitude is outside -180 to 180 range.</p> Source code in <code>src/py_gdelt/models/graphs.py</code> <pre><code>@field_validator(\"lon\")\n@classmethod\ndef validate_lon(cls, v: float) -&gt; float:\n    \"\"\"Validate longitude is within valid range.\n\n    Args:\n        v: Longitude value.\n\n    Returns:\n        Validated longitude.\n\n    Raises:\n        ValueError: If longitude is outside -180 to 180 range.\n    \"\"\"\n    if not -180 &lt;= v &lt;= 180:\n        msg = f\"Longitude must be between -180 and 180, got {v}\"\n        raise ValueError(msg)\n    return v\n</code></pre>"},{"location":"api/models/#py_gdelt.models.GQGRecord","title":"<code>GQGRecord</code>","text":"<p>               Bases: <code>SchemaEvolutionMixin</code>, <code>BaseModel</code></p> <p>Global Quotation Graph record.</p> <p>Represents a document with extracted quotations and their context.</p> <p>Attributes:</p> Name Type Description <code>date</code> <code>datetime</code> <p>Publication date/time</p> <code>url</code> <code>str</code> <p>Source document URL</p> <code>lang</code> <code>str</code> <p>Language code</p> <code>quotes</code> <code>list[Quote]</code> <p>List of extracted quotations with context</p> Source code in <code>src/py_gdelt/models/graphs.py</code> <pre><code>class GQGRecord(SchemaEvolutionMixin, BaseModel):\n    \"\"\"Global Quotation Graph record.\n\n    Represents a document with extracted quotations and their context.\n\n    Attributes:\n        date: Publication date/time\n        url: Source document URL\n        lang: Language code\n        quotes: List of extracted quotations with context\n    \"\"\"\n\n    date: datetime\n    url: str\n    lang: str\n    quotes: list[Quote] = Field(default_factory=list)\n\n    @field_validator(\"date\", mode=\"before\")\n    @classmethod\n    def parse_date(cls, v: Any) -&gt; datetime:\n        \"\"\"Parse date from ISO or GDELT format.\"\"\"\n        return parse_gdelt_datetime(v)\n</code></pre>"},{"location":"api/models/#py_gdelt.models.GQGRecord.parse_date","title":"<code>parse_date(v)</code>  <code>classmethod</code>","text":"<p>Parse date from ISO or GDELT format.</p> Source code in <code>src/py_gdelt/models/graphs.py</code> <pre><code>@field_validator(\"date\", mode=\"before\")\n@classmethod\ndef parse_date(cls, v: Any) -&gt; datetime:\n    \"\"\"Parse date from ISO or GDELT format.\"\"\"\n    return parse_gdelt_datetime(v)\n</code></pre>"},{"location":"api/models/#py_gdelt.models.MetaTag","title":"<code>MetaTag</code>","text":"<p>               Bases: <code>SchemaEvolutionMixin</code>, <code>BaseModel</code></p> <p>Metadata tag extracted from GEMG (Global Embedded Metadata Graph).</p> <p>Represents a single metadata tag from HTML meta tags or JSON-LD.</p> <p>Attributes:</p> Name Type Description <code>key</code> <code>str</code> <p>Tag key/name</p> <code>tag_type</code> <code>str</code> <p>Tag type</p> <code>value</code> <code>str</code> <p>Tag value</p> Source code in <code>src/py_gdelt/models/graphs.py</code> <pre><code>class MetaTag(SchemaEvolutionMixin, BaseModel):\n    \"\"\"Metadata tag extracted from GEMG (Global Embedded Metadata Graph).\n\n    Represents a single metadata tag from HTML meta tags or JSON-LD.\n\n    Attributes:\n        key: Tag key/name\n        tag_type: Tag type\n        value: Tag value\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"ignore\", populate_by_name=True)\n\n    key: str\n    tag_type: str = Field(alias=\"type\")\n    value: str\n</code></pre>"},{"location":"api/models/#py_gdelt.models.Quote","title":"<code>Quote</code>","text":"<p>               Bases: <code>SchemaEvolutionMixin</code>, <code>BaseModel</code></p> <p>Quote extracted from GQG (Global Quotation Graph).</p> <p>Represents a single quotation with surrounding context.</p> <p>Attributes:</p> Name Type Description <code>pre</code> <code>str</code> <p>Text before the quote</p> <code>quote</code> <code>str</code> <p>The quotation text</p> <code>post</code> <code>str</code> <p>Text after the quote</p> Source code in <code>src/py_gdelt/models/graphs.py</code> <pre><code>class Quote(SchemaEvolutionMixin, BaseModel):\n    \"\"\"Quote extracted from GQG (Global Quotation Graph).\n\n    Represents a single quotation with surrounding context.\n\n    Attributes:\n        pre: Text before the quote\n        quote: The quotation text\n        post: Text after the quote\n    \"\"\"\n\n    pre: str\n    quote: str\n    post: str\n</code></pre>"},{"location":"api/models/#common-models","title":"Common Models","text":""},{"location":"api/models/#py_gdelt.models.Location","title":"<code>Location</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Geographic location from GDELT data.</p> Source code in <code>src/py_gdelt/models/common.py</code> <pre><code>class Location(BaseModel):\n    \"\"\"Geographic location from GDELT data.\"\"\"\n\n    lat: float | None = None\n    lon: float | None = None\n    feature_id: str | None = None  # GNIS/ADM1 code\n    name: str | None = None\n    country_code: str | None = None  # FIPS code\n    adm1_code: str | None = None\n    adm2_code: str | None = None\n    geo_type: int | None = None  # 1=Country, 2=State, 3=City, 4=Coordinates\n\n    def as_tuple(self) -&gt; tuple[float, float]:\n        \"\"\"Return (lat, lon) tuple. Raises ValueError if either is None.\"\"\"\n        if self.lat is None or self.lon is None:\n            msg = \"Cannot create tuple: lat or lon is None\"\n            raise ValueError(msg)\n        return (self.lat, self.lon)\n\n    def as_wkt(self) -&gt; str:\n        \"\"\"Return WKT POINT string for geopandas compatibility.\n\n        Returns:\n            WKT POINT string in format \"POINT(lon lat)\".\n\n        Raises:\n            ValueError: If lat or lon is None.\n        \"\"\"\n        if self.lat is None or self.lon is None:\n            msg = \"Cannot create WKT: lat or lon is None\"\n            raise ValueError(msg)\n        return f\"POINT({self.lon} {self.lat})\"\n\n    @property\n    def has_coordinates(self) -&gt; bool:\n        \"\"\"Check if location has valid coordinates.\"\"\"\n        return self.lat is not None and self.lon is not None\n</code></pre>"},{"location":"api/models/#py_gdelt.models.Location.has_coordinates","title":"<code>has_coordinates</code>  <code>property</code>","text":"<p>Check if location has valid coordinates.</p>"},{"location":"api/models/#py_gdelt.models.Location.as_tuple","title":"<code>as_tuple()</code>","text":"<p>Return (lat, lon) tuple. Raises ValueError if either is None.</p> Source code in <code>src/py_gdelt/models/common.py</code> <pre><code>def as_tuple(self) -&gt; tuple[float, float]:\n    \"\"\"Return (lat, lon) tuple. Raises ValueError if either is None.\"\"\"\n    if self.lat is None or self.lon is None:\n        msg = \"Cannot create tuple: lat or lon is None\"\n        raise ValueError(msg)\n    return (self.lat, self.lon)\n</code></pre>"},{"location":"api/models/#py_gdelt.models.Location.as_wkt","title":"<code>as_wkt()</code>","text":"<p>Return WKT POINT string for geopandas compatibility.</p> <p>Returns:</p> Type Description <code>str</code> <p>WKT POINT string in format \"POINT(lon lat)\".</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If lat or lon is None.</p> Source code in <code>src/py_gdelt/models/common.py</code> <pre><code>def as_wkt(self) -&gt; str:\n    \"\"\"Return WKT POINT string for geopandas compatibility.\n\n    Returns:\n        WKT POINT string in format \"POINT(lon lat)\".\n\n    Raises:\n        ValueError: If lat or lon is None.\n    \"\"\"\n    if self.lat is None or self.lon is None:\n        msg = \"Cannot create WKT: lat or lon is None\"\n        raise ValueError(msg)\n    return f\"POINT({self.lon} {self.lat})\"\n</code></pre>"},{"location":"api/models/#py_gdelt.models.ToneScores","title":"<code>ToneScores</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Tone analysis scores from GDELT.</p> Source code in <code>src/py_gdelt/models/common.py</code> <pre><code>class ToneScores(BaseModel):\n    \"\"\"Tone analysis scores from GDELT.\"\"\"\n\n    tone: float = Field(..., ge=-100, le=100)  # Overall tone (-100 to +100)\n    positive_score: float\n    negative_score: float\n    polarity: float  # Emotional extremity\n    activity_reference_density: float\n    self_group_reference_density: float\n    word_count: int | None = None\n</code></pre>"},{"location":"api/models/#py_gdelt.models.EntityMention","title":"<code>EntityMention</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Entity mention from GKG records.</p> Source code in <code>src/py_gdelt/models/common.py</code> <pre><code>class EntityMention(BaseModel):\n    \"\"\"Entity mention from GKG records.\"\"\"\n\n    entity_type: str  # PERSON, ORG, LOCATION, etc.\n    name: str\n    offset: int | None = None  # Character offset in source\n    confidence: float | None = None\n</code></pre>"},{"location":"api/models/#result-models","title":"Result Models","text":""},{"location":"api/models/#py_gdelt.models.FetchResult","title":"<code>FetchResult</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[T]</code></p> <p>Result container with partial failure tracking.</p> Source code in <code>src/py_gdelt/models/common.py</code> <pre><code>@dataclass\nclass FetchResult(Generic[T]):\n    \"\"\"Result container with partial failure tracking.\"\"\"\n\n    data: list[T]\n    failed: list[FailedRequest] = field(default_factory=list)\n\n    @property\n    def complete(self) -&gt; bool:\n        \"\"\"True if no requests failed.\"\"\"\n        return len(self.failed) == 0\n\n    @property\n    def partial(self) -&gt; bool:\n        \"\"\"True if some but not all requests failed.\"\"\"\n        return len(self.failed) &gt; 0 and len(self.data) &gt; 0\n\n    @property\n    def total_failed(self) -&gt; int:\n        \"\"\"Number of failed requests.\"\"\"\n        return len(self.failed)\n\n    def __iter__(self) -&gt; Iterator[T]:\n        \"\"\"Allow direct iteration over data.\"\"\"\n        return iter(self.data)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return count of successful items.\"\"\"\n        return len(self.data)\n</code></pre>"},{"location":"api/models/#py_gdelt.models.FetchResult.complete","title":"<code>complete</code>  <code>property</code>","text":"<p>True if no requests failed.</p>"},{"location":"api/models/#py_gdelt.models.FetchResult.partial","title":"<code>partial</code>  <code>property</code>","text":"<p>True if some but not all requests failed.</p>"},{"location":"api/models/#py_gdelt.models.FetchResult.total_failed","title":"<code>total_failed</code>  <code>property</code>","text":"<p>Number of failed requests.</p>"},{"location":"api/models/#py_gdelt.models.FetchResult.__iter__","title":"<code>__iter__()</code>","text":"<p>Allow direct iteration over data.</p> Source code in <code>src/py_gdelt/models/common.py</code> <pre><code>def __iter__(self) -&gt; Iterator[T]:\n    \"\"\"Allow direct iteration over data.\"\"\"\n    return iter(self.data)\n</code></pre>"},{"location":"api/models/#py_gdelt.models.FetchResult.__len__","title":"<code>__len__()</code>","text":"<p>Return count of successful items.</p> Source code in <code>src/py_gdelt/models/common.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return count of successful items.\"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"api/models/#py_gdelt.models.FailedRequest","title":"<code>FailedRequest</code>  <code>dataclass</code>","text":"<p>Represents a failed request in a partial result.</p> Source code in <code>src/py_gdelt/models/common.py</code> <pre><code>@dataclass(slots=True)\nclass FailedRequest:\n    \"\"\"Represents a failed request in a partial result.\"\"\"\n\n    url: str\n    error: str\n    status_code: int | None = None\n    retry_after: int | None = None  # For rate limit errors\n</code></pre>"},{"location":"examples/","title":"Examples Overview","text":"<p>Practical examples demonstrating py-gdelt usage.</p>"},{"location":"examples/#python-examples","title":"Python Examples","text":"<p>Located in <code>examples/</code> directory:</p>"},{"location":"examples/#core-client","title":"Core Client","text":"<ul> <li><code>basic_client_usage.py</code> - Comprehensive client usage</li> <li><code>events_endpoint_example.py</code> - Events queries</li> <li><code>gkg_example.py</code> - GKG data access</li> <li><code>ngrams_example.py</code> - NGrams queries</li> <li><code>download_gdelt_files.py</code> - File downloading</li> </ul>"},{"location":"examples/#rest-apis","title":"REST APIs","text":"<ul> <li><code>context_api_example.py</code> - Context API</li> <li><code>geo_api_example.py</code> - GEO API</li> <li><code>tv_api_example.py</code> - TV and TVAI APIs</li> </ul>"},{"location":"examples/#bigquery","title":"BigQuery","text":"<ul> <li><code>query_mentions.py</code> - Mentions via BigQuery</li> <li><code>bigquery_example.py</code> - General BigQuery usage</li> </ul>"},{"location":"examples/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>Located in <code>notebooks/</code> directory:</p> <ul> <li><code>01_getting_started.ipynb</code> - Introduction</li> <li><code>02_advanced_patterns.ipynb</code> - Production patterns</li> <li><code>03_visualization.ipynb</code> - Data visualization</li> </ul>"},{"location":"examples/#running-examples","title":"Running Examples","text":"<pre><code># Python examples\npython examples/basic_client_usage.py\npython examples/context_api_example.py\n\n# Jupyter notebooks\njupyter lab notebooks/\n</code></pre> <p>See Basic Usage and Advanced Patterns for code snippets.</p>"},{"location":"examples/advanced/","title":"Advanced Patterns","text":"<p>Production-ready patterns for py-gdelt.</p>"},{"location":"examples/advanced/#custom-configuration","title":"Custom Configuration","text":"<pre><code>from pathlib import Path\nfrom py_gdelt import GDELTClient, GDELTSettings\n\nsettings = GDELTSettings(\n    timeout=60,\n    max_retries=5,\n    cache_dir=Path(\"/custom/cache\"),\n    fallback_to_bigquery=True,\n)\n\nasync with GDELTClient(settings=settings) as client:\n    ...\n</code></pre>"},{"location":"examples/advanced/#error-handling","title":"Error Handling","text":"<pre><code>from py_gdelt.exceptions import APIError, DataError\n\ntry:\n    result = await client.doc.query(doc_filter)\nexcept APIError as e:\n    logger.error(f\"API error: {e}\")\nexcept DataError as e:\n    logger.error(f\"Data error: {e}\")\n</code></pre>"},{"location":"examples/advanced/#streaming-large-datasets","title":"Streaming Large Datasets","text":"<pre><code>async for event in client.events.stream(event_filter):\n    process(event)  # Memory-efficient\n</code></pre> <p>See <code>notebooks/02_advanced_patterns.ipynb</code> for detailed examples.</p>"},{"location":"examples/basic/","title":"Basic Usage Examples","text":"<p>Common usage patterns for py-gdelt.</p>"},{"location":"examples/basic/#query-events","title":"Query Events","text":"<pre><code>from datetime import date, timedelta\nfrom py_gdelt import GDELTClient\nfrom py_gdelt.filters import DateRange, EventFilter\n\nasync with GDELTClient() as client:\n    yesterday = date.today() - timedelta(days=1)\n\n    event_filter = EventFilter(\n        date_range=DateRange(start=yesterday, end=yesterday),\n        actor1_country=\"USA\",\n    )\n\n    result = await client.events.query(event_filter)\n    print(f\"Found {len(result)} events\")\n</code></pre>"},{"location":"examples/basic/#search-articles","title":"Search Articles","text":"<pre><code>from py_gdelt.filters import DocFilter\n\nasync with GDELTClient() as client:\n    doc_filter = DocFilter(\n        query=\"climate change\",\n        timespan=\"24h\",\n        max_results=10,\n    )\n\n    articles = await client.doc.query(doc_filter)\n</code></pre>"},{"location":"examples/basic/#geographic-search","title":"Geographic Search","text":"<pre><code>async with GDELTClient() as client:\n    result = await client.geo.search(\n        \"earthquake\",\n        timespan=\"7d\",\n        max_points=20,\n    )\n</code></pre> <p>See complete examples in <code>examples/</code> directory.</p>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>Configure py-gdelt using environment variables, TOML files, or programmatic settings.</p>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<p>Set environment variables to configure default behavior:</p> <pre><code># Timeouts and retries\nexport GDELT_TIMEOUT=60\nexport GDELT_MAX_RETRIES=5\n\n# Caching\nexport GDELT_CACHE_DIR=/path/to/cache\nexport GDELT_CACHE_TTL=3600\n\n# BigQuery\nexport GDELT_BIGQUERY_PROJECT=my-project\nexport GDELT_BIGQUERY_CREDENTIALS=/path/to/credentials.json\n\n# Behavior\nexport GDELT_FALLBACK_TO_BIGQUERY=true\nexport GDELT_VALIDATE_CODES=true\nexport GDELT_MAX_CONCURRENT_DOWNLOADS=10\n</code></pre>"},{"location":"getting-started/configuration/#toml-configuration","title":"TOML Configuration","text":"<p>Create a <code>gdelt.toml</code> file:</p> <pre><code>[gdelt]\ntimeout = 60\nmax_retries = 5\ncache_dir = \"/path/to/cache\"\ncache_ttl = 3600\nfallback_to_bigquery = true\nvalidate_codes = true\nmax_concurrent_downloads = 10\n\n[gdelt.bigquery]\nproject = \"my-project\"\ncredentials = \"/path/to/credentials.json\"\n</code></pre> <p>Load it with:</p> <pre><code>from pathlib import Path\nfrom py_gdelt import GDELTClient\n\nconfig_path = Path(\"gdelt.toml\")\nasync with GDELTClient(config_path=config_path) as client:\n    ...\n</code></pre>"},{"location":"getting-started/configuration/#programmatic-settings","title":"Programmatic Settings","text":"<p>Use <code>GDELTSettings</code> for full control:</p> <pre><code>from pathlib import Path\nfrom py_gdelt import GDELTClient, GDELTSettings\n\nsettings = GDELTSettings(\n    # Timeouts and retries\n    timeout=60,\n    max_retries=5,\n\n    # Caching\n    cache_dir=Path.home() / \".cache\" / \"gdelt\",\n    cache_ttl=3600,\n\n    # BigQuery\n    bigquery_project=\"my-project\",\n    bigquery_credentials=Path(\"/path/to/credentials.json\"),\n\n    # Behavior\n    fallback_to_bigquery=True,\n    validate_codes=True,\n    max_concurrent_downloads=10,\n)\n\nasync with GDELTClient(settings=settings) as client:\n    ...\n</code></pre>"},{"location":"getting-started/configuration/#configuration-options","title":"Configuration Options","text":""},{"location":"getting-started/configuration/#timeouts","title":"Timeouts","text":"<ul> <li>timeout (int): HTTP request timeout in seconds. Default: <code>30</code></li> <li>connect_timeout (int): Connection timeout in seconds. Default: <code>10</code></li> </ul>"},{"location":"getting-started/configuration/#retries","title":"Retries","text":"<ul> <li>max_retries (int): Maximum retry attempts. Default: <code>3</code></li> <li>retry_backoff (float): Backoff multiplier for retries. Default: <code>2.0</code></li> </ul>"},{"location":"getting-started/configuration/#caching","title":"Caching","text":"<ul> <li>cache_dir (Path): Directory for cached files. Default: <code>~/.cache/gdelt</code></li> <li>cache_ttl (int): Cache time-to-live in seconds. Default: <code>3600</code> (1 hour)</li> <li>use_cache (bool): Enable/disable caching. Default: <code>True</code></li> </ul>"},{"location":"getting-started/configuration/#bigquery","title":"BigQuery","text":"<ul> <li>bigquery_project (str): Google Cloud project ID. Default: <code>None</code></li> <li>bigquery_credentials (Path): Path to credentials JSON. Default: <code>None</code></li> <li>fallback_to_bigquery (bool): Use BigQuery when file sources fail. Default: <code>False</code></li> </ul>"},{"location":"getting-started/configuration/#performance","title":"Performance","text":"<ul> <li>max_concurrent_downloads (int): Max parallel file downloads. Default: <code>5</code></li> <li>chunk_size (int): Download chunk size in bytes. Default: <code>8192</code></li> </ul>"},{"location":"getting-started/configuration/#validation","title":"Validation","text":"<ul> <li>validate_codes (bool): Validate CAMEO/country codes. Default: <code>False</code></li> <li>strict_mode (bool): Raise errors on invalid codes. Default: <code>False</code></li> </ul>"},{"location":"getting-started/configuration/#priority-order","title":"Priority Order","text":"<p>Configuration is loaded in this order (later overrides earlier):</p> <ol> <li>Default values</li> <li>Environment variables</li> <li>TOML configuration file</li> <li>Programmatic <code>GDELTSettings</code></li> </ol>"},{"location":"getting-started/configuration/#bigquery-setup","title":"BigQuery Setup","text":"<p>For BigQuery access:</p> <ol> <li>Install dependencies: <code>pip install py-gdelt[bigquery]</code></li> <li>Create Google Cloud project</li> <li>Enable BigQuery API</li> <li>Create service account and download credentials</li> <li>Set credentials path:</li> </ol> <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/credentials.json\"\n</code></pre> <p>Or in code:</p> <pre><code>settings = GDELTSettings(\n    bigquery_project=\"my-project\",\n    bigquery_credentials=Path(\"/path/to/credentials.json\"),\n    fallback_to_bigquery=True,\n)\n</code></pre>"},{"location":"getting-started/configuration/#best-practices","title":"Best Practices","text":"<ul> <li>Use environment variables for sensitive data (credentials)</li> <li>Use TOML for team-shared configuration</li> <li>Use programmatic settings for runtime customization</li> <li>Enable caching in development, configure carefully in production</li> <li>Set appropriate timeouts for your network conditions</li> <li>Enable BigQuery fallback for production reliability</li> </ul>"},{"location":"getting-started/data-sources/","title":"Data Sources &amp; Access Matrix","text":"<p>GDELT provides multiple data sources accessible through different methods. Understanding which source to use for your specific needs is crucial for efficient data retrieval.</p>"},{"location":"getting-started/data-sources/#complete-data-source-matrix","title":"Complete Data Source Matrix","text":"Data Type API BigQuery Raw Files Time Constraint Fallback Available Articles (fulltext) DOC 2.0 - - Rolling 3 months No equivalent Article geo heatmaps GEO 2.0 - - Rolling 7 days No equivalent Sentence-level context Context 2.0 - - Rolling 72 hours No equivalent TV captions TV 2.0 - - July 2009+ (full) No equivalent TV visual/AI TV AI 2.0 - - 2010+, limited No equivalent Events v2 - Yes Yes Feb 2015+ Files &lt;-&gt; BigQuery Events v1 - Yes Yes 1979 - Feb 2015 Files &lt;-&gt; BigQuery Mentions - Yes Yes Feb 2015+ (v2 only) Files &lt;-&gt; BigQuery GKG v2 - Yes Yes Feb 2015+ Files &lt;-&gt; BigQuery GKG v1 - Yes Yes 2013 - Feb 2015 Files &lt;-&gt; BigQuery Web NGrams 3.0 - Yes Yes Jan 2020+ Files &lt;-&gt; BigQuery"},{"location":"getting-started/data-sources/#historical-fulltext-search-strategy","title":"Historical Fulltext Search Strategy","text":"<p>Choosing the right source based on time range:</p> Need &lt;3 months 3mo - 5yr &gt;5yr Fulltext search DOC API NGrams 3.0 Not available Entity/theme search GKG GKG GKG (v1 to 2013) Event tracking Events Events Events (v1 to 1979)"},{"location":"getting-started/data-sources/#api-endpoints-reference","title":"API Endpoints Reference","text":""},{"location":"getting-started/data-sources/#doc-20-api","title":"DOC 2.0 API","text":"<ul> <li>Endpoint: <code>https://api.gdeltproject.org/api/v2/doc/doc</code></li> <li>Purpose: Full-text article search</li> <li>Time Window: Rolling 3 months</li> <li>Max Records: 250</li> <li>Output Modes: artlist, timelinevol, timelinevolraw, timelinetone, timelinelang, timelinesourcecountry, imagecollage, tonechart</li> </ul>"},{"location":"getting-started/data-sources/#geo-20-api","title":"GEO 2.0 API","text":"<ul> <li>Endpoint: <code>https://api.gdeltproject.org/api/v2/geo/geo</code></li> <li>Purpose: Geographic visualizations</li> <li>Time Window: Rolling 7 days</li> <li>Max Points: 1,000-25,000 (mode dependent)</li> </ul>"},{"location":"getting-started/data-sources/#context-20-api","title":"Context 2.0 API","text":"<ul> <li>Endpoint: <code>https://api.gdeltproject.org/api/v2/context/context</code></li> <li>Purpose: Sentence-level search (all terms in same sentence)</li> <li>Time Window: Rolling 72 hours</li> <li>Max Records: 200</li> </ul>"},{"location":"getting-started/data-sources/#tv-20-api","title":"TV 2.0 API","text":"<ul> <li>Endpoint: <code>https://api.gdeltproject.org/api/v2/tv/tv</code></li> <li>Purpose: Television closed caption search</li> <li>Time Window: July 2009+ (full archive)</li> <li>Max Records: 3,000</li> </ul>"},{"location":"getting-started/data-sources/#tv-ai-20-api","title":"TV AI 2.0 API","text":"<ul> <li>Endpoint: <code>https://api.gdeltproject.org/api/v2/tvai/tvai</code></li> <li>Purpose: Visual television search (AI-powered)</li> <li>Time Window: 2010+ (limited channels)</li> </ul>"},{"location":"getting-started/data-sources/#gkg-geojson-api-v10-legacy","title":"GKG GeoJSON API (v1.0 Legacy)","text":"<ul> <li>Endpoint: <code>https://api.gdeltproject.org/api/v1/gkg_geojson</code></li> <li>Purpose: GeoJSON by GKG theme/person/org</li> <li>Note: Uses uppercase parameters</li> </ul>"},{"location":"getting-started/data-sources/#bigquery-tables","title":"BigQuery Tables","text":"Table Size Project <code>gdelt-bq.gdeltv2.events</code> ~63GB gdelt-bq <code>gdelt-bq.gdeltv2.events_partitioned</code> ~63GB gdelt-bq <code>gdelt-bq.gdeltv2.eventmentions</code> ~104GB gdelt-bq <code>gdelt-bq.gdeltv2.gkg</code> ~3.6TB gdelt-bq <code>gdelt-bq.gdeltv2.gkg_partitioned</code> ~3.6TB gdelt-bq <code>gdelt-bq.gdeltv2.webngrams</code> Variable gdelt-bq <p>BigQuery Cost</p> <p>Always use partitioned tables with date filters to avoid full table scans:</p> <pre><code>-- DANGEROUS: Scans entire 3.6TB GKG table (~$18)\nSELECT * FROM `gdelt-bq.gdeltv2.gkg` LIMIT 100\n\n-- SAFE: Use partitioned table with filter\nSELECT * FROM `gdelt-bq.gdeltv2.gkg_partitioned`\nWHERE _PARTITIONTIME &gt;= '2024-01-01'\n  AND _PARTITIONTIME &lt; '2024-01-02'\nLIMIT 100\n</code></pre>"},{"location":"getting-started/data-sources/#raw-file-access","title":"Raw File Access","text":""},{"location":"getting-started/data-sources/#master-file-lists","title":"Master File Lists","text":"<ul> <li>English: <code>http://data.gdeltproject.org/gdeltv2/masterfilelist.txt</code></li> <li>Translated: <code>http://data.gdeltproject.org/gdeltv2/masterfilelist-translation.txt</code></li> <li>Last update: <code>http://data.gdeltproject.org/gdeltv2/lastupdate.txt</code></li> </ul>"},{"location":"getting-started/data-sources/#url-patterns-v2","title":"URL Patterns (v2)","text":"<pre><code>http://data.gdeltproject.org/gdeltv2/YYYYMMDDHHMMSS.export.CSV.zip\nhttp://data.gdeltproject.org/gdeltv2/YYYYMMDDHHMMSS.mentions.CSV.zip\nhttp://data.gdeltproject.org/gdeltv2/YYYYMMDDHHMMSS.gkg.csv.zip\n</code></pre>"},{"location":"getting-started/data-sources/#ngrams-30","title":"NGrams 3.0","text":"<pre><code>http://data.gdeltproject.org/gdeltv3/webngrams/YYYYMMDDHHMMSS.webngrams.json.gz\n</code></pre>"},{"location":"getting-started/data-sources/#update-frequency","title":"Update Frequency","text":"<p>Files are updated every 15 minutes at :00, :15, :30, and :45 past the hour.</p>"},{"location":"getting-started/data-sources/#how-gdelt-py-handles-sources","title":"How gdelt-py Handles Sources","text":"<p>The library provides automatic source selection:</p> <pre><code># Auto mode (default) - tries files first, falls back to BigQuery\nevents = await client.events.query(filter)\n\n# Explicit source selection\nevents = await client.events.query(filter, source=\"bigquery\")\nevents = await client.events.query(filter, source=\"files\")\n</code></pre> <p>For API-only endpoints (DOC, GEO, Context, TV), there are no fallback options - the library will return an error if the API is unavailable.</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11 or higher</li> <li>pip (Python package manager)</li> </ul>"},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":"<p>Install py-gdelt using pip:</p> <pre><code>pip install py-gdelt\n</code></pre> <p>This installs the core library with support for: - Events, Mentions, GKG endpoints (file-based) - NGrams endpoint - All REST APIs (DOC, GEO, Context, TV, TVAI) - Lookup tables</p>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting-started/installation/#bigquery-support","title":"BigQuery Support","text":"<p>For BigQuery data access and automatic fallback:</p> <pre><code>pip install py-gdelt[bigquery]\n</code></pre> <p>This adds: - <code>google-cloud-bigquery&gt;=3.0</code> - Ability to query GDELT BigQuery datasets - Automatic fallback when file sources fail</p>"},{"location":"getting-started/installation/#pandas-integration","title":"Pandas Integration","text":"<p>For data analysis and manipulation:</p> <pre><code>pip install py-gdelt[pandas]\n</code></pre> <p>This adds: - <code>pandas&gt;=2.0</code> - DataFrame conversion utilities - Easier data manipulation</p>"},{"location":"getting-started/installation/#all-optional-dependencies","title":"All Optional Dependencies","text":"<p>Install everything:</p> <pre><code>pip install py-gdelt[bigquery,pandas]\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For contributing to py-gdelt:</p> <pre><code># Clone the repository\ngit clone https://github.com/rbwasilewski/py-gdelt.git\ncd py-gdelt\n\n# Install in editable mode with dev dependencies\npip install -e \".[dev,bigquery,pandas]\"\n</code></pre> <p>Development dependencies include: - pytest for testing - pytest-asyncio for async tests - pytest-cov for coverage - pytest-timeout for test timeouts - mypy for type checking - ruff for linting - respx for HTTP mocking</p>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>Verify your installation:</p> <pre><code>import py_gdelt\nprint(py_gdelt.__version__)\n\nfrom py_gdelt import GDELTClient\nprint(\"Installation successful!\")\n</code></pre>"},{"location":"getting-started/installation/#upgrading","title":"Upgrading","text":"<p>Upgrade to the latest version:</p> <pre><code>pip install --upgrade py-gdelt\n</code></pre>"},{"location":"getting-started/installation/#uninstallation","title":"Uninstallation","text":"<p>Remove py-gdelt:</p> <pre><code>pip uninstall py-gdelt\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide</li> <li>Configuration</li> <li>Examples</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide will get you up and running with py-gdelt in 5 minutes.</p>"},{"location":"getting-started/quickstart/#your-first-query","title":"Your First Query","text":"<p>Let's query recent events from the US:</p> <pre><code>import asyncio\nfrom datetime import date, timedelta\nfrom py_gdelt import GDELTClient\nfrom py_gdelt.filters import DateRange, EventFilter\n\nasync def main():\n    async with GDELTClient() as client:\n        # Query yesterday's events\n        yesterday = date.today() - timedelta(days=1)\n\n        event_filter = EventFilter(\n            date_range=DateRange(start=yesterday, end=yesterday),\n            actor1_country=\"USA\",\n        )\n\n        result = await client.events.query(event_filter)\n        print(f\"Found {len(result)} events\")\n\n        if result:\n            event = result[0]\n            print(f\"First event: {event.global_event_id}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"getting-started/quickstart/#search-articles","title":"Search Articles","text":"<p>Use the DOC API to search for articles:</p> <pre><code>from py_gdelt.filters import DocFilter\n\nasync with GDELTClient() as client:\n    doc_filter = DocFilter(\n        query=\"climate change\",\n        timespan=\"24h\",\n        max_results=10,\n    )\n\n    articles = await client.doc.query(doc_filter)\n\n    for article in articles:\n        print(f\"{article.title}\")\n        print(f\"  {article.url}\")\n</code></pre>"},{"location":"getting-started/quickstart/#geographic-search","title":"Geographic Search","text":"<p>Find locations mentioned in news:</p> <pre><code>async with GDELTClient() as client:\n    result = await client.geo.search(\n        \"earthquake\",\n        timespan=\"7d\",\n        max_points=20,\n    )\n\n    for point in result.points:\n        print(f\"{point.name}: {point.count} articles\")\n</code></pre>"},{"location":"getting-started/quickstart/#contextual-analysis","title":"Contextual Analysis","text":"<p>Analyze themes and entities:</p> <pre><code>async with GDELTClient() as client:\n    result = await client.context.analyze(\n        \"artificial intelligence\",\n        timespan=\"7d\",\n    )\n\n    print(f\"Articles analyzed: {result.article_count}\")\n\n    for theme in result.themes[:5]:\n        print(f\"  {theme.theme}: {theme.count}\")\n</code></pre>"},{"location":"getting-started/quickstart/#streaming-large-datasets","title":"Streaming Large Datasets","text":"<p>For memory efficiency, use streaming:</p> <pre><code>async with GDELTClient() as client:\n    yesterday = date.today() - timedelta(days=1)\n\n    event_filter = EventFilter(\n        date_range=DateRange(start=yesterday, end=yesterday),\n    )\n\n    count = 0\n    async for event in client.events.stream(event_filter):\n        count += 1\n        # Process event without loading all into memory\n\n    print(f\"Processed {count} events\")\n</code></pre>"},{"location":"getting-started/quickstart/#synchronous-usage","title":"Synchronous Usage","text":"<p>For non-async code:</p> <pre><code>with GDELTClient() as client:\n    yesterday = date.today() - timedelta(days=1)\n\n    event_filter = EventFilter(\n        date_range=DateRange(start=yesterday, end=yesterday),\n        actor1_country=\"USA\",\n    )\n\n    result = client.events.query_sync(event_filter)\n    print(f\"Found {len(result)} events\")\n</code></pre>"},{"location":"getting-started/quickstart/#using-lookup-tables","title":"Using Lookup Tables","text":"<p>Access CAMEO codes and other lookups:</p> <pre><code>async with GDELTClient() as client:\n    # CAMEO event codes\n    cameo = client.lookups.cameo\n    event_code = cameo.get(\"14\")\n    print(f\"Code 14: {event_code.name}\")  # \"PROTEST\"\n\n    # Country conversions\n    countries = client.lookups.countries\n    iso3 = countries.fips_to_iso3(\"US\")\n    print(f\"US -&gt; {iso3}\")  # \"USA\"\n</code></pre>"},{"location":"getting-started/quickstart/#error-handling","title":"Error Handling","text":"<p>Always handle errors gracefully:</p> <pre><code>from py_gdelt.exceptions import APIError, DataError\n\nasync with GDELTClient() as client:\n    try:\n        result = await client.doc.query(doc_filter)\n    except APIError as e:\n        print(f\"API error: {e}\")\n    except DataError as e:\n        print(f\"Data error: {e}\")\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide - Customize settings</li> <li>User Guide - Deep dive into features</li> <li>Examples - More complete examples</li> <li>API Reference - Full API documentation</li> </ul>"},{"location":"plans/2026-01-25-refactor-shared-gdelt-date-parsing-plan/","title":"refactor: extract shared GDELT date parsing utility","text":""},{"location":"plans/2026-01-25-refactor-shared-gdelt-date-parsing-plan/#overview","title":"Overview","text":"<p>Extract duplicated GDELT date parsing logic (<code>%Y%m%d%H%M%S</code> and <code>%Y%m%d</code> formats) into a shared utility module at <code>src/py_gdelt/utils/dates.py</code>.</p>"},{"location":"plans/2026-01-25-refactor-shared-gdelt-date-parsing-plan/#problem-statement","title":"Problem Statement","text":"<p>The same date parsing pattern is duplicated in 10+ locations across models and endpoints: - <code>src/py_gdelt/models/graphs.py</code> - <code>_parse_gdelt_date()</code> - <code>src/py_gdelt/models/gkg.py</code> - inline <code>datetime.strptime()</code> - <code>src/py_gdelt/models/vgkg.py</code> - inline <code>datetime.strptime()</code> - <code>src/py_gdelt/models/events.py</code> - inline <code>datetime.strptime()</code> - <code>src/py_gdelt/models/ngrams.py</code> - inline <code>datetime.strptime()</code> - <code>src/py_gdelt/models/articles.py</code> - inline <code>datetime.strptime()</code> - <code>src/py_gdelt/endpoints/tv.py</code> - <code>_parse_date()</code> - <code>src/py_gdelt/endpoints/lowerthird.py</code> - <code>_parse_date()</code> - <code>src/py_gdelt/endpoints/tvv.py</code> - inline <code>datetime.strptime()</code> - <code>src/py_gdelt/sources/files.py</code> - inline <code>datetime.strptime()</code></p> <p>This duplication: - Makes format changes error-prone (must update multiple files) - Creates inconsistent error handling across modules - Obscures the GDELT-specific date format convention</p>"},{"location":"plans/2026-01-25-refactor-shared-gdelt-date-parsing-plan/#proposed-solution","title":"Proposed Solution","text":"<p>Create <code>src/py_gdelt/utils/dates.py</code> with three functions (reduced from four after review):</p> Function Input Output On Invalid <code>parse_gdelt_datetime()</code> <code>str \\| int \\| datetime</code> <code>datetime</code> Raises <code>ValueError</code> <code>try_parse_gdelt_datetime()</code> <code>str \\| int \\| datetime \\| None</code> <code>datetime \\| None</code> Returns <code>None</code> <code>parse_gdelt_date()</code> <code>str \\| date \\| datetime</code> <code>date</code> Raises <code>ValueError</code> <p>Removed: <code>try_parse_gdelt_date()</code> - Zero callers exist in the codebase (YAGNI).</p>"},{"location":"plans/2026-01-25-refactor-shared-gdelt-date-parsing-plan/#format-support","title":"Format Support","text":"<p>All functions support: - GDELT timestamp: <code>YYYYMMDDHHMMSS</code> (14 digits) - GDELT date: <code>YYYYMMDD</code> (8 digits) - ISO 8601 with time: <code>2024-01-15T12:00:00</code> with optional timezone - ISO 8601 date-only: <code>2024-01-15</code> - Integer timestamps: <code>20240115120000</code> (from some GDELT JSON APIs) - Passthrough: <code>datetime</code> / <code>date</code> objects converted to UTC</p> <p>All returned datetimes are converted to UTC (not just tagged).</p>"},{"location":"plans/2026-01-25-refactor-shared-gdelt-date-parsing-plan/#error-handling-rationale","title":"Error Handling Rationale","text":"<p>Strict functions (<code>parse_*</code>) - Used by model transformations (<code>gkg.py</code>, <code>events.py</code>, etc.) where data comes from structured sources (BigQuery, files). Invalid dates indicate bugs or schema changes - fail fast.</p> <p>Lenient function (<code>try_parse_gdelt_datetime</code>) - Used by API endpoints (<code>tv.py</code>, <code>lowerthird.py</code>) where JSON from GDELT APIs can have missing/malformed dates. Graceful degradation preferred.</p>"},{"location":"plans/2026-01-25-refactor-shared-gdelt-date-parsing-plan/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>[x] Create <code>src/py_gdelt/utils/dates.py</code> with three functions</li> <li>[x] Export functions from <code>src/py_gdelt/utils/__init__.py</code></li> <li>[x] Update all 10 files to use shared utilities</li> <li>[x] Remove duplicate <code>_parse_date()</code> / <code>_parse_gdelt_date()</code> functions</li> <li>[x] Add comprehensive tests in <code>tests/test_dates.py</code></li> <li>[x] All tests pass (<code>make test</code>)</li> <li>[x] Type checking passes (<code>make typecheck</code>)</li> <li>[x] Linting passes (<code>make lint</code>)</li> </ul>"},{"location":"plans/2026-01-25-refactor-shared-gdelt-date-parsing-plan/#technical-approach","title":"Technical Approach","text":""},{"location":"plans/2026-01-25-refactor-shared-gdelt-date-parsing-plan/#phase-1-create-utility-module","title":"Phase 1: Create Utility Module","text":"<p>Create <code>src/py_gdelt/utils/dates.py</code>:</p> <pre><code>\"\"\"GDELT date parsing utilities.\n\nThis module provides the canonical implementation of GDELT date format parsing.\nAll models and endpoints should use these functions rather than inline strptime calls.\n\nSupported formats:\n- YYYYMMDDHHMMSS (14-digit GDELT timestamp)\n- YYYYMMDD (8-digit GDELT date)\n- ISO 8601 with time (2024-01-15T12:00:00)\n- ISO 8601 date-only (2024-01-15)\n- Integer timestamps (from some GDELT JSON APIs)\n\nAll datetimes are converted to UTC timezone.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom datetime import UTC, date, datetime\n\n\ndef parse_gdelt_datetime(value: str | int | datetime) -&gt; datetime:\n    \"\"\"Parse GDELT timestamp or ISO format to UTC datetime.\n\n    Args:\n        value: Date string (YYYYMMDDHHMMSS, YYYYMMDD, or ISO 8601),\n               integer timestamp, or datetime object.\n\n    Returns:\n        Parsed datetime converted to UTC timezone.\n\n    Raises:\n        ValueError: If the date format is invalid.\n    \"\"\"\n    if isinstance(value, datetime):\n        # Convert to UTC (not just attach timezone)\n        if value.tzinfo is None:\n            return value.replace(tzinfo=UTC)\n        return value.astimezone(UTC)\n\n    if isinstance(value, int):\n        value = str(value)\n\n    # ISO format (contains 'T' or '-' delimiter)\n    if \"T\" in value or \"-\" in value:\n        try:\n            dt = datetime.fromisoformat(value.replace(\"Z\", \"+00:00\"))\n            if dt.tzinfo is None:\n                return dt.replace(tzinfo=UTC)\n            return dt.astimezone(UTC)\n        except ValueError:\n            pass  # Fall through to error\n\n    # GDELT formats: 14-digit timestamp or 8-digit date\n    try:\n        if len(value) == 14:\n            return datetime.strptime(value, \"%Y%m%d%H%M%S\").replace(tzinfo=UTC)\n        if len(value) == 8:\n            return datetime.strptime(value, \"%Y%m%d\").replace(tzinfo=UTC)\n    except ValueError:\n        pass  # Fall through to error\n\n    msg = f\"Invalid GDELT date format: {value!r}\"\n    raise ValueError(msg)\n\n\ndef try_parse_gdelt_datetime(value: str | int | datetime | None) -&gt; datetime | None:\n    \"\"\"Parse GDELT timestamp or ISO format to UTC datetime, returning None on failure.\n\n    Args:\n        value: Date string, integer timestamp, datetime object, or None.\n\n    Returns:\n        Parsed datetime with UTC timezone, or None if parsing fails.\n    \"\"\"\n    if value is None:\n        return None\n    try:\n        return parse_gdelt_datetime(value)\n    except (ValueError, TypeError):\n        return None\n\n\ndef parse_gdelt_date(value: str | date | datetime) -&gt; date:\n    \"\"\"Parse GDELT date format to date object.\n\n    Args:\n        value: Date string (YYYYMMDD), date object, or datetime object.\n\n    Returns:\n        Parsed date object.\n\n    Raises:\n        ValueError: If the date format is invalid.\n    \"\"\"\n    if isinstance(value, datetime):\n        return value.date()\n    if isinstance(value, date):\n        return value\n    try:\n        return datetime.strptime(value, \"%Y%m%d\").date()\n    except ValueError:\n        msg = f\"Invalid GDELT date format: {value!r}\"\n        raise ValueError(msg) from None\n</code></pre>"},{"location":"plans/2026-01-25-refactor-shared-gdelt-date-parsing-plan/#phase-2-update-models","title":"Phase 2: Update Models","text":"<p>Each model file change follows this pattern:</p> <pre><code>--- a/src/py_gdelt/models/gkg.py\n+++ b/src/py_gdelt/models/gkg.py\n@@ -1,6 +1,7 @@\n from datetime import UTC, datetime\n\n+from py_gdelt.utils.dates import parse_gdelt_datetime\n\n@@ -146,7 +147,7 @@ class GKGRecord:\n         # Parse date (format: YYYYMMDDHHMMSS)\n-        date_str = raw.date\n-        date = datetime.strptime(date_str, \"%Y%m%d%H%M%S\").replace(tzinfo=UTC)\n+        date = parse_gdelt_datetime(raw.date)\n</code></pre> <p>Files to update: - <code>graphs.py</code> - Remove <code>_parse_gdelt_date()</code>, use <code>parse_gdelt_datetime</code> - <code>gkg.py:148</code> - Replace inline strptime - <code>gkg.py:588</code> - Replace inline strptime - <code>vgkg.py:128</code> - Replace inline strptime - <code>events.py:215</code> - Use <code>parse_gdelt_date</code> for date-only - <code>events.py:221-222</code> - Use <code>parse_gdelt_datetime</code> - <code>events.py:392,395</code> - Use <code>parse_gdelt_datetime</code> - <code>ngrams.py:63</code> - Use <code>parse_gdelt_datetime</code> - <code>ngrams.py:141</code> - Use <code>parse_gdelt_date</code> - <code>articles.py:57,93</code> - Use <code>parse_gdelt_datetime</code></p>"},{"location":"plans/2026-01-25-refactor-shared-gdelt-date-parsing-plan/#phase-3-update-endpoints","title":"Phase 3: Update Endpoints","text":"<pre><code>--- a/src/py_gdelt/endpoints/tv.py\n+++ b/src/py_gdelt/endpoints/tv.py\n@@ -1,6 +1,7 @@\n from datetime import UTC, datetime, timedelta\n\n+from py_gdelt.utils.dates import try_parse_gdelt_datetime\n\n@@ -294,7 +295,7 @@ class TVEndpoint:\n-                date=_parse_date(item.get(\"date\")),\n+                date=try_parse_gdelt_datetime(item.get(\"date\")),\n\n@@ -530,28 +531,0 @@\n-def _parse_date(date_str: str | None) -&gt; datetime | None:\n-    ...\n</code></pre> <p>Files to update: - <code>tv.py</code> - Remove <code>_parse_date()</code>, use <code>try_parse_gdelt_datetime</code> - <code>lowerthird.py</code> - Remove <code>_parse_date()</code>, use <code>try_parse_gdelt_datetime</code> - <code>tvv.py:76</code> - Use <code>parse_gdelt_date</code> - <code>sources/files.py:519</code> - Use <code>parse_gdelt_datetime</code></p>"},{"location":"plans/2026-01-25-refactor-shared-gdelt-date-parsing-plan/#phase-4-update-exports","title":"Phase 4: Update Exports","text":"<pre><code>--- a/src/py_gdelt/utils/__init__.py\n+++ b/src/py_gdelt/utils/__init__.py\n@@ -1,11 +1,19 @@\n \"\"\"Utility functions for the py-gdelt library.\"\"\"\n\n+from py_gdelt.utils.dates import (\n+    parse_gdelt_date,\n+    parse_gdelt_datetime,\n+    try_parse_gdelt_datetime,\n+)\n from py_gdelt.utils.dedup import DedupeStrategy, deduplicate\n from py_gdelt.utils.streaming import ResultStream\n\n\n __all__ = [\n     \"DedupeStrategy\",\n     \"ResultStream\",\n     \"deduplicate\",\n+    \"parse_gdelt_date\",\n+    \"parse_gdelt_datetime\",\n+    \"try_parse_gdelt_datetime\",\n ]\n</code></pre>"},{"location":"plans/2026-01-25-refactor-shared-gdelt-date-parsing-plan/#phase-5-add-tests","title":"Phase 5: Add Tests","text":"<p>Create <code>tests/test_dates.py</code>:</p> <pre><code>\"\"\"Tests for GDELT date parsing utilities.\"\"\"\n\nfrom __future__ import annotations\n\nfrom datetime import UTC, date, datetime, timezone\n\nimport pytest\n\nfrom py_gdelt.utils.dates import (\n    parse_gdelt_date,\n    parse_gdelt_datetime,\n    try_parse_gdelt_datetime,\n)\n\n\nclass TestParseGdeltDatetime:\n    \"\"\"Tests for parse_gdelt_datetime (strict).\"\"\"\n\n    # GDELT 14-digit format\n    def test_gdelt_14_digit_format(self) -&gt; None:\n        result = parse_gdelt_datetime(\"20240115120000\")\n        assert result == datetime(2024, 1, 15, 12, 0, 0, tzinfo=UTC)\n\n    # GDELT 8-digit format (returns midnight UTC)\n    def test_gdelt_8_digit_format(self) -&gt; None:\n        result = parse_gdelt_datetime(\"20240115\")\n        assert result == datetime(2024, 1, 15, 0, 0, 0, tzinfo=UTC)\n\n    # Integer input (from some GDELT JSON APIs)\n    def test_integer_input(self) -&gt; None:\n        result = parse_gdelt_datetime(20240115120000)\n        assert result == datetime(2024, 1, 15, 12, 0, 0, tzinfo=UTC)\n\n    # ISO format with T separator\n    def test_iso_format_with_t(self) -&gt; None:\n        result = parse_gdelt_datetime(\"2024-01-15T12:00:00\")\n        assert result == datetime(2024, 1, 15, 12, 0, 0, tzinfo=UTC)\n\n    def test_iso_format_with_z(self) -&gt; None:\n        result = parse_gdelt_datetime(\"2024-01-15T12:00:00Z\")\n        assert result == datetime(2024, 1, 15, 12, 0, 0, tzinfo=UTC)\n\n    def test_iso_format_with_offset(self) -&gt; None:\n        # Input is +05:00, should convert to UTC (7:00 AM UTC)\n        result = parse_gdelt_datetime(\"2024-01-15T12:00:00+05:00\")\n        assert result == datetime(2024, 1, 15, 7, 0, 0, tzinfo=UTC)\n\n    def test_iso_format_with_microseconds(self) -&gt; None:\n        result = parse_gdelt_datetime(\"2024-01-15T12:00:00.123456\")\n        assert result.microsecond == 123456\n\n    # ISO date-only format\n    def test_iso_date_only(self) -&gt; None:\n        result = parse_gdelt_datetime(\"2024-01-15\")\n        assert result == datetime(2024, 1, 15, 0, 0, 0, tzinfo=UTC)\n\n    # Datetime passthrough with UTC conversion\n    def test_datetime_with_utc_passthrough(self) -&gt; None:\n        dt = datetime(2024, 1, 15, 12, 0, 0, tzinfo=UTC)\n        result = parse_gdelt_datetime(dt)\n        assert result == dt\n        assert result.tzinfo == UTC\n\n    def test_naive_datetime_gets_utc(self) -&gt; None:\n        dt = datetime(2024, 1, 15, 12, 0, 0)\n        result = parse_gdelt_datetime(dt)\n        assert result.tzinfo == UTC\n\n    def test_aware_datetime_converted_to_utc(self) -&gt; None:\n        # Create datetime with +05:00 offset\n        tz_plus5 = timezone(datetime.timedelta(hours=5))\n        dt = datetime(2024, 1, 15, 12, 0, 0, tzinfo=tz_plus5)\n        result = parse_gdelt_datetime(dt)\n        # Should be converted to 7:00 AM UTC\n        assert result == datetime(2024, 1, 15, 7, 0, 0, tzinfo=UTC)\n\n    # Error cases\n    def test_invalid_format_raises(self) -&gt; None:\n        with pytest.raises(ValueError, match=\"Invalid GDELT date format\"):\n            parse_gdelt_datetime(\"invalid\")\n\n    def test_empty_string_raises(self) -&gt; None:\n        with pytest.raises(ValueError, match=\"Invalid GDELT date format\"):\n            parse_gdelt_datetime(\"\")\n\n    def test_partial_format_raises(self) -&gt; None:\n        with pytest.raises(ValueError, match=\"Invalid GDELT date format\"):\n            parse_gdelt_datetime(\"2024011512\")  # 10 digits\n\n\nclass TestTryParseGdeltDatetime:\n    \"\"\"Tests for try_parse_gdelt_datetime (lenient).\"\"\"\n\n    def test_valid_format_returns_datetime(self) -&gt; None:\n        result = try_parse_gdelt_datetime(\"20240115120000\")\n        assert result == datetime(2024, 1, 15, 12, 0, 0, tzinfo=UTC)\n\n    def test_none_returns_none(self) -&gt; None:\n        assert try_parse_gdelt_datetime(None) is None\n\n    def test_invalid_format_returns_none(self) -&gt; None:\n        assert try_parse_gdelt_datetime(\"invalid\") is None\n\n    def test_empty_string_returns_none(self) -&gt; None:\n        assert try_parse_gdelt_datetime(\"\") is None\n\n    def test_integer_input(self) -&gt; None:\n        result = try_parse_gdelt_datetime(20240115120000)\n        assert result == datetime(2024, 1, 15, 12, 0, 0, tzinfo=UTC)\n\n\nclass TestParseGdeltDate:\n    \"\"\"Tests for parse_gdelt_date (strict).\"\"\"\n\n    def test_gdelt_format(self) -&gt; None:\n        result = parse_gdelt_date(\"20240115\")\n        assert result == date(2024, 1, 15)\n\n    def test_date_passthrough(self) -&gt; None:\n        d = date(2024, 1, 15)\n        assert parse_gdelt_date(d) is d\n\n    def test_datetime_extracts_date(self) -&gt; None:\n        dt = datetime(2024, 1, 15, 12, 0, 0, tzinfo=UTC)\n        assert parse_gdelt_date(dt) == date(2024, 1, 15)\n\n    def test_invalid_format_raises(self) -&gt; None:\n        with pytest.raises(ValueError, match=\"Invalid GDELT date format\"):\n            parse_gdelt_date(\"invalid\")\n\n    def test_empty_string_raises(self) -&gt; None:\n        with pytest.raises(ValueError, match=\"Invalid GDELT date format\"):\n            parse_gdelt_date(\"\")\n</code></pre>"},{"location":"plans/2026-01-25-refactor-shared-gdelt-date-parsing-plan/#review-findings-addressed","title":"Review Findings Addressed","text":"<p>This plan incorporates feedback from five reviewers:</p> Issue Resolution Missing 8-digit support in <code>parse_gdelt_datetime()</code> Added length-based dispatch for 8 and 14 digit formats Integer inputs not handled Added <code>int</code> to type signature with string conversion Brittle ISO detection (<code>\"T\" in value</code>) Added <code>\"-\" in value</code> check for date-only ISO format <code>try_parse_gdelt_date()</code> has no callers Removed (YAGNI) - reduced from 4 to 3 functions Type hint excludes <code>datetime</code> in <code>parse_gdelt_date</code> Fixed: <code>str \\| date \\| datetime</code> UTC not enforced for aware datetimes Use <code>.astimezone(UTC)</code> instead of passthrough Inconsistent error messages Unified to <code>f\"Invalid GDELT date format: {value!r}\"</code> Missing edge case tests Added: empty string, partial formats, timezone offsets, integers"},{"location":"plans/2026-01-25-refactor-shared-gdelt-date-parsing-plan/#references","title":"References","text":""},{"location":"plans/2026-01-25-refactor-shared-gdelt-date-parsing-plan/#internal-references","title":"Internal References","text":"<ul> <li>Existing utility pattern: <code>src/py_gdelt/utils/streaming.py</code></li> <li>Best implementation to extract: <code>src/py_gdelt/models/graphs.py:47-68</code> (<code>_parse_gdelt_date</code>)</li> <li>Lenient implementation pattern: <code>src/py_gdelt/endpoints/tv.py:530-557</code> (<code>_parse_date</code>)</li> </ul>"},{"location":"plans/2026-01-25-refactor-shared-gdelt-date-parsing-plan/#related-work","title":"Related Work","text":"<ul> <li>Origin: PR #66 code review (deferred as scope creep)</li> <li>GitHub Issue: #67</li> </ul>"},{"location":"plans/2026-01-25-refactor-shared-gdelt-date-parsing-plan/#tasks","title":"Tasks","text":"<p>Run <code>/workflows:work</code> with this plan to execute. Tasks are stored in <code>~/.claude/tasks/a05c4710-19d1-41fb-bee2-ea3fe0de2320/</code>.</p> <p>To work on these tasks from another session: <pre><code>skill: import-tasks a05c4710-19d1-41fb-bee2-ea3fe0de2320\n</code></pre></p>"},{"location":"user-guide/deduplication/","title":"Deduplication","text":"<p>GDELT data often contains duplicates. Use deduplication strategies to clean data.</p>"},{"location":"user-guide/deduplication/#strategies","title":"Strategies","text":"<ul> <li><code>URL_ONLY</code> - Deduplicate by source URL</li> <li><code>URL_DATE</code> - By URL and date</li> <li><code>URL_DATE_LOCATION</code> - By URL, date, and location</li> <li><code>ACTOR_PAIR</code> - By actor pair</li> <li><code>FULL</code> - By all fields</li> </ul>"},{"location":"user-guide/deduplication/#usage","title":"Usage","text":"<pre><code>from py_gdelt.utils.dedup import DedupeStrategy\n\nresult = await client.events.query(\n    event_filter,\n    deduplicate=True,\n    dedupe_strategy=DedupeStrategy.URL_DATE_LOCATION,\n)\n</code></pre> <p>For details, see Events guide.</p>"},{"location":"user-guide/errors/","title":"Error Handling","text":"<p>Proper error handling for robust applications.</p>"},{"location":"user-guide/errors/#exception-hierarchy","title":"Exception Hierarchy","text":"<ul> <li><code>GDELTError</code> - Base exception</li> <li><code>APIError</code> - API-related errors</li> <li><code>DataError</code> - Data parsing errors</li> <li><code>SecurityError</code> - Security violations</li> <li><code>ConfigurationError</code> - Configuration issues</li> </ul>"},{"location":"user-guide/errors/#usage","title":"Usage","text":"<pre><code>from py_gdelt.exceptions import APIError, DataError\n\ntry:\n    result = await client.doc.query(doc_filter)\nexcept APIError as e:\n    # Handle API errors (rate limiting, network, etc.)\n    logger.error(f\"API error: {e}\")\nexcept DataError as e:\n    # Handle data parsing errors\n    logger.error(f\"Data error: {e}\")\nexcept Exception as e:\n    # Handle unexpected errors\n    logger.error(f\"Unexpected error: {e}\")\n</code></pre> <p>For details, see API reference.</p>"},{"location":"user-guide/events/","title":"Events &amp; Mentions","text":"<p>Query GDELT Events and Mentions data from files or BigQuery.</p>"},{"location":"user-guide/events/#overview","title":"Overview","text":"<p>Events are the core of GDELT - structured records of \"who did what to whom, when, where, and how\" extracted from global news articles.</p>"},{"location":"user-guide/events/#basic-event-queries","title":"Basic Event Queries","text":"<pre><code>from datetime import date, timedelta\nfrom py_gdelt import GDELTClient\nfrom py_gdelt.filters import DateRange, EventFilter\n\nasync with GDELTClient() as client:\n    yesterday = date.today() - timedelta(days=1)\n\n    event_filter = EventFilter(\n        date_range=DateRange(start=yesterday, end=yesterday),\n        actor1_country=\"USA\",\n    )\n\n    events = await client.events.query(event_filter)\n    print(f\"Found {len(events)} events\")\n</code></pre>"},{"location":"user-guide/events/#event-model","title":"Event Model","text":"<p>Events contain: - global_event_id: Unique identifier - date: Event date - actor1, actor2: Participants (country, name, codes) - event_code: CAMEO event type code - goldstein_scale: Conflict/cooperation score (-10 to +10) - avg_tone: Sentiment (-100 to +100) - action_geo: Location information - source_url: Article URL</p>"},{"location":"user-guide/events/#filtering-options","title":"Filtering Options","text":""},{"location":"user-guide/events/#by-actors","title":"By Actors","text":"<pre><code>event_filter = EventFilter(\n    date_range=DateRange(start=date(2024, 1, 1)),\n    actor1_country=\"USA\",\n    actor2_country=\"CHN\",\n)\n</code></pre>"},{"location":"user-guide/events/#by-event-type","title":"By Event Type","text":"<pre><code>event_filter = EventFilter(\n    date_range=DateRange(start=date(2024, 1, 1)),\n    event_code=\"14\",  # Protest\n)\n</code></pre>"},{"location":"user-guide/events/#by-tone","title":"By Tone","text":"<pre><code>event_filter = EventFilter(\n    date_range=DateRange(start=date(2024, 1, 1)),\n    min_tone=-5.0,  # Negative events\n    max_tone=0.0,\n)\n</code></pre>"},{"location":"user-guide/events/#by-location","title":"By Location","text":"<pre><code>event_filter = EventFilter(\n    date_range=DateRange(start=date(2024, 1, 1)),\n    country_code=\"US\",\n)\n</code></pre>"},{"location":"user-guide/events/#streaming-events","title":"Streaming Events","text":"<p>For large datasets, use streaming:</p> <pre><code>async with GDELTClient() as client:\n    event_filter = EventFilter(\n        date_range=DateRange(\n            start=date(2024, 1, 1),\n            end=date(2024, 1, 7),\n        ),\n    )\n\n    async for event in client.events.stream(event_filter):\n        process(event)  # Process one at a time\n</code></pre>"},{"location":"user-guide/events/#deduplication","title":"Deduplication","text":"<p>GDELT often contains duplicate events. Use deduplication:</p> <pre><code>from py_gdelt.utils.dedup import DedupeStrategy\n\nresult = await client.events.query(\n    event_filter,\n    deduplicate=True,\n    dedupe_strategy=DedupeStrategy.URL_DATE_LOCATION,\n)\n</code></pre> <p>Available strategies: - <code>URL_ONLY</code> - By source URL - <code>URL_DATE</code> - By URL and date - <code>URL_DATE_LOCATION</code> - By URL, date, and location - <code>ACTOR_PAIR</code> - By actor pair - <code>FULL</code> - By all fields</p>"},{"location":"user-guide/events/#mentions","title":"Mentions","text":"<p>Mentions track article citations of events:</p> <pre><code>async with GDELTClient() as client:\n    mentions = await client.mentions.query(\"123456789\", event_filter)\n</code></pre>"},{"location":"user-guide/events/#bigquery-fallback","title":"BigQuery Fallback","text":"<p>When file sources fail, automatically fallback to BigQuery:</p> <pre><code>settings = GDELTSettings(\n    fallback_to_bigquery=True,\n    bigquery_project=\"my-project\",\n)\n\nasync with GDELTClient(settings=settings) as client:\n    # Automatically uses BigQuery if files unavailable\n    events = await client.events.query(event_filter)\n</code></pre>"},{"location":"user-guide/events/#best-practices","title":"Best Practices","text":"<ul> <li>Use streaming for &gt;1000 events</li> <li>Enable deduplication for cleaner data</li> <li>Use specific filters to reduce data volume</li> <li>Handle empty results gracefully</li> <li>Set appropriate date ranges (files available for 2015+)</li> </ul>"},{"location":"user-guide/gkg/","title":"Global Knowledge Graph (GKG)","text":"<p>The GKG endpoint provides rich semantic metadata extracted from news articles.</p>"},{"location":"user-guide/gkg/#overview","title":"Overview","text":"<p>GKG records contain: - Themes (ENV_CLIMATE, ECON_STOCKMARKET, etc.) - Entities (people, organizations, locations) - Quotations - Tone/sentiment - Source metadata</p>"},{"location":"user-guide/gkg/#basic-usage","title":"Basic Usage","text":"<pre><code>from py_gdelt import GDELTClient\nfrom py_gdelt.filters import DateRange, GKGFilter\nfrom datetime import date\n\nasync with GDELTClient() as client:\n    gkg_filter = GKGFilter(\n        date_range=DateRange(start=date(2024, 1, 1)),\n        themes=[\"ENV_CLIMATECHANGE\"],\n    )\n\n    records = await client.gkg.query(gkg_filter)\n</code></pre> <p>For details, see GKG example.</p>"},{"location":"user-guide/lookups/","title":"Lookup Tables","text":"<p>GDELT provides lookup tables for codes and classifications.</p>"},{"location":"user-guide/lookups/#available-lookups","title":"Available Lookups","text":"<ul> <li>CAMEO - Event codes and descriptions</li> <li>Themes - Theme taxonomy</li> <li>Countries - Country code conversions</li> <li>Ethnic/Religious Groups - Group classifications</li> </ul>"},{"location":"user-guide/lookups/#usage","title":"Usage","text":"<pre><code>async with GDELTClient() as client:\n    # CAMEO codes\n    cameo = client.lookups.cameo\n    event = cameo.get(\"14\")  # PROTEST\n\n    # Country conversions\n    countries = client.lookups.countries\n    iso3 = countries.fips_to_iso3(\"US\")  # USA\n</code></pre> <p>For details, see examples.</p>"},{"location":"user-guide/ngrams/","title":"NGrams","text":"<p>Query word and phrase occurrences from GDELT NGrams 3.0.</p>"},{"location":"user-guide/ngrams/#overview","title":"Overview","text":"<p>NGrams track word/phrase positions within articles for linguistic analysis.</p>"},{"location":"user-guide/ngrams/#basic-usage","title":"Basic Usage","text":"<pre><code>from py_gdelt import GDELTClient\nfrom py_gdelt.filters import DateRange, NGramsFilter\nfrom datetime import date\n\nasync with GDELTClient() as client:\n    ngrams_filter = NGramsFilter(\n        date_range=DateRange(start=date(2024, 1, 1)),\n        ngram=\"climate\",\n        language=\"en\",\n    )\n\n    records = await client.ngrams.query(ngrams_filter)\n</code></pre> <p>For details, see NGrams example.</p>"},{"location":"user-guide/rest-apis/","title":"REST APIs","text":"<p>GDELT provides several REST APIs for searching and analyzing global news.</p>"},{"location":"user-guide/rest-apis/#doc-20-api-article-search","title":"DOC 2.0 API - Article Search","text":"<p>Search for news articles:</p> <pre><code>from py_gdelt.filters import DocFilter\n\nasync with GDELTClient() as client:\n    doc_filter = DocFilter(\n        query=\"climate change\",\n        timespan=\"24h\",\n        max_results=100,\n        sort_by=\"relevance\",\n    )\n\n    articles = await client.doc.query(doc_filter)\n\n    for article in articles:\n        print(f\"{article.title}\")\n        print(f\"  {article.url}\")\n</code></pre>"},{"location":"user-guide/rest-apis/#timeline-analysis","title":"Timeline Analysis","text":"<pre><code>timeline = await client.doc.timeline(\n    query=\"artificial intelligence\",\n    timespan=\"7d\",\n)\n\nfor point in timeline.points:\n    print(f\"{point.date}: {point.count} articles\")\n</code></pre>"},{"location":"user-guide/rest-apis/#geo-20-api-geographic-search","title":"GEO 2.0 API - Geographic Search","text":"<p>Find geographic locations mentioned in news:</p> <pre><code>async with GDELTClient() as client:\n    result = await client.geo.search(\n        \"earthquake\",\n        timespan=\"7d\",\n        max_points=50,\n    )\n\n    for point in result.points:\n        print(f\"{point.name}: {point.count} articles\")\n        print(f\"  ({point.lat}, {point.lon})\")\n</code></pre>"},{"location":"user-guide/rest-apis/#bounding-box-filtering","title":"Bounding Box Filtering","text":"<pre><code># Europe bounding box\neurope_bbox = (35.0, -10.0, 70.0, 40.0)\n\nresult = await client.geo.search(\n    \"protests\",\n    timespan=\"7d\",\n    bounding_box=europe_bbox,\n)\n</code></pre>"},{"location":"user-guide/rest-apis/#geojson-output","title":"GeoJSON Output","text":"<pre><code>geojson = await client.geo.to_geojson(\n    \"climate protest\",\n    timespan=\"30d\",\n)\n\n# Use with folium, leaflet, etc.\n</code></pre>"},{"location":"user-guide/rest-apis/#context-20-api-contextual-analysis","title":"Context 2.0 API - Contextual Analysis","text":"<p>Analyze themes, entities, and sentiment:</p> <pre><code>async with GDELTClient() as client:\n    result = await client.context.analyze(\n        \"technology\",\n        timespan=\"7d\",\n    )\n\n    print(f\"Articles: {result.article_count}\")\n\n    # Top themes\n    for theme in result.themes[:10]:\n        print(f\"  {theme.theme}: {theme.count}\")\n\n    # Top entities\n    for entity in result.entities[:10]:\n        print(f\"  {entity.name} ({entity.entity_type}): {entity.count}\")\n\n    # Sentiment\n    if result.tone:\n        print(f\"Average tone: {result.tone.average_tone}\")\n</code></pre>"},{"location":"user-guide/rest-apis/#entity-filtering","title":"Entity Filtering","text":"<pre><code># Get people mentioned\npeople = await client.context.get_entities(\n    \"election\",\n    entity_type=\"PERSON\",\n    limit=20,\n)\n\n# Get organizations\norgs = await client.context.get_entities(\n    \"economy\",\n    entity_type=\"ORG\",\n    limit=20,\n)\n</code></pre>"},{"location":"user-guide/rest-apis/#tv-api-television-news","title":"TV API - Television News","text":"<p>Search TV transcripts:</p> <pre><code>async with GDELTClient() as client:\n    clips = await client.tv.search(\n        \"healthcare\",\n        timespan=\"24h\",\n        station=\"CNN\",\n        max_results=20,\n    )\n\n    for clip in clips:\n        print(f\"{clip.station} - {clip.show_name}\")\n        print(f\"  {clip.snippet}\")\n</code></pre>"},{"location":"user-guide/rest-apis/#tv-timeline","title":"TV Timeline","text":"<pre><code>timeline = await client.tv.timeline(\n    \"election\",\n    timespan=\"7d\",\n)\n</code></pre>"},{"location":"user-guide/rest-apis/#station-comparison","title":"Station Comparison","text":"<pre><code>chart = await client.tv.station_chart(\n    \"immigration\",\n    timespan=\"7d\",\n)\n\nfor station in chart.stations:\n    print(f\"{station.station}: {station.count} ({station.percentage}%)\")\n</code></pre>"},{"location":"user-guide/rest-apis/#tvai-api-ai-enhanced-tv-search","title":"TVAI API - AI-Enhanced TV Search","text":"<p>Use AI for better TV transcript search:</p> <pre><code>clips = await client.tv_ai.search(\n    \"impact of artificial intelligence on employment\",\n    timespan=\"7d\",\n    max_results=10,\n)\n</code></pre>"},{"location":"user-guide/rest-apis/#timespan-options","title":"Timespan Options","text":"<p>All REST APIs support these timespans: - <code>\"15min\"</code> - Last 15 minutes - <code>\"30min\"</code> - Last 30 minutes - <code>\"1h\"</code> - Last hour - <code>\"6h\"</code> - Last 6 hours - <code>\"24h\"</code> - Last 24 hours - <code>\"7d\"</code> - Last 7 days - <code>\"30d\"</code> - Last 30 days</p>"},{"location":"user-guide/rest-apis/#rate-limiting","title":"Rate Limiting","text":"<p>GDELT APIs may rate limit. Handle gracefully:</p> <pre><code>from py_gdelt.exceptions import APIError\n\ntry:\n    result = await client.doc.query(doc_filter)\nexcept APIError as e:\n    if \"rate limit\" in str(e).lower():\n        # Wait and retry\n        await asyncio.sleep(60)\n</code></pre>"},{"location":"user-guide/rest-apis/#best-practices","title":"Best Practices","text":"<ul> <li>Use appropriate timespans (shorter = faster)</li> <li>Limit result counts to what you need</li> <li>Handle empty results gracefully</li> <li>Respect rate limits</li> <li>Cache results when appropriate</li> </ul>"},{"location":"user-guide/streaming/","title":"Streaming Large Datasets","text":"<p>Memory-efficient data processing with streaming.</p>"},{"location":"user-guide/streaming/#why-stream","title":"Why Stream?","text":"<p>Loading large datasets into memory can exhaust resources. Streaming processes data incrementally:</p> <ul> <li>Memory Efficient: Process millions of records without loading all at once</li> <li>Faster Start: Begin processing immediately without waiting for complete download</li> <li>Scalable: Handle datasets of any size</li> <li>Interruptible: Stop early if you find what you need</li> </ul>"},{"location":"user-guide/streaming/#basic-streaming","title":"Basic Streaming","text":"<pre><code>from py_gdelt import GDELTClient\nfrom py_gdelt.filters import DateRange, EventFilter\nfrom datetime import date, timedelta\n\nasync with GDELTClient() as client:\n    event_filter = EventFilter(\n        date_range=DateRange(\n            start=date(2024, 1, 1),\n            end=date(2024, 1, 7),\n        ),\n    )\n\n    # Stream instead of query()\n    async for event in client.events.stream(event_filter):\n        process(event)  # Process one at a time\n</code></pre>"},{"location":"user-guide/streaming/#filtering-while-streaming","title":"Filtering While Streaming","text":"<p>Apply additional filters during streaming:</p> <pre><code>us_protest_count = 0\n\nasync for event in client.events.stream(event_filter):\n    # Filter in-stream\n    if event.event_code == \"14\":  # Protest\n        if hasattr(event, 'actor1') and event.actor1:\n            if hasattr(event.actor1, 'country_code') and event.actor1.country_code == 'US':\n                us_protest_count += 1\n</code></pre>"},{"location":"user-guide/streaming/#early-exit","title":"Early Exit","text":"<p>Stop processing when you have enough data:</p> <pre><code>count = 0\nasync for event in client.events.stream(event_filter):\n    count += 1\n    if count &gt;= 1000:\n        break  # Stop after 1000 events\n</code></pre>"},{"location":"user-guide/streaming/#batching","title":"Batching","text":"<p>Process in batches for efficiency:</p> <pre><code>batch_size = 100\nbatch = []\n\nasync for event in client.events.stream(event_filter):\n    batch.append(event)\n\n    if len(batch) &gt;= batch_size:\n        process_batch(batch)\n        batch = []\n\n# Process remaining\nif batch:\n    process_batch(batch)\n</code></pre>"},{"location":"user-guide/streaming/#gkg-streaming","title":"GKG Streaming","text":"<p>Stream GKG records similarly:</p> <pre><code>from py_gdelt.filters import GKGFilter\n\ngkg_filter = GKGFilter(\n    date_range=DateRange(start=date(2024, 1, 1)),\n    themes=[\"ENV_CLIMATECHANGE\"],\n)\n\nasync for record in client.gkg.stream(gkg_filter):\n    # Process GKG record\n    for theme in record.themes:\n        print(theme.name)\n</code></pre>"},{"location":"user-guide/streaming/#ngrams-streaming","title":"NGrams Streaming","text":"<p>Stream word/phrase occurrences:</p> <pre><code>from py_gdelt.filters import NGramsFilter\n\nngrams_filter = NGramsFilter(\n    date_range=DateRange(start=date(2024, 1, 1)),\n    ngram=\"climate\",\n    language=\"en\",\n)\n\nasync for ngram in client.ngrams.stream(ngrams_filter):\n    print(f\"{ngram.context}\")\n</code></pre>"},{"location":"user-guide/streaming/#memory-monitoring","title":"Memory Monitoring","text":"<p>Track memory usage during streaming:</p> <pre><code>import psutil\nimport os\n\nprocess = psutil.Process(os.getpid())\n\ncount = 0\nasync for event in client.events.stream(event_filter):\n    count += 1\n\n    if count % 10000 == 0:\n        mem_mb = process.memory_info().rss / 1024 / 1024\n        print(f\"Processed {count} events, Memory: {mem_mb:.2f} MB\")\n</code></pre>"},{"location":"user-guide/streaming/#error-handling","title":"Error Handling","text":"<p>Handle errors gracefully during streaming:</p> <pre><code>from py_gdelt.exceptions import DataError\n\ntry:\n    async for event in client.events.stream(event_filter):\n        try:\n            process(event)\n        except Exception as e:\n            # Log and continue\n            logger.error(f\"Error processing event: {e}\")\n            continue\nexcept DataError as e:\n    logger.error(f\"Data stream error: {e}\")\n</code></pre>"},{"location":"user-guide/streaming/#comparison-query-vs-stream","title":"Comparison: Query vs Stream","text":"<pre><code># query() - Loads all into memory\nresult = await client.events.query(event_filter)\nfor event in result:\n    process(event)\n# Memory: ~500MB for 100k events\n\n# stream() - Process incrementally\nasync for event in client.events.stream(event_filter):\n    process(event)\n# Memory: ~50MB constant\n</code></pre>"},{"location":"user-guide/streaming/#best-practices","title":"Best Practices","text":"<ul> <li>Use <code>stream()</code> for &gt;1000 records</li> <li>Use <code>query()</code> for &lt;1000 records (simpler)</li> <li>Batch processing for efficiency</li> <li>Handle errors per-record, not per-stream</li> <li>Monitor memory in production</li> <li>Use early exit to save resources</li> <li>Apply filters early to reduce data volume</li> </ul>"}]}