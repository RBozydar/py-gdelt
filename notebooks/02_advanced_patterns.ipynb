{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Usage Patterns\n",
    "\n",
    "Production-ready patterns and best practices for py-gdelt.\n",
    "\n",
    "## Contents\n",
    "1. Configuration Management\n",
    "2. Error Handling and Retry Strategies\n",
    "3. Deduplication Strategies\n",
    "4. Memory-Efficient Streaming\n",
    "5. Combining Multiple Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import logging\n",
    "from datetime import date, timedelta\n",
    "\n",
    "from py_gdelt import GDELTClient, GDELTSettings\n",
    "from py_gdelt.filters import DateRange, EventFilter\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Management\n",
    "\n",
    "### 1.1 Programmatic Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom settings for production\n",
    "settings = GDELTSettings(\n",
    "    timeout=60,  # Longer timeout for slow connections\n",
    "    max_retries=5,  # More retries for reliability\n",
    "    max_concurrent_downloads=10,  # Higher concurrency\n",
    "    cache_ttl=7200,  # 2 hour cache\n",
    "    validate_codes=True,  # Enable validation\n",
    ")\n",
    "\n",
    "async with GDELTClient(settings=settings) as client:\n",
    "    print(f\"Timeout: {client.settings.timeout}s\")\n",
    "    print(f\"Max retries: {client.settings.max_retries}\")\n",
    "    print(f\"Cache TTL: {client.settings.cache_ttl}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Environment Variables\n",
    "\n",
    "```bash\n",
    "export GDELT_TIMEOUT=60\n",
    "export GDELT_MAX_RETRIES=5\n",
    "export GDELT_CACHE_DIR=/path/to/cache\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Error Handling and Retry Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_gdelt.exceptions import APIError, DataError\n",
    "\n",
    "\n",
    "async with GDELTClient() as client:\n",
    "    try:\n",
    "        # Query with error handling\n",
    "        from py_gdelt.filters import DocFilter\n",
    "\n",
    "        doc_filter = DocFilter(\n",
    "            query=\"technology\",\n",
    "            timespan=\"24h\",\n",
    "            max_results=10,\n",
    "        )\n",
    "\n",
    "        articles = await client.doc.query(doc_filter)\n",
    "        print(f\"Success: Found {len(articles)} articles\")\n",
    "\n",
    "    except APIError as e:\n",
    "        print(f\"API Error: {e}\")\n",
    "        # Handle API-specific errors (rate limiting, etc.)\n",
    "\n",
    "    except DataError as e:\n",
    "        print(f\"Data Error: {e}\")\n",
    "        # Handle data parsing errors\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        # Handle any other errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deduplication Strategies\n",
    "\n",
    "GDELT data often contains duplicates. The library provides multiple strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async with GDELTClient() as client:\n",
    "    yesterday = date.today() - timedelta(days=2)\n",
    "\n",
    "    event_filter = EventFilter(\n",
    "        date_range=DateRange(start=yesterday, end=yesterday),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Without deduplication\n",
    "        result_raw = await client.events.query(event_filter)\n",
    "        print(f\"Without dedup: {len(result_raw)} events\")\n",
    "\n",
    "        # With URL-based deduplication\n",
    "        # result_dedup = await client.events.query(\n",
    "        #     event_filter,\n",
    "        #     deduplicate=True,\n",
    "        #     dedupe_strategy=DedupeStrategy.URL_ONLY,\n",
    "        # )\n",
    "        # print(f\"With URL dedup: {len(result_dedup)} events\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available Deduplication Strategies:\n",
    "\n",
    "- `URL_ONLY` - Deduplicate by source URL only\n",
    "- `URL_DATE` - Deduplicate by URL and date\n",
    "- `URL_DATE_LOCATION` - Deduplicate by URL, date, and location\n",
    "- `ACTOR_PAIR` - Deduplicate by actor pair\n",
    "- `FULL` - Deduplicate by all available fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Memory-Efficient Streaming\n",
    "\n",
    "For large datasets, always prefer streaming over loading all data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async with GDELTClient() as client:\n",
    "    yesterday = date.today() - timedelta(days=2)\n",
    "\n",
    "    event_filter = EventFilter(\n",
    "        date_range=DateRange(start=yesterday, end=yesterday),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Process events one at a time\n",
    "        event_count = 0\n",
    "        us_events = 0\n",
    "\n",
    "        async for event in client.events.stream(event_filter):\n",
    "            event_count += 1\n",
    "\n",
    "            # Filter and process in-stream\n",
    "            if hasattr(event, \"actor1\") and event.actor1:\n",
    "                if hasattr(event.actor1, \"country_code\") and event.actor1.country_code == \"US\":\n",
    "                    us_events += 1\n",
    "\n",
    "            # Early exit for demo\n",
    "            if event_count >= 100:\n",
    "                break\n",
    "\n",
    "        print(f\"Processed {event_count} events, found {us_events} US events\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combining Multiple Data Sources\n",
    "\n",
    "Combine Events with GKG for enriched analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_gdelt.filters import GKGFilter\n",
    "\n",
    "\n",
    "async with GDELTClient() as client:\n",
    "    yesterday = date.today() - timedelta(days=2)\n",
    "\n",
    "    try:\n",
    "        # Get events\n",
    "        event_filter = EventFilter(\n",
    "            date_range=DateRange(start=yesterday, end=yesterday),\n",
    "            actor1_country=\"USA\",\n",
    "        )\n",
    "\n",
    "        # Get GKG records for same period\n",
    "        gkg_filter = GKGFilter(\n",
    "            date_range=DateRange(start=yesterday, end=yesterday),\n",
    "            themes=[\"ECON_STOCKMARKET\"],\n",
    "        )\n",
    "\n",
    "        # Note: In production, you'd correlate these by URL or time\n",
    "        print(\"Querying events...\")\n",
    "        events_count = 0\n",
    "        async for event in client.events.stream(event_filter):\n",
    "            events_count += 1\n",
    "            if events_count >= 10:\n",
    "                break\n",
    "\n",
    "        print(f\"Events sample: {events_count}\")\n",
    "\n",
    "        print(\"\\nQuerying GKG...\")\n",
    "        gkg_count = 0\n",
    "        async for record in client.gkg.stream(gkg_filter):\n",
    "            gkg_count += 1\n",
    "            if gkg_count >= 10:\n",
    "                break\n",
    "\n",
    "        print(f\"GKG sample: {gkg_count}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned:\n",
    "- ✅ How to configure the client for production use\n",
    "- ✅ Proper error handling strategies\n",
    "- ✅ Deduplication techniques for cleaner data\n",
    "- ✅ Memory-efficient streaming patterns\n",
    "- ✅ Combining multiple GDELT data sources\n",
    "\n",
    "**Best Practices:**\n",
    "- Always use streaming for large datasets\n",
    "- Handle errors gracefully with try/except\n",
    "- Choose appropriate deduplication strategy for your use case\n",
    "- Configure timeouts and retries based on your network\n",
    "- Use caching to reduce API calls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
