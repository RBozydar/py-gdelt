# gdelt-py

> Python client library for GDELT (Global Database of Events, Language, and Tone)

gdelt-py is a Python client library for GDELT (Global Database of Events, Language, and Tone).
It provides unified access to all 6 REST APIs, 3 database tables (Events, Mentions, GKG),
and NGrams dataset with modern async-first design, Pydantic models, and streaming support.


# Getting Started

# Installation

## Requirements

- Python 3.11 or higher
- pip (Python package manager)

## Basic Installation

Install py-gdelt using pip:

```
pip install py-gdelt
```

This installs the core library with support for:

- Events, Mentions, GKG endpoints (file-based)
- NGrams endpoint
- All REST APIs (DOC, GEO, Context, TV, TVAI)
- Lookup tables

## Optional Dependencies

### BigQuery Support

For BigQuery data access and automatic fallback:

```
pip install py-gdelt[bigquery]
```

This adds:

- `google-cloud-bigquery>=3.0`
- Ability to query GDELT BigQuery datasets
- Automatic fallback when file sources fail

### Pandas Integration

For data analysis and manipulation:

```
pip install py-gdelt[pandas]
```

This adds:

- `pandas>=2.0`
- DataFrame conversion utilities
- Easier data manipulation

### All Optional Dependencies

Install everything:

```
pip install py-gdelt[bigquery,pandas]
```

## Development Installation

For contributing to py-gdelt:

```
# Clone the repository
git clone https://github.com/rbwasilewski/py-gdelt.git
cd py-gdelt

# Install in editable mode with dev dependencies
pip install -e ".[dev,bigquery,pandas]"
```

Development dependencies include:

- pytest for testing
- pytest-asyncio for async tests
- pytest-cov for coverage
- pytest-timeout for test timeouts
- mypy for type checking
- ruff for linting
- respx for HTTP mocking

## Verification

Verify your installation:

```
import py_gdelt
print(py_gdelt.__version__)

from py_gdelt import GDELTClient
print("Installation successful!")
```

## Upgrading

Upgrade to the latest version:

```
pip install --upgrade py-gdelt
```

## Uninstallation

Remove py-gdelt:

```
pip uninstall py-gdelt
```

## Next Steps

- [Quick Start Guide](https://rbozydar.github.io/py-gdelt/getting-started/quickstart/index.md)
- [Configuration](https://rbozydar.github.io/py-gdelt/getting-started/configuration/index.md)
- [Examples](https://rbozydar.github.io/py-gdelt/examples/index.md)

# Quick Start

This guide will get you up and running with py-gdelt in 5 minutes.

## Your First Query

Let's query recent events from the US:

```
import asyncio
from datetime import date, timedelta
from py_gdelt import GDELTClient
from py_gdelt.filters import DateRange, EventFilter

async def main():
    async with GDELTClient() as client:
        # Query yesterday's events
        yesterday = date.today() - timedelta(days=1)

        event_filter = EventFilter(
            date_range=DateRange(start=yesterday, end=yesterday),
            actor1_country="USA",
        )

        result = await client.events.query(event_filter)
        print(f"Found {len(result)} events")

        if result:
            event = result[0]
            print(f"First event: {event.global_event_id}")

if __name__ == "__main__":
    asyncio.run(main())
```

## Search Articles

Use the DOC API to search for articles:

```
from py_gdelt.filters import DocFilter

async with GDELTClient() as client:
    doc_filter = DocFilter(
        query="climate change",
        timespan="24h",
        max_results=10,
    )

    articles = await client.doc.query(doc_filter)

    for article in articles:
        print(f"{article.title}")
        print(f"  {article.url}")
```

## Geographic Search

Find locations mentioned in news:

```
async with GDELTClient() as client:
    result = await client.geo.search(
        "earthquake",
        timespan="7d",
        max_points=20,
    )

    for point in result.points:
        print(f"{point.name}: {point.count} articles")
```

## Contextual Analysis

Analyze themes and entities:

```
async with GDELTClient() as client:
    result = await client.context.analyze(
        "artificial intelligence",
        timespan="7d",
    )

    print(f"Articles analyzed: {result.article_count}")

    for theme in result.themes[:5]:
        print(f"  {theme.theme}: {theme.count}")
```

## Streaming Large Datasets

For memory efficiency, use streaming:

```
async with GDELTClient() as client:
    yesterday = date.today() - timedelta(days=1)

    event_filter = EventFilter(
        date_range=DateRange(start=yesterday, end=yesterday),
    )

    count = 0
    async for event in client.events.stream(event_filter):
        count += 1
        # Process event without loading all into memory

    print(f"Processed {count} events")
```

## Synchronous Usage

For non-async code:

```
with GDELTClient() as client:
    yesterday = date.today() - timedelta(days=1)

    event_filter = EventFilter(
        date_range=DateRange(start=yesterday, end=yesterday),
        actor1_country="USA",
    )

    result = client.events.query_sync(event_filter)
    print(f"Found {len(result)} events")
```

## Using Lookup Tables

Access CAMEO codes and other lookups:

```
async with GDELTClient() as client:
    # CAMEO event codes
    cameo = client.lookups.cameo
    event_code = cameo.get("14")
    print(f"Code 14: {event_code.name}")  # "PROTEST"

    # Country conversions
    countries = client.lookups.countries
    iso3 = countries.fips_to_iso3("US")
    print(f"US -> {iso3}")  # "USA"
```

## Error Handling

Always handle errors gracefully:

```
from py_gdelt.exceptions import APIError, DataError

async with GDELTClient() as client:
    try:
        result = await client.doc.query(doc_filter)
    except APIError as e:
        print(f"API error: {e}")
    except DataError as e:
        print(f"Data error: {e}")
    except Exception as e:
        print(f"Unexpected error: {e}")
```

## Next Steps

- [Configuration Guide](https://rbozydar.github.io/py-gdelt/getting-started/configuration/index.md) - Customize settings
- [User Guide](https://rbozydar.github.io/py-gdelt/user-guide/events/index.md) - Deep dive into features
- [Examples](https://rbozydar.github.io/py-gdelt/examples/index.md) - More complete examples
- [API Reference](https://rbozydar.github.io/py-gdelt/api/client/index.md) - Full API documentation

# Configuration

Configure py-gdelt using environment variables, TOML files, or programmatic settings.

## Environment Variables

Set environment variables to configure default behavior:

```
# Timeouts and retries
export GDELT_TIMEOUT=60
export GDELT_MAX_RETRIES=5

# Caching
export GDELT_CACHE_DIR=/path/to/cache
export GDELT_CACHE_TTL=3600

# BigQuery
export GDELT_BIGQUERY_PROJECT=my-project
export GDELT_BIGQUERY_CREDENTIALS=/path/to/credentials.json

# Behavior
export GDELT_FALLBACK_TO_BIGQUERY=true
export GDELT_VALIDATE_CODES=true
export GDELT_MAX_CONCURRENT_DOWNLOADS=10
```

## TOML Configuration

Create a `gdelt.toml` file:

```
[gdelt]
timeout = 60
max_retries = 5
cache_dir = "/path/to/cache"
cache_ttl = 3600
fallback_to_bigquery = true
validate_codes = true
max_concurrent_downloads = 10

[gdelt.bigquery]
project = "my-project"
credentials = "/path/to/credentials.json"
```

Load it with:

```
from pathlib import Path
from py_gdelt import GDELTClient

config_path = Path("gdelt.toml")
async with GDELTClient(config_path=config_path) as client:
    ...
```

## Programmatic Settings

Use `GDELTSettings` for full control:

```
from pathlib import Path
from py_gdelt import GDELTClient, GDELTSettings

settings = GDELTSettings(
    # Timeouts and retries
    timeout=60,
    max_retries=5,

    # Caching
    cache_dir=Path.home() / ".cache" / "gdelt",
    cache_ttl=3600,

    # BigQuery
    bigquery_project="my-project",
    bigquery_credentials=Path("/path/to/credentials.json"),

    # Behavior
    fallback_to_bigquery=True,
    validate_codes=True,
    max_concurrent_downloads=10,
)

async with GDELTClient(settings=settings) as client:
    ...
```

## Configuration Options

### Timeouts

- **timeout** (int): HTTP request timeout in seconds. Default: `30`
- **connect_timeout** (int): Connection timeout in seconds. Default: `10`

### Retries

- **max_retries** (int): Maximum retry attempts. Default: `3`
- **retry_backoff** (float): Backoff multiplier for retries. Default: `2.0`

### Caching

- **cache_dir** (Path): Directory for cached files. Default: `~/.cache/gdelt`
- **cache_ttl** (int): Cache time-to-live in seconds. Default: `3600` (1 hour)
- **use_cache** (bool): Enable/disable caching. Default: `True`

### BigQuery

- **bigquery_project** (str): Google Cloud project ID. Default: `None`
- **bigquery_credentials** (Path): Path to credentials JSON. Default: `None`
- **fallback_to_bigquery** (bool): Use BigQuery when file sources fail. Default: `False`

### Performance

- **max_concurrent_downloads** (int): Max parallel file downloads. Default: `5`
- **chunk_size** (int): Download chunk size in bytes. Default: `8192`

### Validation

- **validate_codes** (bool): Validate CAMEO/country codes. Default: `False`
- **strict_mode** (bool): Raise errors on invalid codes. Default: `False`

## Priority Order

Configuration is loaded in this order (later overrides earlier):

1. Default values
1. Environment variables
1. TOML configuration file
1. Programmatic `GDELTSettings`

## BigQuery Setup

For BigQuery access:

1. Install dependencies: `pip install py-gdelt[bigquery]`
1. Create Google Cloud project
1. Enable BigQuery API
1. Create service account and download credentials
1. Set credentials path:

```
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/credentials.json"
```

Or in code:

```
settings = GDELTSettings(
    bigquery_project="my-project",
    bigquery_credentials=Path("/path/to/credentials.json"),
    fallback_to_bigquery=True,
)
```

## Best Practices

- Use environment variables for sensitive data (credentials)
- Use TOML for team-shared configuration
- Use programmatic settings for runtime customization
- Enable caching in development, configure carefully in production
- Set appropriate timeouts for your network conditions
- Enable BigQuery fallback for production reliability
# User Guide

# Events & Mentions

Query GDELT Events and Mentions data from files or BigQuery.

## Overview

Events are the core of GDELT - structured records of "who did what to whom, when, where, and how" extracted from global news articles.

## Basic Event Queries

```
from datetime import date, timedelta
from py_gdelt import GDELTClient
from py_gdelt.filters import DateRange, EventFilter

async with GDELTClient() as client:
    yesterday = date.today() - timedelta(days=1)

    event_filter = EventFilter(
        date_range=DateRange(start=yesterday, end=yesterday),
        actor1_country="USA",
    )

    events = await client.events.query(event_filter)
    print(f"Found {len(events)} events")
```

## Event Model

Events contain:

- **global_event_id**: Unique identifier
- **date**: Event date
- **actor1**, **actor2**: Participants (country, name, codes)
- **event_code**: CAMEO event type code
- **goldstein_scale**: Conflict/cooperation score (-10 to +10)
- **avg_tone**: Sentiment (-100 to +100)
- **action_geo**: Location information
- **source_url**: Article URL

## Filtering Options

### By Actors

```
event_filter = EventFilter(
    date_range=DateRange(start=date(2024, 1, 1)),
    actor1_country="USA",
    actor2_country="CHN",
)
```

### By Event Type

```
event_filter = EventFilter(
    date_range=DateRange(start=date(2024, 1, 1)),
    event_code="14",  # Protest
)
```

### By Tone

```
event_filter = EventFilter(
    date_range=DateRange(start=date(2024, 1, 1)),
    min_tone=-5.0,  # Negative events
    max_tone=0.0,
)
```

### By Location

```
event_filter = EventFilter(
    date_range=DateRange(start=date(2024, 1, 1)),
    country_code="US",
)
```

## Streaming Events

For large datasets, use streaming:

```
async with GDELTClient() as client:
    event_filter = EventFilter(
        date_range=DateRange(
            start=date(2024, 1, 1),
            end=date(2024, 1, 7),
        ),
    )

    async for event in client.events.stream(event_filter):
        process(event)  # Process one at a time
```

## Deduplication

GDELT often contains duplicate events. Use deduplication:

```
from py_gdelt.utils.dedup import DedupeStrategy

result = await client.events.query(
    event_filter,
    deduplicate=True,
    dedupe_strategy=DedupeStrategy.URL_DATE_LOCATION,
)
```

Available strategies:

- `URL_ONLY` - By source URL
- `URL_DATE` - By URL and date
- `URL_DATE_LOCATION` - By URL, date, and location
- `ACTOR_PAIR` - By actor pair
- `FULL` - By all fields

## Mentions

Mentions track article citations of events:

```
async with GDELTClient() as client:
    mentions = await client.mentions.query("123456789", event_filter)
```

## BigQuery Fallback

When file sources fail, automatically fallback to BigQuery:

```
settings = GDELTSettings(
    fallback_to_bigquery=True,
    bigquery_project="my-project",
)

async with GDELTClient(settings=settings) as client:
    # Automatically uses BigQuery if files unavailable
    events = await client.events.query(event_filter)
```

## Best Practices

- Use streaming for >1000 events
- Enable deduplication for cleaner data
- Use specific filters to reduce data volume
- Handle empty results gracefully
- Set appropriate date ranges (files available for 2015+)

# Global Knowledge Graph (GKG)

The GKG endpoint provides rich semantic metadata extracted from news articles.

## Overview

GKG records contain:

- Themes (ENV_CLIMATE, ECON_STOCKMARKET, etc.)
- Entities (people, organizations, locations)
- Quotations
- Tone/sentiment
- Source metadata

## Basic Usage

```
from py_gdelt import GDELTClient
from py_gdelt.filters import DateRange, GKGFilter
from datetime import date

async with GDELTClient() as client:
    gkg_filter = GKGFilter(
        date_range=DateRange(start=date(2024, 1, 1)),
        themes=["ENV_CLIMATECHANGE"],
    )

    records = await client.gkg.query(gkg_filter)
```

For details, see [GKG example](https://rbozydar.github.io/py-gdelt/examples/basic/index.md).

# NGrams

Query word and phrase occurrences from GDELT NGrams 3.0.

## Overview

NGrams track word/phrase positions within articles for linguistic analysis.

## Basic Usage

```
from py_gdelt import GDELTClient
from py_gdelt.filters import DateRange, NGramsFilter
from datetime import date

async with GDELTClient() as client:
    ngrams_filter = NGramsFilter(
        date_range=DateRange(start=date(2024, 1, 1)),
        ngram="climate",
        language="en",
    )

    records = await client.ngrams.query(ngrams_filter)
```

For details, see [NGrams example](https://rbozydar.github.io/py-gdelt/examples/basic/index.md).

# REST APIs

GDELT provides several REST APIs for searching and analyzing global news.

## DOC 2.0 API - Article Search

Search for news articles:

```
from py_gdelt.filters import DocFilter

async with GDELTClient() as client:
    doc_filter = DocFilter(
        query="climate change",
        timespan="24h",
        max_results=100,
        sort_by="relevance",
    )

    articles = await client.doc.query(doc_filter)

    for article in articles:
        print(f"{article.title}")
        print(f"  {article.url}")
```

### Timeline Analysis

```
timeline = await client.doc.timeline(
    query="artificial intelligence",
    timespan="7d",
)

for point in timeline.points:
    print(f"{point.date}: {point.count} articles")
```

## GEO 2.0 API - Geographic Search

Find geographic locations mentioned in news:

```
async with GDELTClient() as client:
    result = await client.geo.search(
        "earthquake",
        timespan="7d",
        max_points=50,
    )

    for point in result.points:
        print(f"{point.name}: {point.count} articles")
        print(f"  ({point.lat}, {point.lon})")
```

### Bounding Box Filtering

```
# Europe bounding box
europe_bbox = (35.0, -10.0, 70.0, 40.0)

result = await client.geo.search(
    "protests",
    timespan="7d",
    bounding_box=europe_bbox,
)
```

### GeoJSON Output

```
geojson = await client.geo.to_geojson(
    "climate protest",
    timespan="30d",
)

# Use with folium, leaflet, etc.
```

## Context 2.0 API - Contextual Analysis

Analyze themes, entities, and sentiment:

```
async with GDELTClient() as client:
    result = await client.context.analyze(
        "technology",
        timespan="7d",
    )

    print(f"Articles: {result.article_count}")

    # Top themes
    for theme in result.themes[:10]:
        print(f"  {theme.theme}: {theme.count}")

    # Top entities
    for entity in result.entities[:10]:
        print(f"  {entity.name} ({entity.entity_type}): {entity.count}")

    # Sentiment
    if result.tone:
        print(f"Average tone: {result.tone.average_tone}")
```

### Entity Filtering

```
# Get people mentioned
people = await client.context.get_entities(
    "election",
    entity_type="PERSON",
    limit=20,
)

# Get organizations
orgs = await client.context.get_entities(
    "economy",
    entity_type="ORG",
    limit=20,
)
```

## TV API - Television News

Search TV transcripts:

```
async with GDELTClient() as client:
    clips = await client.tv.search(
        "healthcare",
        timespan="24h",
        station="CNN",
        max_results=20,
    )

    for clip in clips:
        print(f"{clip.station} - {clip.show_name}")
        print(f"  {clip.snippet}")
```

### TV Timeline

```
timeline = await client.tv.timeline(
    "election",
    timespan="7d",
)
```

### Station Comparison

```
chart = await client.tv.station_chart(
    "immigration",
    timespan="7d",
)

for station in chart.stations:
    print(f"{station.station}: {station.count} ({station.percentage}%)")
```

## TVAI API - AI-Enhanced TV Search

Use AI for better TV transcript search:

```
clips = await client.tv_ai.search(
    "impact of artificial intelligence on employment",
    timespan="7d",
    max_results=10,
)
```

## Timespan Options

All REST APIs support these timespans:

- `"15min"` - Last 15 minutes
- `"30min"` - Last 30 minutes
- `"1h"` - Last hour
- `"6h"` - Last 6 hours
- `"24h"` - Last 24 hours
- `"7d"` - Last 7 days
- `"30d"` - Last 30 days

## Rate Limiting

GDELT APIs may rate limit. Handle gracefully:

```
from py_gdelt.exceptions import APIError

try:
    result = await client.doc.query(doc_filter)
except APIError as e:
    if "rate limit" in str(e).lower():
        # Wait and retry
        await asyncio.sleep(60)
```

## Best Practices

- Use appropriate timespans (shorter = faster)
- Limit result counts to what you need
- Handle empty results gracefully
- Respect rate limits
- Cache results when appropriate

# Lookup Tables

GDELT provides lookup tables for codes and classifications.

## Available Lookups

- **CAMEO** - Event codes and descriptions
- **Themes** - Theme taxonomy
- **Countries** - Country code conversions
- **Ethnic/Religious Groups** - Group classifications

## Usage

```
async with GDELTClient() as client:
    # CAMEO codes
    cameo = client.lookups.cameo
    event = cameo.get("14")  # PROTEST

    # Country conversions
    countries = client.lookups.countries
    iso3 = countries.fips_to_iso3("US")  # USA
```

For details, see [examples](https://rbozydar.github.io/py-gdelt/examples/basic/index.md).

# Streaming Large Datasets

Memory-efficient data processing with streaming.

## Why Stream?

Loading large datasets into memory can exhaust resources. Streaming processes data incrementally:

- **Memory Efficient**: Process millions of records without loading all at once
- **Faster Start**: Begin processing immediately without waiting for complete download
- **Scalable**: Handle datasets of any size
- **Interruptible**: Stop early if you find what you need

## Basic Streaming

```
from py_gdelt import GDELTClient
from py_gdelt.filters import DateRange, EventFilter
from datetime import date, timedelta

async with GDELTClient() as client:
    event_filter = EventFilter(
        date_range=DateRange(
            start=date(2024, 1, 1),
            end=date(2024, 1, 7),
        ),
    )

    # Stream instead of query()
    async for event in client.events.stream(event_filter):
        process(event)  # Process one at a time
```

## Filtering While Streaming

Apply additional filters during streaming:

```
us_protest_count = 0

async for event in client.events.stream(event_filter):
    # Filter in-stream
    if event.event_code == "14":  # Protest
        if hasattr(event, 'actor1') and event.actor1:
            if hasattr(event.actor1, 'country_code') and event.actor1.country_code == 'US':
                us_protest_count += 1
```

## Early Exit

Stop processing when you have enough data:

```
count = 0
async for event in client.events.stream(event_filter):
    count += 1
    if count >= 1000:
        break  # Stop after 1000 events
```

## Batching

Process in batches for efficiency:

```
batch_size = 100
batch = []

async for event in client.events.stream(event_filter):
    batch.append(event)

    if len(batch) >= batch_size:
        process_batch(batch)
        batch = []

# Process remaining
if batch:
    process_batch(batch)
```

## GKG Streaming

Stream GKG records similarly:

```
from py_gdelt.filters import GKGFilter

gkg_filter = GKGFilter(
    date_range=DateRange(start=date(2024, 1, 1)),
    themes=["ENV_CLIMATECHANGE"],
)

async for record in client.gkg.stream(gkg_filter):
    # Process GKG record
    for theme in record.themes:
        print(theme.name)
```

## NGrams Streaming

Stream word/phrase occurrences:

```
from py_gdelt.filters import NGramsFilter

ngrams_filter = NGramsFilter(
    date_range=DateRange(start=date(2024, 1, 1)),
    ngram="climate",
    language="en",
)

async for ngram in client.ngrams.stream(ngrams_filter):
    print(f"{ngram.context}")
```

## Memory Monitoring

Track memory usage during streaming:

```
import psutil
import os

process = psutil.Process(os.getpid())

count = 0
async for event in client.events.stream(event_filter):
    count += 1

    if count % 10000 == 0:
        mem_mb = process.memory_info().rss / 1024 / 1024
        print(f"Processed {count} events, Memory: {mem_mb:.2f} MB")
```

## Error Handling

Handle errors gracefully during streaming:

```
from py_gdelt.exceptions import DataError

try:
    async for event in client.events.stream(event_filter):
        try:
            process(event)
        except Exception as e:
            # Log and continue
            logger.error(f"Error processing event: {e}")
            continue
except DataError as e:
    logger.error(f"Data stream error: {e}")
```

## Comparison: Query vs Stream

```
# query() - Loads all into memory
result = await client.events.query(event_filter)
for event in result:
    process(event)
# Memory: ~500MB for 100k events

# stream() - Process incrementally
async for event in client.events.stream(event_filter):
    process(event)
# Memory: ~50MB constant
```

## Best Practices

- Use `stream()` for >1000 records
- Use `query()` for \<1000 records (simpler)
- Batch processing for efficiency
- Handle errors per-record, not per-stream
- Monitor memory in production
- Use early exit to save resources
- Apply filters early to reduce data volume

# Deduplication

GDELT data often contains duplicates. Use deduplication strategies to clean data.

## Strategies

- `URL_ONLY` - Deduplicate by source URL
- `URL_DATE` - By URL and date
- `URL_DATE_LOCATION` - By URL, date, and location
- `ACTOR_PAIR` - By actor pair
- `FULL` - By all fields

## Usage

```
from py_gdelt.utils.dedup import DedupeStrategy

result = await client.events.query(
    event_filter,
    deduplicate=True,
    dedupe_strategy=DedupeStrategy.URL_DATE_LOCATION,
)
```

For details, see [Events guide](https://rbozydar.github.io/py-gdelt/user-guide/events/index.md).

# Error Handling

Proper error handling for robust applications.

## Exception Hierarchy

- `GDELTError` - Base exception
- `APIError` - API-related errors
- `DataError` - Data parsing errors
- `SecurityError` - Security violations
- `ConfigurationError` - Configuration issues

## Usage

```
from py_gdelt.exceptions import APIError, DataError

try:
    result = await client.doc.query(doc_filter)
except APIError as e:
    # Handle API errors (rate limiting, network, etc.)
    logger.error(f"API error: {e}")
except DataError as e:
    # Handle data parsing errors
    logger.error(f"Data error: {e}")
except Exception as e:
    # Handle unexpected errors
    logger.error(f"Unexpected error: {e}")
```

For details, see [API reference](https://rbozydar.github.io/py-gdelt/api/exceptions/index.md).
# API Reference

# Client API

## `GDELTClient`

Main client for accessing all GDELT data sources.

This is the primary entry point for the py-gdelt library. It manages the lifecycle of all dependencies (HTTP client, file source, BigQuery source) and provides convenient namespace access to all endpoints.

The client can be used as either an async or sync context manager, and supports dependency injection for testing.

Parameters:

| Name          | Type            | Description | Default                                                                                                                                                                                    |
| ------------- | --------------- | ----------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `settings`    | \`GDELTSettings | None\`      | Optional GDELTSettings instance. If None, creates default settings.                                                                                                                        |
| `config_path` | \`Path          | None\`      | Optional path to TOML configuration file. Only used if settings is None. If both are provided, settings takes precedence.                                                                  |
| `http_client` | \`AsyncClient   | None\`      | Optional shared HTTP client for testing. If None, client creates and owns its own HTTP client. If provided, the lifecycle is managed externally and the client will not be closed on exit. |

Example

> > > async with GDELTClient() as client: ... events = await client.events.query(filter_obj) ... articles = await client.doc.search("climate") ... theme = client.lookups.themes.get_category("ENV_CLIMATECHANGE")
> > >
> > > ### With config file
> > >
> > > async with GDELTClient(config_path=Path("gdelt.toml")) as client: ... pass
> > >
> > > ### With custom settings
> > >
> > > settings = GDELTSettings(timeout=60, max_retries=5) async with GDELTClient(settings=settings) as client: ... pass
> > >
> > > ### With dependency injection for testing
> > >
> > > async with httpx.AsyncClient() as http_client: ... async with GDELTClient(http_client=http_client) as client: ... pass

Source code in `src/py_gdelt/client.py`

```
class GDELTClient:
    """Main client for accessing all GDELT data sources.

    This is the primary entry point for the py-gdelt library. It manages the
    lifecycle of all dependencies (HTTP client, file source, BigQuery source)
    and provides convenient namespace access to all endpoints.

    The client can be used as either an async or sync context manager, and
    supports dependency injection for testing.

    Args:
        settings: Optional GDELTSettings instance. If None, creates default settings.
        config_path: Optional path to TOML configuration file. Only used if
            settings is None. If both are provided, settings takes precedence.
        http_client: Optional shared HTTP client for testing. If None, client
            creates and owns its own HTTP client. If provided, the lifecycle
            is managed externally and the client will not be closed on exit.

    Example:
        >>> async with GDELTClient() as client:
        ...     events = await client.events.query(filter_obj)
        ...     articles = await client.doc.search("climate")
        ...     theme = client.lookups.themes.get_category("ENV_CLIMATECHANGE")

        >>> # With config file
        >>> async with GDELTClient(config_path=Path("gdelt.toml")) as client:
        ...     pass

        >>> # With custom settings
        >>> settings = GDELTSettings(timeout=60, max_retries=5)
        >>> async with GDELTClient(settings=settings) as client:
        ...     pass

        >>> # With dependency injection for testing
        >>> async with httpx.AsyncClient() as http_client:
        ...     async with GDELTClient(http_client=http_client) as client:
        ...         pass
    """

    def __init__(
        self,
        settings: GDELTSettings | None = None,
        config_path: Path | None = None,
        http_client: httpx.AsyncClient | None = None,
    ) -> None:
        # Initialize settings
        if settings is not None:
            self.settings = settings
        elif config_path is not None:
            self.settings = GDELTSettings(config_path=config_path)
        else:
            self.settings = GDELTSettings()

        # HTTP client management
        self._http_client = http_client
        self._owns_http_client = http_client is None

        # Source instances (created lazily)
        self._file_source: FileSource | None = None
        self._bigquery_source: BigQuerySource | None = None
        self._owns_sources = True

        # Lifecycle state
        self._initialized = False

    async def _initialize(self) -> None:
        """Initialize sources and HTTP client.

        Called automatically on first use via context manager.
        Creates HTTP client (if not injected) and initializes file source.
        BigQuery source is created only if credentials are configured.
        """
        if self._initialized:
            return

        # Create HTTP client if not injected
        if self._owns_http_client:
            self._http_client = httpx.AsyncClient(
                timeout=httpx.Timeout(
                    connect=10.0,
                    read=self.settings.timeout,
                    write=10.0,
                    pool=5.0,
                ),
                follow_redirects=True,
            )

        # Initialize file source
        self._file_source = FileSource(
            settings=self.settings,
            client=self._http_client,
        )
        await self._file_source.__aenter__()

        # Initialize BigQuery source if credentials are configured
        if self.settings.bigquery_project and self.settings.bigquery_credentials:
            try:
                self._bigquery_source = BigQuerySource(settings=self.settings)
                logger.debug(
                    "Initialized BigQuerySource with project %s",
                    self.settings.bigquery_project,
                )
            except ImportError as e:
                # google-cloud-bigquery package not installed
                logger.warning(
                    "BigQuery package not installed: %s. "
                    "Install with: pip install py-gdelt[bigquery]",
                    e,
                )
                self._bigquery_source = None
            except (OSError, FileNotFoundError) as e:
                # Credentials file not found or not readable
                logger.warning(
                    "BigQuery credentials file error: %s. BigQuery fallback will be unavailable.",
                    e,
                )
                self._bigquery_source = None
            except Exception as e:  # noqa: BLE001
                # Catch all Google SDK errors without importing optional dependency
                # This is an error boundary - BigQuery is optional, errors should not crash
                logger.warning(
                    "Failed to initialize BigQuerySource (%s): %s. "
                    "BigQuery fallback will be unavailable.",
                    type(e).__name__,
                    e,
                )
                self._bigquery_source = None

        self._initialized = True
        logger.debug("GDELTClient initialized successfully")

    async def _cleanup(self) -> None:
        """Clean up resources.

        Closes file source, BigQuery source (if created), and HTTP client (if owned).
        """
        if not self._initialized:
            return

        # Close file source
        if self._file_source is not None:
            await self._file_source.__aexit__(None, None, None)
            self._file_source = None

        # BigQuery source doesn't need explicit cleanup (no persistent connections)
        self._bigquery_source = None

        # Close HTTP client if we own it
        if self._owns_http_client and self._http_client is not None:
            await self._http_client.aclose()
            self._http_client = None

        self._initialized = False
        logger.debug("GDELTClient cleaned up successfully")

    async def __aenter__(self) -> GDELTClient:
        """Async context manager entry.

        Returns:
            Self for use in async with statement.

        Example:
            >>> async with GDELTClient() as client:
            ...     events = await client.events.query(filter_obj)
        """
        await self._initialize()
        return self

    async def __aexit__(self, *args: Any) -> None:
        """Async context manager exit.

        Cleans up all owned resources.

        Args:
            *args: Exception info (unused, but required by protocol).
        """
        await self._cleanup()

    def __enter__(self) -> GDELTClient:
        """Sync context manager entry.

        This provides synchronous (blocking) access to the client for use in
        non-async code. It uses asyncio.run() internally to manage the event loop.

        Important Limitations:
            - MUST be called from outside any existing async context/event loop.
              Calling from within an async function will raise RuntimeError.
            - Creates a new event loop for each context manager entry.
            - Use the async context manager (async with) when possible for
              better performance and compatibility.

        Returns:
            Self for use in with statement.

        Raises:
            RuntimeError: If called from within an already running event loop.

        Example:
            >>> # Correct: Used from synchronous code
            >>> with GDELTClient() as client:
            ...     events = client.events.query_sync(filter_obj)
            ...
            >>> # Wrong: Don't use from async code - use 'async with' instead
            >>> async def bad_example():
            ...     with GDELTClient() as client:  # RuntimeError!
            ...         pass
        """
        asyncio.run(self._initialize())
        return self

    def __exit__(self, *args: Any) -> None:
        """Sync context manager exit.

        Cleans up all owned resources. Uses asyncio.run() internally.

        Args:
            *args: Exception info (unused, but required by protocol).

        Raises:
            RuntimeError: If called from within an already running event loop.
        """
        asyncio.run(self._cleanup())

    # Endpoint namespaces (lazy initialization via cached_property)

    @cached_property
    def events(self) -> EventsEndpoint:
        """Access the Events endpoint.

        Provides methods for querying GDELT Events data from files or BigQuery.

        Returns:
            EventsEndpoint instance.

        Raises:
            RuntimeError: If client not initialized (use context manager).

        Example:
            >>> async with GDELTClient() as client:
            ...     filter_obj = EventFilter(date_range=DateRange(start=date(2024, 1, 1)))
            ...     events = await client.events.query(filter_obj)
        """
        if self._file_source is None:
            msg = "GDELTClient not initialized. Use 'async with GDELTClient() as client:'"
            raise RuntimeError(msg)
        return EventsEndpoint(
            file_source=self._file_source,
            bigquery_source=self._bigquery_source,
            fallback_enabled=self.settings.fallback_to_bigquery,
        )

    @cached_property
    def mentions(self) -> MentionsEndpoint:
        """Access the Mentions endpoint.

        Provides methods for querying GDELT Mentions data from files or BigQuery.

        Returns:
            MentionsEndpoint instance.

        Raises:
            RuntimeError: If client not initialized (use context manager).

        Example:
            >>> async with GDELTClient() as client:
            ...     filter_obj = EventFilter(date_range=DateRange(start=date(2024, 1, 1)))
            ...     mentions = await client.mentions.query("123456789", filter_obj)
        """
        if self._file_source is None:
            msg = "GDELTClient not initialized. Use 'async with GDELTClient() as client:'"
            raise RuntimeError(msg)
        return MentionsEndpoint(
            file_source=self._file_source,
            bigquery_source=self._bigquery_source,
            fallback_enabled=self.settings.fallback_to_bigquery,
        )

    @cached_property
    def gkg(self) -> GKGEndpoint:
        """Access the GKG (Global Knowledge Graph) endpoint.

        Provides methods for querying GDELT GKG data from files or BigQuery.

        Returns:
            GKGEndpoint instance.

        Raises:
            RuntimeError: If client not initialized (use context manager).

        Example:
            >>> async with GDELTClient() as client:
            ...     filter_obj = GKGFilter(
            ...         date_range=DateRange(start=date(2024, 1, 1)),
            ...         themes=["ENV_CLIMATECHANGE"]
            ...     )
            ...     records = await client.gkg.query(filter_obj)
        """
        if self._file_source is None:
            msg = "GDELTClient not initialized. Use 'async with GDELTClient() as client:'"
            raise RuntimeError(msg)
        return GKGEndpoint(
            file_source=self._file_source,
            bigquery_source=self._bigquery_source,
            fallback_enabled=self.settings.fallback_to_bigquery,
        )

    @cached_property
    def ngrams(self) -> NGramsEndpoint:
        """Access the NGrams endpoint.

        Provides methods for querying GDELT NGrams data (files only).

        Returns:
            NGramsEndpoint instance.

        Raises:
            RuntimeError: If client not initialized (use context manager).

        Example:
            >>> async with GDELTClient() as client:
            ...     filter_obj = NGramsFilter(
            ...         date_range=DateRange(start=date(2024, 1, 1)),
            ...         language="en"
            ...     )
            ...     records = await client.ngrams.query(filter_obj)
        """
        if self._file_source is None:
            msg = "GDELTClient not initialized. Use 'async with GDELTClient() as client:'"
            raise RuntimeError(msg)
        return NGramsEndpoint(
            settings=self.settings,
            file_source=self._file_source,
        )

    @cached_property
    def tv_ngrams(self) -> TVNGramsEndpoint:
        """Access the TV NGrams endpoint.

        Provides methods for querying word frequency from TV broadcast closed captions.

        Returns:
            TVNGramsEndpoint instance.

        Raises:
            RuntimeError: If client not initialized (use context manager).

        Example:
            >>> async with GDELTClient() as client:
            ...     filter_obj = BroadcastNGramsFilter(
            ...         date_range=DateRange(start=date(2024, 1, 1)),
            ...         station="CNN"
            ...     )
            ...     records = await client.tv_ngrams.query(filter_obj)
        """
        if self._file_source is None:
            msg = "GDELTClient not initialized. Use 'async with GDELTClient() as client:'"
            raise RuntimeError(msg)
        return TVNGramsEndpoint(
            settings=self.settings,
            file_source=self._file_source,
        )

    @cached_property
    def radio_ngrams(self) -> RadioNGramsEndpoint:
        """Access the Radio NGrams endpoint.

        Provides methods for querying word frequency from radio broadcast transcripts.

        Returns:
            RadioNGramsEndpoint instance.

        Raises:
            RuntimeError: If client not initialized (use context manager).

        Example:
            >>> async with GDELTClient() as client:
            ...     filter_obj = BroadcastNGramsFilter(
            ...         date_range=DateRange(start=date(2024, 1, 1)),
            ...         station="NPR"
            ...     )
            ...     records = await client.radio_ngrams.query(filter_obj)
        """
        if self._file_source is None:
            msg = "GDELTClient not initialized. Use 'async with GDELTClient() as client:'"
            raise RuntimeError(msg)
        return RadioNGramsEndpoint(
            settings=self.settings,
            file_source=self._file_source,
        )

    @cached_property
    def vgkg(self) -> VGKGEndpoint:
        """Access the VGKG (Visual Global Knowledge Graph) endpoint.

        Provides methods for querying Google Cloud Vision API analysis of news images.

        Returns:
            VGKGEndpoint instance.

        Raises:
            RuntimeError: If client not initialized (use context manager).

        Example:
            >>> async with GDELTClient() as client:
            ...     filter_obj = VGKGFilter(
            ...         date_range=DateRange(start=date(2024, 1, 1)),
            ...         domain="cnn.com"
            ...     )
            ...     records = await client.vgkg.query(filter_obj)
        """
        if self._file_source is None:
            msg = "GDELTClient not initialized. Use 'async with GDELTClient() as client:'"
            raise RuntimeError(msg)
        return VGKGEndpoint(
            settings=self.settings,
            file_source=self._file_source,
        )

    @cached_property
    def tv_gkg(self) -> TVGKGEndpoint:
        """Access the TV GKG (TV Global Knowledge Graph) endpoint.

        Provides methods for querying GKG data from TV broadcast closed captions.

        Returns:
            TVGKGEndpoint instance.

        Raises:
            RuntimeError: If client not initialized (use context manager).

        Example:
            >>> async with GDELTClient() as client:
            ...     filter_obj = TVGKGFilter(
            ...         date_range=DateRange(start=date(2024, 1, 1)),
            ...         station="CNN"
            ...     )
            ...     records = await client.tv_gkg.query(filter_obj)
        """
        if self._file_source is None:
            msg = "GDELTClient not initialized. Use 'async with GDELTClient() as client:'"
            raise RuntimeError(msg)
        return TVGKGEndpoint(
            settings=self.settings,
            file_source=self._file_source,
        )

    @cached_property
    def graphs(self) -> GraphEndpoint:
        """Access the Graph datasets endpoint.

        Provides methods for querying GDELT Graph datasets (GQG, GEG, GFG, GGG, GEMG, GAL)
        from file downloads.

        Returns:
            GraphEndpoint instance.

        Raises:
            RuntimeError: If client not initialized (use context manager).

        Example:
            >>> async with GDELTClient() as client:
            ...     from py_gdelt.filters import GQGFilter, DateRange
            ...     filter_obj = GQGFilter(
            ...         date_range=DateRange(start=date(2025, 1, 20))
            ...     )
            ...     result = await client.graphs.query_gqg(filter_obj)
            ...     for record in result:
            ...         print(record.quotes)
        """
        if self._file_source is None:
            msg = "GDELTClient not initialized. Use 'async with GDELTClient() as client:'"
            raise RuntimeError(msg)
        return GraphEndpoint(
            file_source=self._file_source,
        )

    @cached_property
    def doc(self) -> DocEndpoint:
        """Access the DOC 2.0 API endpoint.

        Provides methods for searching GDELT articles via the DOC API.

        Returns:
            DocEndpoint instance.

        Raises:
            RuntimeError: If client not initialized (use context manager).

        Example:
            >>> async with GDELTClient() as client:
            ...     articles = await client.doc.search("climate change", max_results=100)
        """
        if self._http_client is None:
            msg = "GDELTClient not initialized. Use 'async with GDELTClient() as client:'"
            raise RuntimeError(msg)
        return DocEndpoint(
            settings=self.settings,
            client=self._http_client,
        )

    @cached_property
    def geo(self) -> GeoEndpoint:
        """Access the GEO 2.0 API endpoint.

        Provides methods for querying geographic locations from news articles.

        Returns:
            GeoEndpoint instance.

        Raises:
            RuntimeError: If client not initialized (use context manager).

        Example:
            >>> async with GDELTClient() as client:
            ...     result = await client.geo.search("earthquake", max_points=100)
        """
        if self._http_client is None:
            msg = "GDELTClient not initialized. Use 'async with GDELTClient() as client:'"
            raise RuntimeError(msg)
        return GeoEndpoint(
            settings=self.settings,
            client=self._http_client,
        )

    @cached_property
    def context(self) -> ContextEndpoint:
        """Access the Context 2.0 API endpoint.

        Provides methods for contextual analysis of search terms.

        Returns:
            ContextEndpoint instance.

        Raises:
            RuntimeError: If client not initialized (use context manager).

        Example:
            >>> async with GDELTClient() as client:
            ...     result = await client.context.analyze("climate change")
        """
        if self._http_client is None:
            msg = "GDELTClient not initialized. Use 'async with GDELTClient() as client:'"
            raise RuntimeError(msg)
        return ContextEndpoint(
            settings=self.settings,
            client=self._http_client,
        )

    @cached_property
    def tv(self) -> TVEndpoint:
        """Access the TV API endpoint.

        Provides methods for querying television news transcripts.

        Returns:
            TVEndpoint instance.

        Raises:
            RuntimeError: If client not initialized (use context manager).

        Example:
            >>> async with GDELTClient() as client:
            ...     clips = await client.tv.search("climate change", station="CNN")
        """
        if self._http_client is None:
            msg = "GDELTClient not initialized. Use 'async with GDELTClient() as client:'"
            raise RuntimeError(msg)
        return TVEndpoint(
            settings=self.settings,
            client=self._http_client,
        )

    @cached_property
    def tv_ai(self) -> TVAIEndpoint:
        """Access the TVAI API endpoint.

        Provides methods for AI-enhanced television news analysis.

        Returns:
            TVAIEndpoint instance.

        Raises:
            RuntimeError: If client not initialized (use context manager).

        Example:
            >>> async with GDELTClient() as client:
            ...     result = await client.tv_ai.analyze("election coverage")
        """
        if self._http_client is None:
            msg = "GDELTClient not initialized. Use 'async with GDELTClient() as client:'"
            raise RuntimeError(msg)
        return TVAIEndpoint(
            settings=self.settings,
            client=self._http_client,
        )

    @cached_property
    def lowerthird(self) -> LowerThirdEndpoint:
        """Access the LowerThird (Chyron) API.

        Provides methods for searching OCR'd TV chyrons (lower-third text overlays).

        Returns:
            LowerThirdEndpoint for searching TV chyrons.

        Raises:
            RuntimeError: If client not initialized (use context manager).

        Example:
            >>> async with GDELTClient() as client:
            ...     clips = await client.lowerthird.search("breaking news")
        """
        if self._http_client is None:
            msg = "GDELTClient not initialized. Use 'async with GDELTClient() as client:'"
            raise RuntimeError(msg)
        return LowerThirdEndpoint(
            settings=self.settings,
            client=self._http_client,
        )

    @cached_property
    def tvv(self) -> TVVEndpoint:
        """Access the TV Visual (TVV) API for channel inventory.

        Provides methods for retrieving TV channel metadata.

        Returns:
            TVVEndpoint for channel metadata.

        Raises:
            RuntimeError: If client not initialized (use context manager).

        Example:
            >>> async with GDELTClient() as client:
            ...     channels = await client.tvv.get_inventory()
        """
        if self._http_client is None:
            msg = "GDELTClient not initialized. Use 'async with GDELTClient() as client:'"
            raise RuntimeError(msg)
        return TVVEndpoint(
            settings=self.settings,
            client=self._http_client,
        )

    @cached_property
    def gkg_geojson(self) -> GKGGeoJSONEndpoint:
        """Access the GKG GeoJSON API (v1.0 Legacy).

        Provides methods for querying geographic GKG data as GeoJSON.

        Returns:
            GKGGeoJSONEndpoint for geographic GKG queries.

        Raises:
            RuntimeError: If client not initialized (use context manager).

        Example:
            >>> async with GDELTClient() as client:
            ...     result = await client.gkg_geojson.search("TERROR", timespan=60)
        """
        if self._http_client is None:
            msg = "GDELTClient not initialized. Use 'async with GDELTClient() as client:'"
            raise RuntimeError(msg)
        return GKGGeoJSONEndpoint(
            settings=self.settings,
            client=self._http_client,
        )

    @cached_property
    def lookups(self) -> Lookups:
        """Access lookup tables for CAMEO codes, themes, and countries.

        Provides access to all GDELT lookup tables with lazy loading.

        Returns:
            Lookups instance for code/theme/country lookups.

        Example:
            >>> async with GDELTClient() as client:
            ...     # CAMEO codes
            ...     event_entry = client.lookups.cameo["14"]
            ...     event_name = event_entry.name  # "PROTEST"
            ...
            ...     # GKG themes
            ...     category = client.lookups.themes.get_category("ENV_CLIMATECHANGE")
            ...
            ...     # Country codes
            ...     iso_code = client.lookups.countries.fips_to_iso3("US")  # "USA"
        """
        return Lookups()
```

### `events`

Access the Events endpoint.

Provides methods for querying GDELT Events data from files or BigQuery.

Returns:

| Type             | Description              |
| ---------------- | ------------------------ |
| `EventsEndpoint` | EventsEndpoint instance. |

Raises:

| Type           | Description                                      |
| -------------- | ------------------------------------------------ |
| `RuntimeError` | If client not initialized (use context manager). |

Example

> > > async with GDELTClient() as client: ... filter_obj = EventFilter(date_range=DateRange(start=date(2024, 1, 1))) ... events = await client.events.query(filter_obj)

### `mentions`

Access the Mentions endpoint.

Provides methods for querying GDELT Mentions data from files or BigQuery.

Returns:

| Type               | Description                |
| ------------------ | -------------------------- |
| `MentionsEndpoint` | MentionsEndpoint instance. |

Raises:

| Type           | Description                                      |
| -------------- | ------------------------------------------------ |
| `RuntimeError` | If client not initialized (use context manager). |

Example

> > > async with GDELTClient() as client: ... filter_obj = EventFilter(date_range=DateRange(start=date(2024, 1, 1))) ... mentions = await client.mentions.query("123456789", filter_obj)

### `gkg`

Access the GKG (Global Knowledge Graph) endpoint.

Provides methods for querying GDELT GKG data from files or BigQuery.

Returns:

| Type          | Description           |
| ------------- | --------------------- |
| `GKGEndpoint` | GKGEndpoint instance. |

Raises:

| Type           | Description                                      |
| -------------- | ------------------------------------------------ |
| `RuntimeError` | If client not initialized (use context manager). |

Example

> > > async with GDELTClient() as client: ... filter_obj = GKGFilter( ... date_range=DateRange(start=date(2024, 1, 1)), ... themes=["ENV_CLIMATECHANGE"] ... ) ... records = await client.gkg.query(filter_obj)

### `ngrams`

Access the NGrams endpoint.

Provides methods for querying GDELT NGrams data (files only).

Returns:

| Type             | Description              |
| ---------------- | ------------------------ |
| `NGramsEndpoint` | NGramsEndpoint instance. |

Raises:

| Type           | Description                                      |
| -------------- | ------------------------------------------------ |
| `RuntimeError` | If client not initialized (use context manager). |

Example

> > > async with GDELTClient() as client: ... filter_obj = NGramsFilter( ... date_range=DateRange(start=date(2024, 1, 1)), ... language="en" ... ) ... records = await client.ngrams.query(filter_obj)

### `tv_ngrams`

Access the TV NGrams endpoint.

Provides methods for querying word frequency from TV broadcast closed captions.

Returns:

| Type               | Description                |
| ------------------ | -------------------------- |
| `TVNGramsEndpoint` | TVNGramsEndpoint instance. |

Raises:

| Type           | Description                                      |
| -------------- | ------------------------------------------------ |
| `RuntimeError` | If client not initialized (use context manager). |

Example

> > > async with GDELTClient() as client: ... filter_obj = BroadcastNGramsFilter( ... date_range=DateRange(start=date(2024, 1, 1)), ... station="CNN" ... ) ... records = await client.tv_ngrams.query(filter_obj)

### `radio_ngrams`

Access the Radio NGrams endpoint.

Provides methods for querying word frequency from radio broadcast transcripts.

Returns:

| Type                  | Description                   |
| --------------------- | ----------------------------- |
| `RadioNGramsEndpoint` | RadioNGramsEndpoint instance. |

Raises:

| Type           | Description                                      |
| -------------- | ------------------------------------------------ |
| `RuntimeError` | If client not initialized (use context manager). |

Example

> > > async with GDELTClient() as client: ... filter_obj = BroadcastNGramsFilter( ... date_range=DateRange(start=date(2024, 1, 1)), ... station="NPR" ... ) ... records = await client.radio_ngrams.query(filter_obj)

### `vgkg`

Access the VGKG (Visual Global Knowledge Graph) endpoint.

Provides methods for querying Google Cloud Vision API analysis of news images.

Returns:

| Type           | Description            |
| -------------- | ---------------------- |
| `VGKGEndpoint` | VGKGEndpoint instance. |

Raises:

| Type           | Description                                      |
| -------------- | ------------------------------------------------ |
| `RuntimeError` | If client not initialized (use context manager). |

Example

> > > async with GDELTClient() as client: ... filter_obj = VGKGFilter( ... date_range=DateRange(start=date(2024, 1, 1)), ... domain="cnn.com" ... ) ... records = await client.vgkg.query(filter_obj)

### `tv_gkg`

Access the TV GKG (TV Global Knowledge Graph) endpoint.

Provides methods for querying GKG data from TV broadcast closed captions.

Returns:

| Type            | Description             |
| --------------- | ----------------------- |
| `TVGKGEndpoint` | TVGKGEndpoint instance. |

Raises:

| Type           | Description                                      |
| -------------- | ------------------------------------------------ |
| `RuntimeError` | If client not initialized (use context manager). |

Example

> > > async with GDELTClient() as client: ... filter_obj = TVGKGFilter( ... date_range=DateRange(start=date(2024, 1, 1)), ... station="CNN" ... ) ... records = await client.tv_gkg.query(filter_obj)

### `graphs`

Access the Graph datasets endpoint.

Provides methods for querying GDELT Graph datasets (GQG, GEG, GFG, GGG, GEMG, GAL) from file downloads.

Returns:

| Type            | Description             |
| --------------- | ----------------------- |
| `GraphEndpoint` | GraphEndpoint instance. |

Raises:

| Type           | Description                                      |
| -------------- | ------------------------------------------------ |
| `RuntimeError` | If client not initialized (use context manager). |

Example

> > > async with GDELTClient() as client: ... from py_gdelt.filters import GQGFilter, DateRange ... filter_obj = GQGFilter( ... date_range=DateRange(start=date(2025, 1, 20)) ... ) ... result = await client.graphs.query_gqg(filter_obj) ... for record in result: ... print(record.quotes)

### `doc`

Access the DOC 2.0 API endpoint.

Provides methods for searching GDELT articles via the DOC API.

Returns:

| Type          | Description           |
| ------------- | --------------------- |
| `DocEndpoint` | DocEndpoint instance. |

Raises:

| Type           | Description                                      |
| -------------- | ------------------------------------------------ |
| `RuntimeError` | If client not initialized (use context manager). |

Example

> > > async with GDELTClient() as client: ... articles = await client.doc.search("climate change", max_results=100)

### `geo`

Access the GEO 2.0 API endpoint.

Provides methods for querying geographic locations from news articles.

Returns:

| Type          | Description           |
| ------------- | --------------------- |
| `GeoEndpoint` | GeoEndpoint instance. |

Raises:

| Type           | Description                                      |
| -------------- | ------------------------------------------------ |
| `RuntimeError` | If client not initialized (use context manager). |

Example

> > > async with GDELTClient() as client: ... result = await client.geo.search("earthquake", max_points=100)

### `context`

Access the Context 2.0 API endpoint.

Provides methods for contextual analysis of search terms.

Returns:

| Type              | Description               |
| ----------------- | ------------------------- |
| `ContextEndpoint` | ContextEndpoint instance. |

Raises:

| Type           | Description                                      |
| -------------- | ------------------------------------------------ |
| `RuntimeError` | If client not initialized (use context manager). |

Example

> > > async with GDELTClient() as client: ... result = await client.context.analyze("climate change")

### `tv`

Access the TV API endpoint.

Provides methods for querying television news transcripts.

Returns:

| Type         | Description          |
| ------------ | -------------------- |
| `TVEndpoint` | TVEndpoint instance. |

Raises:

| Type           | Description                                      |
| -------------- | ------------------------------------------------ |
| `RuntimeError` | If client not initialized (use context manager). |

Example

> > > async with GDELTClient() as client: ... clips = await client.tv.search("climate change", station="CNN")

### `tv_ai`

Access the TVAI API endpoint.

Provides methods for AI-enhanced television news analysis.

Returns:

| Type           | Description            |
| -------------- | ---------------------- |
| `TVAIEndpoint` | TVAIEndpoint instance. |

Raises:

| Type           | Description                                      |
| -------------- | ------------------------------------------------ |
| `RuntimeError` | If client not initialized (use context manager). |

Example

> > > async with GDELTClient() as client: ... result = await client.tv_ai.analyze("election coverage")

### `lowerthird`

Access the LowerThird (Chyron) API.

Provides methods for searching OCR'd TV chyrons (lower-third text overlays).

Returns:

| Type                 | Description                                  |
| -------------------- | -------------------------------------------- |
| `LowerThirdEndpoint` | LowerThirdEndpoint for searching TV chyrons. |

Raises:

| Type           | Description                                      |
| -------------- | ------------------------------------------------ |
| `RuntimeError` | If client not initialized (use context manager). |

Example

> > > async with GDELTClient() as client: ... clips = await client.lowerthird.search("breaking news")

### `tvv`

Access the TV Visual (TVV) API for channel inventory.

Provides methods for retrieving TV channel metadata.

Returns:

| Type          | Description                       |
| ------------- | --------------------------------- |
| `TVVEndpoint` | TVVEndpoint for channel metadata. |

Raises:

| Type           | Description                                      |
| -------------- | ------------------------------------------------ |
| `RuntimeError` | If client not initialized (use context manager). |

Example

> > > async with GDELTClient() as client: ... channels = await client.tvv.get_inventory()

### `gkg_geojson`

Access the GKG GeoJSON API (v1.0 Legacy).

Provides methods for querying geographic GKG data as GeoJSON.

Returns:

| Type                 | Description                                    |
| -------------------- | ---------------------------------------------- |
| `GKGGeoJSONEndpoint` | GKGGeoJSONEndpoint for geographic GKG queries. |

Raises:

| Type           | Description                                      |
| -------------- | ------------------------------------------------ |
| `RuntimeError` | If client not initialized (use context manager). |

Example

> > > async with GDELTClient() as client: ... result = await client.gkg_geojson.search("TERROR", timespan=60)

### `lookups`

Access lookup tables for CAMEO codes, themes, and countries.

Provides access to all GDELT lookup tables with lazy loading.

Returns:

| Type      | Description                                      |
| --------- | ------------------------------------------------ |
| `Lookups` | Lookups instance for code/theme/country lookups. |

Example

> > > async with GDELTClient() as client: ... # CAMEO codes ... event_entry = client.lookups.cameo["14"] ... event_name = event_entry.name # "PROTEST" ... ... # GKG themes ... category = client.lookups.themes.get_category("ENV_CLIMATECHANGE") ... ... # Country codes ... iso_code = client.lookups.countries.fips_to_iso3("US") # "USA"

### `__aenter__()`

Async context manager entry.

Returns:

| Type          | Description                           |
| ------------- | ------------------------------------- |
| `GDELTClient` | Self for use in async with statement. |

Example

> > > async with GDELTClient() as client: ... events = await client.events.query(filter_obj)

Source code in `src/py_gdelt/client.py`

```
async def __aenter__(self) -> GDELTClient:
    """Async context manager entry.

    Returns:
        Self for use in async with statement.

    Example:
        >>> async with GDELTClient() as client:
        ...     events = await client.events.query(filter_obj)
    """
    await self._initialize()
    return self
```

### `__aexit__(*args)`

Async context manager exit.

Cleans up all owned resources.

Parameters:

| Name    | Type  | Description                                        | Default |
| ------- | ----- | -------------------------------------------------- | ------- |
| `*args` | `Any` | Exception info (unused, but required by protocol). | `()`    |

Source code in `src/py_gdelt/client.py`

```
async def __aexit__(self, *args: Any) -> None:
    """Async context manager exit.

    Cleans up all owned resources.

    Args:
        *args: Exception info (unused, but required by protocol).
    """
    await self._cleanup()
```

### `__enter__()`

Sync context manager entry.

This provides synchronous (blocking) access to the client for use in non-async code. It uses asyncio.run() internally to manage the event loop.

Important Limitations

- MUST be called from outside any existing async context/event loop. Calling from within an async function will raise RuntimeError.
- Creates a new event loop for each context manager entry.
- Use the async context manager (async with) when possible for better performance and compatibility.

Returns:

| Type          | Description                     |
| ------------- | ------------------------------- |
| `GDELTClient` | Self for use in with statement. |

Raises:

| Type           | Description                                          |
| -------------- | ---------------------------------------------------- |
| `RuntimeError` | If called from within an already running event loop. |

Example

> > > #### Correct: Used from synchronous code
> > >
> > > with GDELTClient() as client: ... events = client.events.query_sync(filter_obj) ...
> > >
> > > #### Wrong: Don't use from async code - use 'async with' instead
> > >
> > > async def bad_example(): ... with GDELTClient() as client: # RuntimeError! ... pass

Source code in `src/py_gdelt/client.py`

```
def __enter__(self) -> GDELTClient:
    """Sync context manager entry.

    This provides synchronous (blocking) access to the client for use in
    non-async code. It uses asyncio.run() internally to manage the event loop.

    Important Limitations:
        - MUST be called from outside any existing async context/event loop.
          Calling from within an async function will raise RuntimeError.
        - Creates a new event loop for each context manager entry.
        - Use the async context manager (async with) when possible for
          better performance and compatibility.

    Returns:
        Self for use in with statement.

    Raises:
        RuntimeError: If called from within an already running event loop.

    Example:
        >>> # Correct: Used from synchronous code
        >>> with GDELTClient() as client:
        ...     events = client.events.query_sync(filter_obj)
        ...
        >>> # Wrong: Don't use from async code - use 'async with' instead
        >>> async def bad_example():
        ...     with GDELTClient() as client:  # RuntimeError!
        ...         pass
    """
    asyncio.run(self._initialize())
    return self
```

### `__exit__(*args)`

Sync context manager exit.

Cleans up all owned resources. Uses asyncio.run() internally.

Parameters:

| Name    | Type  | Description                                        | Default |
| ------- | ----- | -------------------------------------------------- | ------- |
| `*args` | `Any` | Exception info (unused, but required by protocol). | `()`    |

Raises:

| Type           | Description                                          |
| -------------- | ---------------------------------------------------- |
| `RuntimeError` | If called from within an already running event loop. |

Source code in `src/py_gdelt/client.py`

```
def __exit__(self, *args: Any) -> None:
    """Sync context manager exit.

    Cleans up all owned resources. Uses asyncio.run() internally.

    Args:
        *args: Exception info (unused, but required by protocol).

    Raises:
        RuntimeError: If called from within an already running event loop.
    """
    asyncio.run(self._cleanup())
```

## `GDELTSettings`

Bases: `BaseSettings`

Configuration settings for the GDELT client library.

Settings can be configured via:

- Environment variables with GDELT\_ prefix (e.g., GDELT_TIMEOUT=60)
- TOML configuration file passed to config_path parameter
- Default values

Environment variables take precedence over TOML configuration.

Parameters:

| Name          | Type   | Description                                            | Default                                                                                                                                               |
| ------------- | ------ | ------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
| `config_path` | \`Path | None\`                                                 | Optional path to TOML configuration file. If provided and exists, settings will be loaded from it. Environment variables will override TOML settings. |
| `**kwargs`    | `Any`  | Additional keyword arguments for setting field values. | `{}`                                                                                                                                                  |

Attributes:

| Name                       | Type   | Description                                                    |
| -------------------------- | ------ | -------------------------------------------------------------- |
| `model_config`             |        | Pydantic settings configuration (env prefix, case sensitivity) |
| `bigquery_project`         | \`str  | None\`                                                         |
| `bigquery_credentials`     | \`str  | None\`                                                         |
| `cache_dir`                | `Path` | Directory for caching downloaded GDELT data                    |
| `cache_ttl`                | `int`  | Cache time-to-live in seconds                                  |
| `master_file_list_ttl`     | `int`  | Master file list cache TTL in seconds                          |
| `max_retries`              | `int`  | Maximum number of HTTP request retries                         |
| `timeout`                  | `int`  | HTTP request timeout in seconds                                |
| `max_concurrent_requests`  | `int`  | Maximum concurrent HTTP requests                               |
| `max_concurrent_downloads` | `int`  | Maximum concurrent file downloads                              |
| `fallback_to_bigquery`     | `bool` | Whether to fallback to BigQuery when APIs fail                 |
| `validate_codes`           | `bool` | Whether to validate CAMEO/country codes                        |

Example

> > > ### Using defaults
> > >
> > > settings = GDELTSettings()
> > >
> > > ### Loading from TOML file
> > >
> > > settings = GDELTSettings(config_path=Path("gdelt.toml"))
> > >
> > > ### Environment variables override TOML
> > >
> > > import os os.environ["GDELT_TIMEOUT"] = "60" settings = GDELTSettings() settings.timeout 60

Source code in `src/py_gdelt/config.py`

```
class GDELTSettings(BaseSettings):
    """Configuration settings for the GDELT client library.

    Settings can be configured via:
    - Environment variables with GDELT_ prefix (e.g., GDELT_TIMEOUT=60)
    - TOML configuration file passed to config_path parameter
    - Default values

    Environment variables take precedence over TOML configuration.

    Args:
        config_path: Optional path to TOML configuration file.
            If provided and exists, settings will be loaded from it.
            Environment variables will override TOML settings.
        **kwargs: Additional keyword arguments for setting field values.

    Attributes:
        model_config: Pydantic settings configuration (env prefix, case sensitivity)
        bigquery_project: Google Cloud project ID for BigQuery access
        bigquery_credentials: Path to Google Cloud credentials JSON file
        cache_dir: Directory for caching downloaded GDELT data
        cache_ttl: Cache time-to-live in seconds
        master_file_list_ttl: Master file list cache TTL in seconds
        max_retries: Maximum number of HTTP request retries
        timeout: HTTP request timeout in seconds
        max_concurrent_requests: Maximum concurrent HTTP requests
        max_concurrent_downloads: Maximum concurrent file downloads
        fallback_to_bigquery: Whether to fallback to BigQuery when APIs fail
        validate_codes: Whether to validate CAMEO/country codes

    Example:
        >>> # Using defaults
        >>> settings = GDELTSettings()

        >>> # Loading from TOML file
        >>> settings = GDELTSettings(config_path=Path("gdelt.toml"))

        >>> # Environment variables override TOML
        >>> import os
        >>> os.environ["GDELT_TIMEOUT"] = "60"
        >>> settings = GDELTSettings()
        >>> settings.timeout
        60
    """

    model_config = SettingsConfigDict(
        env_prefix="GDELT_",
        case_sensitive=False,
        extra="ignore",
    )

    # BigQuery settings (optional)
    bigquery_project: str | None = Field(
        default=None,
        description="Google Cloud project ID for BigQuery access",
    )
    bigquery_credentials: str | None = Field(
        default=None,
        description="Path to Google Cloud credentials JSON file",
    )

    # Cache settings
    cache_dir: Path = Field(
        default_factory=lambda: Path.home() / ".cache" / "gdelt",
        description="Directory for caching downloaded GDELT data",
    )
    cache_ttl: int = Field(
        default=3600,
        description="Cache time-to-live in seconds",
    )
    master_file_list_ttl: int = Field(
        default=300,
        description="Master file list cache TTL in seconds (default 5 minutes)",
    )

    # HTTP settings
    max_retries: int = Field(
        default=3,
        description="Maximum number of HTTP request retries",
    )
    timeout: int = Field(
        default=30,
        description="HTTP request timeout in seconds",
    )
    max_concurrent_requests: int = Field(
        default=10,
        description="Maximum concurrent HTTP requests",
    )
    max_concurrent_downloads: int = Field(
        default=10,
        description="Maximum concurrent file downloads",
    )

    # Behavior settings
    fallback_to_bigquery: bool = Field(
        default=True,
        description="Whether to fallback to BigQuery when APIs fail",
    )
    validate_codes: bool = Field(
        default=True,
        description="Whether to validate CAMEO/country codes",
    )

    # Class variable to store config_path during initialization
    _current_config_path: Path | None = None

    def __init__(self, config_path: Path | None = None, **kwargs: Any) -> None:
        # Store config_path temporarily on class for settings_customise_sources
        GDELTSettings._current_config_path = config_path
        try:
            # Initialize the parent BaseSettings
            super().__init__(**kwargs)
        finally:
            # Clean up class variable
            GDELTSettings._current_config_path = None

    @classmethod
    def settings_customise_sources(
        cls,
        settings_cls: type[BaseSettings],
        init_settings: PydanticBaseSettingsSource,
        env_settings: PydanticBaseSettingsSource,
        dotenv_settings: PydanticBaseSettingsSource,  # noqa: ARG003
        file_secret_settings: PydanticBaseSettingsSource,  # noqa: ARG003
        **_kwargs: Any,
    ) -> tuple[PydanticBaseSettingsSource, ...]:
        """Customize settings sources to include TOML configuration.

        The order of sources determines precedence (first source wins):
        1. Init settings (kwargs passed to __init__)
        2. Environment variables (GDELT_ prefix)
        3. TOML configuration file
        4. Default values

        Args:
            settings_cls: The settings class being customized.
            init_settings: Settings from __init__ kwargs.
            env_settings: Settings from environment variables.
            dotenv_settings: Settings from .env file (unused).
            file_secret_settings: Settings from secret files (unused).
            **_kwargs: Additional keyword arguments (unused).

        Returns:
            Tuple of settings sources in priority order.
        """
        # Get config_path from class variable set in __init__
        config_path = cls._current_config_path
        toml_source = TOMLConfigSource(settings_cls, config_path=config_path)

        # Return sources in priority order (first wins)
        return (
            init_settings,  # Highest priority: explicit kwargs
            env_settings,  # Environment variables
            toml_source,  # TOML configuration
            # Default values are handled by Pydantic automatically
        )
```

### `settings_customise_sources(settings_cls, init_settings, env_settings, dotenv_settings, file_secret_settings, **_kwargs)`

Customize settings sources to include TOML configuration.

The order of sources determines precedence (first source wins):

1. Init settings (kwargs passed to **init**)
1. Environment variables (GDELT\_ prefix)
1. TOML configuration file
1. Default values

Parameters:

| Name                   | Type                         | Description                            | Default    |
| ---------------------- | ---------------------------- | -------------------------------------- | ---------- |
| `settings_cls`         | `type[BaseSettings]`         | The settings class being customized.   | *required* |
| `init_settings`        | `PydanticBaseSettingsSource` | Settings from init kwargs.             | *required* |
| `env_settings`         | `PydanticBaseSettingsSource` | Settings from environment variables.   | *required* |
| `dotenv_settings`      | `PydanticBaseSettingsSource` | Settings from .env file (unused).      | *required* |
| `file_secret_settings` | `PydanticBaseSettingsSource` | Settings from secret files (unused).   | *required* |
| `**_kwargs`            | `Any`                        | Additional keyword arguments (unused). | `{}`       |

Returns:

| Type                                     | Description                                  |
| ---------------------------------------- | -------------------------------------------- |
| `tuple[PydanticBaseSettingsSource, ...]` | Tuple of settings sources in priority order. |

Source code in `src/py_gdelt/config.py`

```
@classmethod
def settings_customise_sources(
    cls,
    settings_cls: type[BaseSettings],
    init_settings: PydanticBaseSettingsSource,
    env_settings: PydanticBaseSettingsSource,
    dotenv_settings: PydanticBaseSettingsSource,  # noqa: ARG003
    file_secret_settings: PydanticBaseSettingsSource,  # noqa: ARG003
    **_kwargs: Any,
) -> tuple[PydanticBaseSettingsSource, ...]:
    """Customize settings sources to include TOML configuration.

    The order of sources determines precedence (first source wins):
    1. Init settings (kwargs passed to __init__)
    2. Environment variables (GDELT_ prefix)
    3. TOML configuration file
    4. Default values

    Args:
        settings_cls: The settings class being customized.
        init_settings: Settings from __init__ kwargs.
        env_settings: Settings from environment variables.
        dotenv_settings: Settings from .env file (unused).
        file_secret_settings: Settings from secret files (unused).
        **_kwargs: Additional keyword arguments (unused).

    Returns:
        Tuple of settings sources in priority order.
    """
    # Get config_path from class variable set in __init__
    config_path = cls._current_config_path
    toml_source = TOMLConfigSource(settings_cls, config_path=config_path)

    # Return sources in priority order (first wins)
    return (
        init_settings,  # Highest priority: explicit kwargs
        env_settings,  # Environment variables
        toml_source,  # TOML configuration
        # Default values are handled by Pydantic automatically
    )
```

# Endpoints API

## File-Based Endpoints

### EventsEndpoint

### `EventsEndpoint`

Endpoint for querying GDELT Events data.

This endpoint orchestrates querying GDELT Events data from multiple sources (files and BigQuery) using a DataFetcher. It handles:

- Source selection and fallback logic
- Type conversion from internal \_RawEvent to public Event models
- Optional deduplication
- Both streaming and batch query modes

The endpoint uses dependency injection to receive source instances, making it easy to test and configure.

Parameters:

| Name               | Type             | Description                                               | Default                                               |
| ------------------ | ---------------- | --------------------------------------------------------- | ----------------------------------------------------- |
| `file_source`      | `FileSource`     | FileSource instance for downloading GDELT files           | *required*                                            |
| `bigquery_source`  | \`BigQuerySource | None\`                                                    | Optional BigQuerySource instance for fallback queries |
| `fallback_enabled` | `bool`           | Whether to fallback to BigQuery on errors (default: True) | `True`                                                |

Note

BigQuery fallback only activates if both fallback_enabled=True AND bigquery_source is provided AND credentials are configured.

Example

> > > from py_gdelt.sources import FileSource from py_gdelt.filters import DateRange, EventFilter from datetime import date
> > >
> > > async with FileSource() as file_source: ... endpoint = EventsEndpoint(file_source=file_source) ... filter_obj = EventFilter( ... date_range=DateRange(start=date(2024, 1, 1)), ... actor1_country="USA", ... ) ... # Batch query ... result = await endpoint.query(filter_obj, deduplicate=True) ... for event in result: ... print(event.global_event_id) ... # Streaming query ... async for event in endpoint.stream(filter_obj): ... process(event)

Source code in `src/py_gdelt/endpoints/events.py`

```
class EventsEndpoint:
    """Endpoint for querying GDELT Events data.

    This endpoint orchestrates querying GDELT Events data from multiple sources
    (files and BigQuery) using a DataFetcher. It handles:
    - Source selection and fallback logic
    - Type conversion from internal _RawEvent to public Event models
    - Optional deduplication
    - Both streaming and batch query modes

    The endpoint uses dependency injection to receive source instances, making
    it easy to test and configure.

    Args:
        file_source: FileSource instance for downloading GDELT files
        bigquery_source: Optional BigQuerySource instance for fallback queries
        fallback_enabled: Whether to fallback to BigQuery on errors (default: True)

    Note:
        BigQuery fallback only activates if both fallback_enabled=True AND
        bigquery_source is provided AND credentials are configured.

    Example:
        >>> from py_gdelt.sources import FileSource
        >>> from py_gdelt.filters import DateRange, EventFilter
        >>> from datetime import date
        >>>
        >>> async with FileSource() as file_source:
        ...     endpoint = EventsEndpoint(file_source=file_source)
        ...     filter_obj = EventFilter(
        ...         date_range=DateRange(start=date(2024, 1, 1)),
        ...         actor1_country="USA",
        ...     )
        ...     # Batch query
        ...     result = await endpoint.query(filter_obj, deduplicate=True)
        ...     for event in result:
        ...         print(event.global_event_id)
        ...     # Streaming query
        ...     async for event in endpoint.stream(filter_obj):
        ...         process(event)
    """

    def __init__(
        self,
        file_source: FileSource,
        bigquery_source: BigQuerySource | None = None,
        *,
        fallback_enabled: bool = True,
    ) -> None:
        # Import DataFetcher here to avoid circular imports
        from py_gdelt.sources.fetcher import DataFetcher

        self._fetcher: DataFetcher = DataFetcher(
            file_source=file_source,
            bigquery_source=bigquery_source,
            fallback_enabled=fallback_enabled,
        )

        logger.debug(
            "EventsEndpoint initialized (fallback_enabled=%s)",
            fallback_enabled,
        )

    async def query(
        self,
        filter_obj: EventFilter,
        *,
        deduplicate: bool = False,
        dedupe_strategy: DedupeStrategy | None = None,
        use_bigquery: bool = False,
    ) -> FetchResult[Event]:
        """Query GDELT Events with automatic fallback.

        This is a batch query method that materializes all results into memory.
        For large datasets, prefer stream() for memory-efficient iteration.

        Files are always tried first (free, no credentials), with automatic fallback
        to BigQuery on rate limit/error if credentials are configured.

        Args:
            filter_obj: Event filter with date range and query parameters
            deduplicate: If True, deduplicate events based on dedupe_strategy
            dedupe_strategy: Deduplication strategy (default: URL_DATE_LOCATION)
            use_bigquery: If True, skip files and use BigQuery directly

        Returns:
            FetchResult containing Event instances. Use .data to access the list,
            .failed to see any failed requests, and .complete to check if all
            requests succeeded.

        Raises:
            RateLimitError: If rate limited and fallback not available
            APIError: If download fails and fallback not available
            ConfigurationError: If BigQuery requested but not configured

        Example:
            >>> filter_obj = EventFilter(
            ...     date_range=DateRange(start=date(2024, 1, 1)),
            ...     actor1_country="USA",
            ... )
            >>> result = await endpoint.query(filter_obj, deduplicate=True)
            >>> print(f"Found {len(result)} unique events")
            >>> for event in result:
            ...     print(event.global_event_id)
        """
        # Default dedupe strategy
        if deduplicate and dedupe_strategy is None:
            dedupe_strategy = DedupeStrategy.URL_DATE_LOCATION

        # Fetch raw events first (for deduplication)
        raw_events_list: list[_RawEvent] = [
            raw_event
            async for raw_event in self._fetcher.fetch_events(
                filter_obj,
                use_bigquery=use_bigquery,
            )
        ]

        logger.info("Fetched %d raw events from sources", len(raw_events_list))

        # Apply deduplication on raw events if requested
        # Deduplication happens on _RawEvent which implements HasDedupeFields protocol
        if deduplicate and dedupe_strategy is not None:
            original_count = len(raw_events_list)
            # Convert to iterator, deduplicate, then back to list
            raw_events_list = list(apply_dedup(iter(raw_events_list), dedupe_strategy))
            logger.info(
                "Deduplicated %d events to %d unique (strategy=%s)",
                original_count,
                len(raw_events_list),
                dedupe_strategy,
            )

        # Convert _RawEvent to Event models after deduplication
        events: list[Event] = []
        for raw_event in raw_events_list:
            event = Event.from_raw(raw_event)
            events.append(event)

        logger.info("Converted %d events to Event models", len(events))

        # Return as FetchResult (no failed requests tracked yet)
        return FetchResult(data=events)

    async def stream(
        self,
        filter_obj: EventFilter,
        *,
        deduplicate: bool = False,
        dedupe_strategy: DedupeStrategy | None = None,
        use_bigquery: bool = False,
    ) -> AsyncIterator[Event]:
        """Stream GDELT Events with memory-efficient iteration.

        This is a streaming method that yields events one at a time, making it
        suitable for large datasets. Memory usage is constant regardless of
        result size.

        Files are always tried first (free, no credentials), with automatic fallback
        to BigQuery on rate limit/error if credentials are configured.

        Args:
            filter_obj: Event filter with date range and query parameters
            deduplicate: If True, deduplicate events based on dedupe_strategy
            dedupe_strategy: Deduplication strategy (default: URL_DATE_LOCATION)
            use_bigquery: If True, skip files and use BigQuery directly

        Yields:
            Event: Individual Event instances matching the filter

        Raises:
            RateLimitError: If rate limited and fallback not available
            APIError: If download fails and fallback not available
            ConfigurationError: If BigQuery requested but not configured

        Example:
            >>> filter_obj = EventFilter(
            ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7)),
            ...     actor1_country="USA",
            ... )
            >>> count = 0
            >>> async for event in endpoint.stream(filter_obj, deduplicate=True):
            ...     print(event.global_event_id)
            ...     count += 1
            >>> print(f"Streamed {count} unique events")
        """
        # Default dedupe strategy
        if deduplicate and dedupe_strategy is None:
            dedupe_strategy = DedupeStrategy.URL_DATE_LOCATION

        # Fetch raw events from DataFetcher
        raw_events = self._fetcher.fetch_events(
            filter_obj,
            use_bigquery=use_bigquery,
        )

        # Apply deduplication if requested
        if deduplicate and dedupe_strategy is not None:
            logger.debug("Applying deduplication (strategy=%s)", dedupe_strategy)
            raw_events = apply_dedup_async(raw_events, dedupe_strategy)

        # Convert _RawEvent to Event at yield boundary
        count = 0
        async for raw_event in raw_events:
            event = Event.from_raw(raw_event)
            yield event
            count += 1

        logger.info("Streamed %d events", count)

    def query_sync(
        self,
        filter_obj: EventFilter,
        *,
        deduplicate: bool = False,
        dedupe_strategy: DedupeStrategy | None = None,
        use_bigquery: bool = False,
    ) -> FetchResult[Event]:
        """Synchronous wrapper for query().

        This is a convenience method that runs the async query() method
        in a new event loop. Prefer using the async version when possible.

        Args:
            filter_obj: Event filter with date range and query parameters
            deduplicate: If True, deduplicate events based on dedupe_strategy
            dedupe_strategy: Deduplication strategy (default: URL_DATE_LOCATION)
            use_bigquery: If True, skip files and use BigQuery directly

        Returns:
            FetchResult containing Event instances

        Raises:
            RateLimitError: If rate limited and fallback not available
            APIError: If download fails and fallback not available
            ConfigurationError: If BigQuery requested but not configured
            RuntimeError: If called from within an already running event loop

        Example:
            >>> filter_obj = EventFilter(
            ...     date_range=DateRange(start=date(2024, 1, 1)),
            ...     actor1_country="USA",
            ... )
            >>> result = endpoint.query_sync(filter_obj)
            >>> for event in result:
            ...     print(event.global_event_id)
        """
        return asyncio.run(
            self.query(
                filter_obj,
                deduplicate=deduplicate,
                dedupe_strategy=dedupe_strategy,
                use_bigquery=use_bigquery,
            ),
        )

    def stream_sync(
        self,
        filter_obj: EventFilter,
        *,
        deduplicate: bool = False,
        dedupe_strategy: DedupeStrategy | None = None,
        use_bigquery: bool = False,
    ) -> Iterator[Event]:
        """Synchronous wrapper for stream().

        This method provides a synchronous iterator interface over async streaming.
        It internally manages the event loop and yields events one at a time,
        providing true streaming behavior with memory efficiency.

        Note: This creates a new event loop for each iteration, which has some overhead.
        For better performance, use the async stream() method directly if possible.

        Args:
            filter_obj: Event filter with date range and query parameters
            deduplicate: If True, deduplicate events based on dedupe_strategy
            dedupe_strategy: Deduplication strategy (default: URL_DATE_LOCATION)
            use_bigquery: If True, skip files and use BigQuery directly

        Returns:
            Iterator that yields Event instances for each matching event

        Raises:
            RateLimitError: If rate limited and fallback not available
            APIError: If download fails and fallback not available
            ConfigurationError: If BigQuery requested but not configured
            RuntimeError: If called from within an already running event loop

        Example:
            >>> filter_obj = EventFilter(
            ...     date_range=DateRange(start=date(2024, 1, 1)),
            ...     actor1_country="USA",
            ... )
            >>> for event in endpoint.stream_sync(filter_obj, deduplicate=True):
            ...     print(event.global_event_id)
        """

        async def _async_generator() -> AsyncIterator[Event]:
            """Internal async generator for sync wrapper."""
            async for event in self.stream(
                filter_obj,
                deduplicate=deduplicate,
                dedupe_strategy=dedupe_strategy,
                use_bigquery=use_bigquery,
            ):
                yield event

        # Run async generator and yield results synchronously
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            async_gen = _async_generator()
            while True:
                try:
                    event = loop.run_until_complete(async_gen.__anext__())
                    yield event
                except StopAsyncIteration:
                    break
        finally:
            loop.close()

    async def _build_url(self, **kwargs: Any) -> str:
        """Build URL for events endpoint.

        Note: Events endpoint doesn't use URLs since it fetches from files/BigQuery.
        This method is provided for compatibility with BaseEndpoint pattern but
        is not used in practice.

        Args:
            **kwargs: Unused, but kept for interface consistency.

        Returns:
            Empty string (not used for file/BigQuery sources).
        """
        return ""
```

#### `query(filter_obj, *, deduplicate=False, dedupe_strategy=None, use_bigquery=False)`

Query GDELT Events with automatic fallback.

This is a batch query method that materializes all results into memory. For large datasets, prefer stream() for memory-efficient iteration.

Files are always tried first (free, no credentials), with automatic fallback to BigQuery on rate limit/error if credentials are configured.

Parameters:

| Name              | Type             | Description                                          | Default                                             |
| ----------------- | ---------------- | ---------------------------------------------------- | --------------------------------------------------- |
| `filter_obj`      | `EventFilter`    | Event filter with date range and query parameters    | *required*                                          |
| `deduplicate`     | `bool`           | If True, deduplicate events based on dedupe_strategy | `False`                                             |
| `dedupe_strategy` | \`DedupeStrategy | None\`                                               | Deduplication strategy (default: URL_DATE_LOCATION) |
| `use_bigquery`    | `bool`           | If True, skip files and use BigQuery directly        | `False`                                             |

Returns:

| Type                 | Description                                                           |
| -------------------- | --------------------------------------------------------------------- |
| `FetchResult[Event]` | FetchResult containing Event instances. Use .data to access the list, |
| `FetchResult[Event]` | .failed to see any failed requests, and .complete to check if all     |
| `FetchResult[Event]` | requests succeeded.                                                   |

Raises:

| Type                 | Description                                  |
| -------------------- | -------------------------------------------- |
| `RateLimitError`     | If rate limited and fallback not available   |
| `APIError`           | If download fails and fallback not available |
| `ConfigurationError` | If BigQuery requested but not configured     |

Example

> > > filter_obj = EventFilter( ... date_range=DateRange(start=date(2024, 1, 1)), ... actor1_country="USA", ... ) result = await endpoint.query(filter_obj, deduplicate=True) print(f"Found {len(result)} unique events") for event in result: ... print(event.global_event_id)

Source code in `src/py_gdelt/endpoints/events.py`

```
async def query(
    self,
    filter_obj: EventFilter,
    *,
    deduplicate: bool = False,
    dedupe_strategy: DedupeStrategy | None = None,
    use_bigquery: bool = False,
) -> FetchResult[Event]:
    """Query GDELT Events with automatic fallback.

    This is a batch query method that materializes all results into memory.
    For large datasets, prefer stream() for memory-efficient iteration.

    Files are always tried first (free, no credentials), with automatic fallback
    to BigQuery on rate limit/error if credentials are configured.

    Args:
        filter_obj: Event filter with date range and query parameters
        deduplicate: If True, deduplicate events based on dedupe_strategy
        dedupe_strategy: Deduplication strategy (default: URL_DATE_LOCATION)
        use_bigquery: If True, skip files and use BigQuery directly

    Returns:
        FetchResult containing Event instances. Use .data to access the list,
        .failed to see any failed requests, and .complete to check if all
        requests succeeded.

    Raises:
        RateLimitError: If rate limited and fallback not available
        APIError: If download fails and fallback not available
        ConfigurationError: If BigQuery requested but not configured

    Example:
        >>> filter_obj = EventFilter(
        ...     date_range=DateRange(start=date(2024, 1, 1)),
        ...     actor1_country="USA",
        ... )
        >>> result = await endpoint.query(filter_obj, deduplicate=True)
        >>> print(f"Found {len(result)} unique events")
        >>> for event in result:
        ...     print(event.global_event_id)
    """
    # Default dedupe strategy
    if deduplicate and dedupe_strategy is None:
        dedupe_strategy = DedupeStrategy.URL_DATE_LOCATION

    # Fetch raw events first (for deduplication)
    raw_events_list: list[_RawEvent] = [
        raw_event
        async for raw_event in self._fetcher.fetch_events(
            filter_obj,
            use_bigquery=use_bigquery,
        )
    ]

    logger.info("Fetched %d raw events from sources", len(raw_events_list))

    # Apply deduplication on raw events if requested
    # Deduplication happens on _RawEvent which implements HasDedupeFields protocol
    if deduplicate and dedupe_strategy is not None:
        original_count = len(raw_events_list)
        # Convert to iterator, deduplicate, then back to list
        raw_events_list = list(apply_dedup(iter(raw_events_list), dedupe_strategy))
        logger.info(
            "Deduplicated %d events to %d unique (strategy=%s)",
            original_count,
            len(raw_events_list),
            dedupe_strategy,
        )

    # Convert _RawEvent to Event models after deduplication
    events: list[Event] = []
    for raw_event in raw_events_list:
        event = Event.from_raw(raw_event)
        events.append(event)

    logger.info("Converted %d events to Event models", len(events))

    # Return as FetchResult (no failed requests tracked yet)
    return FetchResult(data=events)
```

#### `stream(filter_obj, *, deduplicate=False, dedupe_strategy=None, use_bigquery=False)`

Stream GDELT Events with memory-efficient iteration.

This is a streaming method that yields events one at a time, making it suitable for large datasets. Memory usage is constant regardless of result size.

Files are always tried first (free, no credentials), with automatic fallback to BigQuery on rate limit/error if credentials are configured.

Parameters:

| Name              | Type             | Description                                          | Default                                             |
| ----------------- | ---------------- | ---------------------------------------------------- | --------------------------------------------------- |
| `filter_obj`      | `EventFilter`    | Event filter with date range and query parameters    | *required*                                          |
| `deduplicate`     | `bool`           | If True, deduplicate events based on dedupe_strategy | `False`                                             |
| `dedupe_strategy` | \`DedupeStrategy | None\`                                               | Deduplication strategy (default: URL_DATE_LOCATION) |
| `use_bigquery`    | `bool`           | If True, skip files and use BigQuery directly        | `False`                                             |

Yields:

| Name    | Type                   | Description                                    |
| ------- | ---------------------- | ---------------------------------------------- |
| `Event` | `AsyncIterator[Event]` | Individual Event instances matching the filter |

Raises:

| Type                 | Description                                  |
| -------------------- | -------------------------------------------- |
| `RateLimitError`     | If rate limited and fallback not available   |
| `APIError`           | If download fails and fallback not available |
| `ConfigurationError` | If BigQuery requested but not configured     |

Example

> > > filter_obj = EventFilter( ... date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7)), ... actor1_country="USA", ... ) count = 0 async for event in endpoint.stream(filter_obj, deduplicate=True): ... print(event.global_event_id) ... count += 1 print(f"Streamed {count} unique events")

Source code in `src/py_gdelt/endpoints/events.py`

```
async def stream(
    self,
    filter_obj: EventFilter,
    *,
    deduplicate: bool = False,
    dedupe_strategy: DedupeStrategy | None = None,
    use_bigquery: bool = False,
) -> AsyncIterator[Event]:
    """Stream GDELT Events with memory-efficient iteration.

    This is a streaming method that yields events one at a time, making it
    suitable for large datasets. Memory usage is constant regardless of
    result size.

    Files are always tried first (free, no credentials), with automatic fallback
    to BigQuery on rate limit/error if credentials are configured.

    Args:
        filter_obj: Event filter with date range and query parameters
        deduplicate: If True, deduplicate events based on dedupe_strategy
        dedupe_strategy: Deduplication strategy (default: URL_DATE_LOCATION)
        use_bigquery: If True, skip files and use BigQuery directly

    Yields:
        Event: Individual Event instances matching the filter

    Raises:
        RateLimitError: If rate limited and fallback not available
        APIError: If download fails and fallback not available
        ConfigurationError: If BigQuery requested but not configured

    Example:
        >>> filter_obj = EventFilter(
        ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7)),
        ...     actor1_country="USA",
        ... )
        >>> count = 0
        >>> async for event in endpoint.stream(filter_obj, deduplicate=True):
        ...     print(event.global_event_id)
        ...     count += 1
        >>> print(f"Streamed {count} unique events")
    """
    # Default dedupe strategy
    if deduplicate and dedupe_strategy is None:
        dedupe_strategy = DedupeStrategy.URL_DATE_LOCATION

    # Fetch raw events from DataFetcher
    raw_events = self._fetcher.fetch_events(
        filter_obj,
        use_bigquery=use_bigquery,
    )

    # Apply deduplication if requested
    if deduplicate and dedupe_strategy is not None:
        logger.debug("Applying deduplication (strategy=%s)", dedupe_strategy)
        raw_events = apply_dedup_async(raw_events, dedupe_strategy)

    # Convert _RawEvent to Event at yield boundary
    count = 0
    async for raw_event in raw_events:
        event = Event.from_raw(raw_event)
        yield event
        count += 1

    logger.info("Streamed %d events", count)
```

#### `query_sync(filter_obj, *, deduplicate=False, dedupe_strategy=None, use_bigquery=False)`

Synchronous wrapper for query().

This is a convenience method that runs the async query() method in a new event loop. Prefer using the async version when possible.

Parameters:

| Name              | Type             | Description                                          | Default                                             |
| ----------------- | ---------------- | ---------------------------------------------------- | --------------------------------------------------- |
| `filter_obj`      | `EventFilter`    | Event filter with date range and query parameters    | *required*                                          |
| `deduplicate`     | `bool`           | If True, deduplicate events based on dedupe_strategy | `False`                                             |
| `dedupe_strategy` | \`DedupeStrategy | None\`                                               | Deduplication strategy (default: URL_DATE_LOCATION) |
| `use_bigquery`    | `bool`           | If True, skip files and use BigQuery directly        | `False`                                             |

Returns:

| Type                 | Description                            |
| -------------------- | -------------------------------------- |
| `FetchResult[Event]` | FetchResult containing Event instances |

Raises:

| Type                 | Description                                         |
| -------------------- | --------------------------------------------------- |
| `RateLimitError`     | If rate limited and fallback not available          |
| `APIError`           | If download fails and fallback not available        |
| `ConfigurationError` | If BigQuery requested but not configured            |
| `RuntimeError`       | If called from within an already running event loop |

Example

> > > filter_obj = EventFilter( ... date_range=DateRange(start=date(2024, 1, 1)), ... actor1_country="USA", ... ) result = endpoint.query_sync(filter_obj) for event in result: ... print(event.global_event_id)

Source code in `src/py_gdelt/endpoints/events.py`

```
def query_sync(
    self,
    filter_obj: EventFilter,
    *,
    deduplicate: bool = False,
    dedupe_strategy: DedupeStrategy | None = None,
    use_bigquery: bool = False,
) -> FetchResult[Event]:
    """Synchronous wrapper for query().

    This is a convenience method that runs the async query() method
    in a new event loop. Prefer using the async version when possible.

    Args:
        filter_obj: Event filter with date range and query parameters
        deduplicate: If True, deduplicate events based on dedupe_strategy
        dedupe_strategy: Deduplication strategy (default: URL_DATE_LOCATION)
        use_bigquery: If True, skip files and use BigQuery directly

    Returns:
        FetchResult containing Event instances

    Raises:
        RateLimitError: If rate limited and fallback not available
        APIError: If download fails and fallback not available
        ConfigurationError: If BigQuery requested but not configured
        RuntimeError: If called from within an already running event loop

    Example:
        >>> filter_obj = EventFilter(
        ...     date_range=DateRange(start=date(2024, 1, 1)),
        ...     actor1_country="USA",
        ... )
        >>> result = endpoint.query_sync(filter_obj)
        >>> for event in result:
        ...     print(event.global_event_id)
    """
    return asyncio.run(
        self.query(
            filter_obj,
            deduplicate=deduplicate,
            dedupe_strategy=dedupe_strategy,
            use_bigquery=use_bigquery,
        ),
    )
```

#### `stream_sync(filter_obj, *, deduplicate=False, dedupe_strategy=None, use_bigquery=False)`

Synchronous wrapper for stream().

This method provides a synchronous iterator interface over async streaming. It internally manages the event loop and yields events one at a time, providing true streaming behavior with memory efficiency.

Note: This creates a new event loop for each iteration, which has some overhead. For better performance, use the async stream() method directly if possible.

Parameters:

| Name              | Type             | Description                                          | Default                                             |
| ----------------- | ---------------- | ---------------------------------------------------- | --------------------------------------------------- |
| `filter_obj`      | `EventFilter`    | Event filter with date range and query parameters    | *required*                                          |
| `deduplicate`     | `bool`           | If True, deduplicate events based on dedupe_strategy | `False`                                             |
| `dedupe_strategy` | \`DedupeStrategy | None\`                                               | Deduplication strategy (default: URL_DATE_LOCATION) |
| `use_bigquery`    | `bool`           | If True, skip files and use BigQuery directly        | `False`                                             |

Returns:

| Type              | Description                                                  |
| ----------------- | ------------------------------------------------------------ |
| `Iterator[Event]` | Iterator that yields Event instances for each matching event |

Raises:

| Type                 | Description                                         |
| -------------------- | --------------------------------------------------- |
| `RateLimitError`     | If rate limited and fallback not available          |
| `APIError`           | If download fails and fallback not available        |
| `ConfigurationError` | If BigQuery requested but not configured            |
| `RuntimeError`       | If called from within an already running event loop |

Example

> > > filter_obj = EventFilter( ... date_range=DateRange(start=date(2024, 1, 1)), ... actor1_country="USA", ... ) for event in endpoint.stream_sync(filter_obj, deduplicate=True): ... print(event.global_event_id)

Source code in `src/py_gdelt/endpoints/events.py`

```
def stream_sync(
    self,
    filter_obj: EventFilter,
    *,
    deduplicate: bool = False,
    dedupe_strategy: DedupeStrategy | None = None,
    use_bigquery: bool = False,
) -> Iterator[Event]:
    """Synchronous wrapper for stream().

    This method provides a synchronous iterator interface over async streaming.
    It internally manages the event loop and yields events one at a time,
    providing true streaming behavior with memory efficiency.

    Note: This creates a new event loop for each iteration, which has some overhead.
    For better performance, use the async stream() method directly if possible.

    Args:
        filter_obj: Event filter with date range and query parameters
        deduplicate: If True, deduplicate events based on dedupe_strategy
        dedupe_strategy: Deduplication strategy (default: URL_DATE_LOCATION)
        use_bigquery: If True, skip files and use BigQuery directly

    Returns:
        Iterator that yields Event instances for each matching event

    Raises:
        RateLimitError: If rate limited and fallback not available
        APIError: If download fails and fallback not available
        ConfigurationError: If BigQuery requested but not configured
        RuntimeError: If called from within an already running event loop

    Example:
        >>> filter_obj = EventFilter(
        ...     date_range=DateRange(start=date(2024, 1, 1)),
        ...     actor1_country="USA",
        ... )
        >>> for event in endpoint.stream_sync(filter_obj, deduplicate=True):
        ...     print(event.global_event_id)
    """

    async def _async_generator() -> AsyncIterator[Event]:
        """Internal async generator for sync wrapper."""
        async for event in self.stream(
            filter_obj,
            deduplicate=deduplicate,
            dedupe_strategy=dedupe_strategy,
            use_bigquery=use_bigquery,
        ):
            yield event

    # Run async generator and yield results synchronously
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        async_gen = _async_generator()
        while True:
            try:
                event = loop.run_until_complete(async_gen.__anext__())
                yield event
            except StopAsyncIteration:
                break
    finally:
        loop.close()
```

### MentionsEndpoint

### `MentionsEndpoint`

Endpoint for querying GDELT Mentions data.

Mentions track individual occurrences of events across different news sources. Each mention links to an event in the Events table via GlobalEventID and contains metadata about the source, timing, document position, and confidence.

This endpoint uses DataFetcher for multi-source orchestration:

- Primary: File downloads (free, no credentials needed)
- Fallback: BigQuery (on rate limit/error, if credentials configured)

Parameters:

| Name               | Type             | Description                                                         | Default                                                                                                   |
| ------------------ | ---------------- | ------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------- |
| `file_source`      | `FileSource`     | FileSource instance for downloading GDELT files                     | *required*                                                                                                |
| `bigquery_source`  | \`BigQuerySource | None\`                                                              | Optional BigQuerySource instance for fallback queries                                                     |
| `settings`         | \`GDELTSettings  | None\`                                                              | Optional GDELTSettings for configuration (currently unused but reserved for future features like caching) |
| `fallback_enabled` | `bool`           | Whether to fallback to BigQuery on errors (default: True)           | `True`                                                                                                    |
| `error_policy`     | `ErrorPolicy`    | How to handle errors - 'raise', 'warn', or 'skip' (default: 'warn') | `'warn'`                                                                                                  |

Note

Mentions queries require BigQuery as files don't support event-specific filtering. File downloads would require fetching entire date ranges and filtering client-side, which is inefficient for single-event queries. BigQuery fallback only activates if both fallback_enabled=True AND bigquery_source is provided AND credentials are configured.

Example

> > > from datetime import date from py_gdelt.filters import DateRange, EventFilter from py_gdelt.sources import FileSource, BigQuerySource from py_gdelt.sources.fetcher import DataFetcher
> > >
> > > async with FileSource() as file_source: ... bq_source = BigQuerySource() ... fetcher = DataFetcher(file_source=file_source, bigquery_source=bq_source) ... endpoint = MentionsEndpoint(fetcher=fetcher) ... ... filter_obj = EventFilter( ... date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7)) ... ) ... ... # Batch query ... result = await endpoint.query(global_event_id="123456789", filter_obj=filter_obj) ... print(f"Found {len(result)} mentions") ... for mention in result: ... print(mention.source_name) ... ... # Streaming query ... async for mention in endpoint.stream(global_event_id="123456789", filter_obj=filter_obj): ... print(mention.source_name)

Source code in `src/py_gdelt/endpoints/mentions.py`

```
class MentionsEndpoint:
    """Endpoint for querying GDELT Mentions data.

    Mentions track individual occurrences of events across different news sources.
    Each mention links to an event in the Events table via GlobalEventID and contains
    metadata about the source, timing, document position, and confidence.

    This endpoint uses DataFetcher for multi-source orchestration:
    - Primary: File downloads (free, no credentials needed)
    - Fallback: BigQuery (on rate limit/error, if credentials configured)

    Args:
        file_source: FileSource instance for downloading GDELT files
        bigquery_source: Optional BigQuerySource instance for fallback queries
        settings: Optional GDELTSettings for configuration (currently unused but
            reserved for future features like caching)
        fallback_enabled: Whether to fallback to BigQuery on errors (default: True)
        error_policy: How to handle errors - 'raise', 'warn', or 'skip' (default: 'warn')

    Note:
        Mentions queries require BigQuery as files don't support event-specific filtering.
        File downloads would require fetching entire date ranges and filtering client-side,
        which is inefficient for single-event queries.
        BigQuery fallback only activates if both fallback_enabled=True AND
        bigquery_source is provided AND credentials are configured.

    Example:
        >>> from datetime import date
        >>> from py_gdelt.filters import DateRange, EventFilter
        >>> from py_gdelt.sources import FileSource, BigQuerySource
        >>> from py_gdelt.sources.fetcher import DataFetcher
        >>>
        >>> async with FileSource() as file_source:
        ...     bq_source = BigQuerySource()
        ...     fetcher = DataFetcher(file_source=file_source, bigquery_source=bq_source)
        ...     endpoint = MentionsEndpoint(fetcher=fetcher)
        ...
        ...     filter_obj = EventFilter(
        ...         date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7))
        ...     )
        ...
        ...     # Batch query
        ...     result = await endpoint.query(global_event_id="123456789", filter_obj=filter_obj)
        ...     print(f"Found {len(result)} mentions")
        ...     for mention in result:
        ...         print(mention.source_name)
        ...
        ...     # Streaming query
        ...     async for mention in endpoint.stream(global_event_id="123456789", filter_obj=filter_obj):
        ...         print(mention.source_name)
    """

    def __init__(
        self,
        file_source: FileSource,
        bigquery_source: BigQuerySource | None = None,
        *,
        settings: GDELTSettings | None = None,
        fallback_enabled: bool = True,
        error_policy: ErrorPolicy = "warn",
    ) -> None:
        from py_gdelt.sources.fetcher import DataFetcher

        self._settings = settings
        self._fetcher: DataFetcher = DataFetcher(
            file_source=file_source,
            bigquery_source=bigquery_source,
            fallback_enabled=fallback_enabled,
            error_policy=error_policy,
        )

        logger.debug(
            "MentionsEndpoint initialized (fallback_enabled=%s, error_policy=%s)",
            fallback_enabled,
            error_policy,
        )

    async def query(
        self,
        global_event_id: int,
        filter_obj: EventFilter,
        *,
        use_bigquery: bool = True,
    ) -> FetchResult[Mention]:
        """Query mentions for a specific event and return all results.

        This method collects all mentions into memory and returns them as a FetchResult.
        For large result sets or memory-constrained environments, use stream() instead.

        Args:
            global_event_id: Global event ID to fetch mentions for (integer)
            filter_obj: Filter with date range for the query window
            use_bigquery: If True, use BigQuery directly (default: True, recommended for mentions)

        Returns:
            FetchResult[Mention]: Container with list of Mention objects and failure tracking

        Raises:
            ConfigurationError: If BigQuery not configured but required
            ValueError: If date range is invalid or too large

        Example:
            >>> filter_obj = EventFilter(
            ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7))
            ... )
            >>> result = await endpoint.query(123456789, filter_obj)
            >>> print(f"Complete: {result.complete}, Count: {len(result)}")
            >>> for mention in result:
            ...     print(f"{mention.source_name}: {mention.confidence}%")
        """
        logger.info(
            "Querying mentions for event %s (date_range=%s to %s, use_bigquery=%s)",
            global_event_id,
            filter_obj.date_range.start,
            filter_obj.date_range.end or filter_obj.date_range.start,
            use_bigquery,
        )

        # Collect all mentions
        mentions: list[Mention] = [
            mention
            async for mention in self.stream(
                global_event_id=global_event_id,
                filter_obj=filter_obj,
                use_bigquery=use_bigquery,
            )
        ]

        logger.info(
            "Query complete: fetched %d mentions for event %s",
            len(mentions),
            global_event_id,
        )

        # For now, return FetchResult with no failures
        # In future, we could track file-level failures if using file source
        return FetchResult(data=mentions, failed=[])

    async def stream(
        self,
        global_event_id: int,
        filter_obj: EventFilter,
        *,
        use_bigquery: bool = True,
    ) -> AsyncIterator[Mention]:
        """Stream mentions for a specific event.

        This method yields mentions one at a time, converting from internal _RawMention
        to public Mention model at the yield boundary. Memory-efficient for large result sets.

        Args:
            global_event_id: Global event ID to fetch mentions for (integer)
            filter_obj: Filter with date range for the query window
            use_bigquery: If True, use BigQuery directly (default: True, recommended for mentions)

        Yields:
            Mention: Individual mention records with full type safety

        Raises:
            ConfigurationError: If BigQuery not configured but required
            ValueError: If date range is invalid or too large

        Example:
            >>> filter_obj = EventFilter(
            ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7))
            ... )
            >>> async for mention in endpoint.stream(123456789, filter_obj):
            ...     if mention.confidence >= 80:
            ...         print(f"High confidence: {mention.source_name}")
        """
        logger.debug(
            "Streaming mentions for event %s (date_range=%s to %s)",
            global_event_id,
            filter_obj.date_range.start,
            filter_obj.date_range.end or filter_obj.date_range.start,
        )

        mentions_count = 0

        # Use DataFetcher to query mentions
        # Note: fetch_mentions() returns AsyncIterator[_RawMention] (or dict from BigQuery)
        raw_mentions: AsyncIterator[_RawMention | dict[str, Any]] = self._fetcher.fetch_mentions(
            global_event_id=global_event_id,
            filter_obj=filter_obj,
            use_bigquery=use_bigquery,
        )

        # Convert _RawMention to Mention at yield boundary
        async for raw_mention in raw_mentions:
            # DataFetcher.fetch_mentions() returns dicts from BigQuery
            # We need to convert them to Mention
            # For now, assume BigQuery returns compatible dict structure
            if isinstance(raw_mention, dict):
                # BigQuery returns dict - convert to Mention directly
                # This is a simplified implementation - in production, we'd need proper BigQuery row mapping
                mention = self._dict_to_mention(raw_mention)
            else:
                # File source would return _RawMention (though mentions don't come from files typically)
                mention = Mention.from_raw(raw_mention)

            mentions_count += 1
            yield mention

        logger.debug("Streamed %d mentions for event %s", mentions_count, global_event_id)

    def _dict_to_mention(self, row: dict[str, Any]) -> Mention:
        """Convert BigQuery row dict to Mention model.

        This is a helper to bridge the gap between BigQuery result dicts and our Pydantic models.
        BigQuery returns rows as dictionaries, which we need to map to our internal structure.

        Args:
            row: BigQuery row as dictionary

        Returns:
            Mention: Validated Mention instance

        Note:
            This is a temporary implementation. In production, we'd use a proper BigQuery row mapper
            that handles field name translations and type conversions.
        """
        # Import here to avoid circular dependency
        from py_gdelt.models._internal import _RawMention

        # Map BigQuery column names to _RawMention fields
        # BigQuery uses different naming (e.g., EventTimeDate vs event_time_date)
        raw_mention = _RawMention(
            global_event_id=str(row.get("GlobalEventID", "")),
            event_time_date=str(row.get("EventTimeDate", "")),
            event_time_full=str(row.get("EventTimeFullDate", "")),
            mention_time_date=str(row.get("MentionTimeDate", "")),
            mention_time_full=str(row.get("MentionTimeFullDate", "")),
            mention_type=str(row.get("MentionType", "1")),
            mention_source_name=str(row.get("MentionSourceName", "")),
            mention_identifier=str(row.get("MentionIdentifier", "")),
            sentence_id=str(row.get("SentenceID", "0")),
            actor1_char_offset=str(row.get("Actor1CharOffset", "")),
            actor2_char_offset=str(row.get("Actor2CharOffset", "")),
            action_char_offset=str(row.get("ActionCharOffset", "")),
            in_raw_text=str(row.get("InRawText", "0")),
            confidence=str(row.get("Confidence", "50")),
            mention_doc_length=str(row.get("MentionDocLen", "0")),
            mention_doc_tone=str(row.get("MentionDocTone", "0.0")),
            mention_doc_translation_info=row.get("MentionDocTranslationInfo"),
            extras=row.get("Extras"),
        )

        return Mention.from_raw(raw_mention)

    def query_sync(
        self,
        global_event_id: int,
        filter_obj: EventFilter,
        *,
        use_bigquery: bool = True,
    ) -> FetchResult[Mention]:
        """Synchronous wrapper for query().

        This is a convenience method for synchronous code. It runs the async query()
        method in a new event loop. For better performance, use the async version directly.

        Args:
            global_event_id: Global event ID to fetch mentions for (integer)
            filter_obj: Filter with date range for the query window
            use_bigquery: If True, use BigQuery directly (default: True)

        Returns:
            FetchResult[Mention]: Container with list of Mention objects

        Raises:
            ConfigurationError: If BigQuery not configured but required
            ValueError: If date range is invalid

        Example:
            >>> filter_obj = EventFilter(
            ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7))
            ... )
            >>> result = endpoint.query_sync(123456789, filter_obj)
            >>> for mention in result:
            ...     print(mention.source_name)
        """
        return asyncio.run(
            self.query(
                global_event_id=global_event_id,
                filter_obj=filter_obj,
                use_bigquery=use_bigquery,
            ),
        )

    def stream_sync(
        self,
        global_event_id: int,
        filter_obj: EventFilter,
        *,
        use_bigquery: bool = True,
    ) -> Iterator[Mention]:
        """Synchronous wrapper for stream().

        This method provides a synchronous iterator interface over async streaming.
        It internally manages the event loop and yields mentions one at a time,
        providing true streaming behavior with memory efficiency.

        Note: This creates a new event loop for each iteration, which has some overhead.
        For better performance, use the async stream() method directly if possible.

        Args:
            global_event_id: Global event ID to fetch mentions for (integer)
            filter_obj: Filter with date range for the query window
            use_bigquery: If True, use BigQuery directly (default: True)

        Returns:
            Iterator of individual Mention records

        Raises:
            ConfigurationError: If BigQuery not configured but required
            ValueError: If date range is invalid
            RuntimeError: If called from within an already running event loop

        Example:
            >>> filter_obj = EventFilter(
            ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7))
            ... )
            >>> for mention in endpoint.stream_sync(123456789, filter_obj):
            ...     print(mention.source_name)
        """

        async def _async_generator() -> AsyncIterator[Mention]:
            """Internal async generator for sync wrapper."""
            async for mention in self.stream(
                global_event_id=global_event_id,
                filter_obj=filter_obj,
                use_bigquery=use_bigquery,
            ):
                yield mention

        # Run async generator and yield results synchronously
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            async_gen = _async_generator()
            while True:
                try:
                    mention = loop.run_until_complete(async_gen.__anext__())
                    yield mention
                except StopAsyncIteration:
                    break
        finally:
            loop.close()
```

#### `query(global_event_id, filter_obj, *, use_bigquery=True)`

Query mentions for a specific event and return all results.

This method collects all mentions into memory and returns them as a FetchResult. For large result sets or memory-constrained environments, use stream() instead.

Parameters:

| Name              | Type          | Description                                                              | Default    |
| ----------------- | ------------- | ------------------------------------------------------------------------ | ---------- |
| `global_event_id` | `int`         | Global event ID to fetch mentions for (integer)                          | *required* |
| `filter_obj`      | `EventFilter` | Filter with date range for the query window                              | *required* |
| `use_bigquery`    | `bool`        | If True, use BigQuery directly (default: True, recommended for mentions) | `True`     |

Returns:

| Type                   | Description                                                                         |
| ---------------------- | ----------------------------------------------------------------------------------- |
| `FetchResult[Mention]` | FetchResult\[Mention\]: Container with list of Mention objects and failure tracking |

Raises:

| Type                 | Description                             |
| -------------------- | --------------------------------------- |
| `ConfigurationError` | If BigQuery not configured but required |
| `ValueError`         | If date range is invalid or too large   |

Example

> > > filter_obj = EventFilter( ... date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7)) ... ) result = await endpoint.query(123456789, filter_obj) print(f"Complete: {result.complete}, Count: {len(result)}") for mention in result: ... print(f"{mention.source_name}: {mention.confidence}%")

Source code in `src/py_gdelt/endpoints/mentions.py`

```
async def query(
    self,
    global_event_id: int,
    filter_obj: EventFilter,
    *,
    use_bigquery: bool = True,
) -> FetchResult[Mention]:
    """Query mentions for a specific event and return all results.

    This method collects all mentions into memory and returns them as a FetchResult.
    For large result sets or memory-constrained environments, use stream() instead.

    Args:
        global_event_id: Global event ID to fetch mentions for (integer)
        filter_obj: Filter with date range for the query window
        use_bigquery: If True, use BigQuery directly (default: True, recommended for mentions)

    Returns:
        FetchResult[Mention]: Container with list of Mention objects and failure tracking

    Raises:
        ConfigurationError: If BigQuery not configured but required
        ValueError: If date range is invalid or too large

    Example:
        >>> filter_obj = EventFilter(
        ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7))
        ... )
        >>> result = await endpoint.query(123456789, filter_obj)
        >>> print(f"Complete: {result.complete}, Count: {len(result)}")
        >>> for mention in result:
        ...     print(f"{mention.source_name}: {mention.confidence}%")
    """
    logger.info(
        "Querying mentions for event %s (date_range=%s to %s, use_bigquery=%s)",
        global_event_id,
        filter_obj.date_range.start,
        filter_obj.date_range.end or filter_obj.date_range.start,
        use_bigquery,
    )

    # Collect all mentions
    mentions: list[Mention] = [
        mention
        async for mention in self.stream(
            global_event_id=global_event_id,
            filter_obj=filter_obj,
            use_bigquery=use_bigquery,
        )
    ]

    logger.info(
        "Query complete: fetched %d mentions for event %s",
        len(mentions),
        global_event_id,
    )

    # For now, return FetchResult with no failures
    # In future, we could track file-level failures if using file source
    return FetchResult(data=mentions, failed=[])
```

#### `stream(global_event_id, filter_obj, *, use_bigquery=True)`

Stream mentions for a specific event.

This method yields mentions one at a time, converting from internal \_RawMention to public Mention model at the yield boundary. Memory-efficient for large result sets.

Parameters:

| Name              | Type          | Description                                                              | Default    |
| ----------------- | ------------- | ------------------------------------------------------------------------ | ---------- |
| `global_event_id` | `int`         | Global event ID to fetch mentions for (integer)                          | *required* |
| `filter_obj`      | `EventFilter` | Filter with date range for the query window                              | *required* |
| `use_bigquery`    | `bool`        | If True, use BigQuery directly (default: True, recommended for mentions) | `True`     |

Yields:

| Name      | Type                     | Description                                      |
| --------- | ------------------------ | ------------------------------------------------ |
| `Mention` | `AsyncIterator[Mention]` | Individual mention records with full type safety |

Raises:

| Type                 | Description                             |
| -------------------- | --------------------------------------- |
| `ConfigurationError` | If BigQuery not configured but required |
| `ValueError`         | If date range is invalid or too large   |

Example

> > > filter_obj = EventFilter( ... date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7)) ... ) async for mention in endpoint.stream(123456789, filter_obj): ... if mention.confidence >= 80: ... print(f"High confidence: {mention.source_name}")

Source code in `src/py_gdelt/endpoints/mentions.py`

```
async def stream(
    self,
    global_event_id: int,
    filter_obj: EventFilter,
    *,
    use_bigquery: bool = True,
) -> AsyncIterator[Mention]:
    """Stream mentions for a specific event.

    This method yields mentions one at a time, converting from internal _RawMention
    to public Mention model at the yield boundary. Memory-efficient for large result sets.

    Args:
        global_event_id: Global event ID to fetch mentions for (integer)
        filter_obj: Filter with date range for the query window
        use_bigquery: If True, use BigQuery directly (default: True, recommended for mentions)

    Yields:
        Mention: Individual mention records with full type safety

    Raises:
        ConfigurationError: If BigQuery not configured but required
        ValueError: If date range is invalid or too large

    Example:
        >>> filter_obj = EventFilter(
        ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7))
        ... )
        >>> async for mention in endpoint.stream(123456789, filter_obj):
        ...     if mention.confidence >= 80:
        ...         print(f"High confidence: {mention.source_name}")
    """
    logger.debug(
        "Streaming mentions for event %s (date_range=%s to %s)",
        global_event_id,
        filter_obj.date_range.start,
        filter_obj.date_range.end or filter_obj.date_range.start,
    )

    mentions_count = 0

    # Use DataFetcher to query mentions
    # Note: fetch_mentions() returns AsyncIterator[_RawMention] (or dict from BigQuery)
    raw_mentions: AsyncIterator[_RawMention | dict[str, Any]] = self._fetcher.fetch_mentions(
        global_event_id=global_event_id,
        filter_obj=filter_obj,
        use_bigquery=use_bigquery,
    )

    # Convert _RawMention to Mention at yield boundary
    async for raw_mention in raw_mentions:
        # DataFetcher.fetch_mentions() returns dicts from BigQuery
        # We need to convert them to Mention
        # For now, assume BigQuery returns compatible dict structure
        if isinstance(raw_mention, dict):
            # BigQuery returns dict - convert to Mention directly
            # This is a simplified implementation - in production, we'd need proper BigQuery row mapping
            mention = self._dict_to_mention(raw_mention)
        else:
            # File source would return _RawMention (though mentions don't come from files typically)
            mention = Mention.from_raw(raw_mention)

        mentions_count += 1
        yield mention

    logger.debug("Streamed %d mentions for event %s", mentions_count, global_event_id)
```

#### `query_sync(global_event_id, filter_obj, *, use_bigquery=True)`

Synchronous wrapper for query().

This is a convenience method for synchronous code. It runs the async query() method in a new event loop. For better performance, use the async version directly.

Parameters:

| Name              | Type          | Description                                     | Default    |
| ----------------- | ------------- | ----------------------------------------------- | ---------- |
| `global_event_id` | `int`         | Global event ID to fetch mentions for (integer) | *required* |
| `filter_obj`      | `EventFilter` | Filter with date range for the query window     | *required* |
| `use_bigquery`    | `bool`        | If True, use BigQuery directly (default: True)  | `True`     |

Returns:

| Type                   | Description                                                    |
| ---------------------- | -------------------------------------------------------------- |
| `FetchResult[Mention]` | FetchResult\[Mention\]: Container with list of Mention objects |

Raises:

| Type                 | Description                             |
| -------------------- | --------------------------------------- |
| `ConfigurationError` | If BigQuery not configured but required |
| `ValueError`         | If date range is invalid                |

Example

> > > filter_obj = EventFilter( ... date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7)) ... ) result = endpoint.query_sync(123456789, filter_obj) for mention in result: ... print(mention.source_name)

Source code in `src/py_gdelt/endpoints/mentions.py`

```
def query_sync(
    self,
    global_event_id: int,
    filter_obj: EventFilter,
    *,
    use_bigquery: bool = True,
) -> FetchResult[Mention]:
    """Synchronous wrapper for query().

    This is a convenience method for synchronous code. It runs the async query()
    method in a new event loop. For better performance, use the async version directly.

    Args:
        global_event_id: Global event ID to fetch mentions for (integer)
        filter_obj: Filter with date range for the query window
        use_bigquery: If True, use BigQuery directly (default: True)

    Returns:
        FetchResult[Mention]: Container with list of Mention objects

    Raises:
        ConfigurationError: If BigQuery not configured but required
        ValueError: If date range is invalid

    Example:
        >>> filter_obj = EventFilter(
        ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7))
        ... )
        >>> result = endpoint.query_sync(123456789, filter_obj)
        >>> for mention in result:
        ...     print(mention.source_name)
    """
    return asyncio.run(
        self.query(
            global_event_id=global_event_id,
            filter_obj=filter_obj,
            use_bigquery=use_bigquery,
        ),
    )
```

#### `stream_sync(global_event_id, filter_obj, *, use_bigquery=True)`

Synchronous wrapper for stream().

This method provides a synchronous iterator interface over async streaming. It internally manages the event loop and yields mentions one at a time, providing true streaming behavior with memory efficiency.

Note: This creates a new event loop for each iteration, which has some overhead. For better performance, use the async stream() method directly if possible.

Parameters:

| Name              | Type          | Description                                     | Default    |
| ----------------- | ------------- | ----------------------------------------------- | ---------- |
| `global_event_id` | `int`         | Global event ID to fetch mentions for (integer) | *required* |
| `filter_obj`      | `EventFilter` | Filter with date range for the query window     | *required* |
| `use_bigquery`    | `bool`        | If True, use BigQuery directly (default: True)  | `True`     |

Returns:

| Type                | Description                            |
| ------------------- | -------------------------------------- |
| `Iterator[Mention]` | Iterator of individual Mention records |

Raises:

| Type                 | Description                                         |
| -------------------- | --------------------------------------------------- |
| `ConfigurationError` | If BigQuery not configured but required             |
| `ValueError`         | If date range is invalid                            |
| `RuntimeError`       | If called from within an already running event loop |

Example

> > > filter_obj = EventFilter( ... date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7)) ... ) for mention in endpoint.stream_sync(123456789, filter_obj): ... print(mention.source_name)

Source code in `src/py_gdelt/endpoints/mentions.py`

```
def stream_sync(
    self,
    global_event_id: int,
    filter_obj: EventFilter,
    *,
    use_bigquery: bool = True,
) -> Iterator[Mention]:
    """Synchronous wrapper for stream().

    This method provides a synchronous iterator interface over async streaming.
    It internally manages the event loop and yields mentions one at a time,
    providing true streaming behavior with memory efficiency.

    Note: This creates a new event loop for each iteration, which has some overhead.
    For better performance, use the async stream() method directly if possible.

    Args:
        global_event_id: Global event ID to fetch mentions for (integer)
        filter_obj: Filter with date range for the query window
        use_bigquery: If True, use BigQuery directly (default: True)

    Returns:
        Iterator of individual Mention records

    Raises:
        ConfigurationError: If BigQuery not configured but required
        ValueError: If date range is invalid
        RuntimeError: If called from within an already running event loop

    Example:
        >>> filter_obj = EventFilter(
        ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7))
        ... )
        >>> for mention in endpoint.stream_sync(123456789, filter_obj):
        ...     print(mention.source_name)
    """

    async def _async_generator() -> AsyncIterator[Mention]:
        """Internal async generator for sync wrapper."""
        async for mention in self.stream(
            global_event_id=global_event_id,
            filter_obj=filter_obj,
            use_bigquery=use_bigquery,
        ):
            yield mention

    # Run async generator and yield results synchronously
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        async_gen = _async_generator()
        while True:
            try:
                mention = loop.run_until_complete(async_gen.__anext__())
                yield mention
            except StopAsyncIteration:
                break
    finally:
        loop.close()
```

### GKGEndpoint

### `GKGEndpoint`

GKG (Global Knowledge Graph) endpoint for querying GDELT enriched content data.

The GKGEndpoint provides access to GDELT's Global Knowledge Graph, which contains rich content analysis including themes, people, organizations, locations, counts, tone, and other metadata extracted from news articles.

This endpoint uses DataFetcher to orchestrate source selection:

- Files are ALWAYS primary (free, no credentials needed)
- BigQuery is FALLBACK ONLY (on 429/error, if credentials configured)

Parameters:

| Name               | Type             | Description                                                         | Default                                                                                                   |
| ------------------ | ---------------- | ------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------- |
| `file_source`      | `FileSource`     | FileSource instance for downloading GDELT files                     | *required*                                                                                                |
| `bigquery_source`  | \`BigQuerySource | None\`                                                              | Optional BigQuerySource instance for fallback queries                                                     |
| `settings`         | \`GDELTSettings  | None\`                                                              | Optional GDELTSettings for configuration (currently unused but reserved for future features like caching) |
| `fallback_enabled` | `bool`           | Whether to fallback to BigQuery on errors (default: True)           | `True`                                                                                                    |
| `error_policy`     | `ErrorPolicy`    | How to handle errors - 'raise', 'warn', or 'skip' (default: 'warn') | `'warn'`                                                                                                  |

Note

BigQuery fallback only activates if both fallback_enabled=True AND bigquery_source is provided AND credentials are configured.

Example

Basic GKG query:

> > > from datetime import date from py_gdelt.filters import GKGFilter, DateRange from py_gdelt.endpoints.gkg import GKGEndpoint from py_gdelt.sources.files import FileSource
> > >
> > > async def main(): ... async with FileSource() as file_source: ... endpoint = GKGEndpoint(file_source=file_source) ... filter_obj = GKGFilter( ... date_range=DateRange(start=date(2024, 1, 1)), ... themes=["ENV_CLIMATECHANGE"] ... ) ... result = await endpoint.query(filter_obj) ... for record in result: ... print(record.record_id, record.source_url)

Streaming large result sets:

> > > async def stream_example(): ... async with FileSource() as file_source: ... endpoint = GKGEndpoint(file_source=file_source) ... filter_obj = GKGFilter( ... date_range=DateRange(start=date(2024, 1, 1)), ... country="USA" ... ) ... async for record in endpoint.stream(filter_obj): ... print(record.record_id, record.primary_theme)

Synchronous usage:

> > > endpoint = GKGEndpoint(file_source=file_source) result = endpoint.query_sync(filter_obj) for record in result: ... print(record.record_id)

Source code in `src/py_gdelt/endpoints/gkg.py`

```
class GKGEndpoint:
    """GKG (Global Knowledge Graph) endpoint for querying GDELT enriched content data.

    The GKGEndpoint provides access to GDELT's Global Knowledge Graph, which contains
    rich content analysis including themes, people, organizations, locations, counts,
    tone, and other metadata extracted from news articles.

    This endpoint uses DataFetcher to orchestrate source selection:
    - Files are ALWAYS primary (free, no credentials needed)
    - BigQuery is FALLBACK ONLY (on 429/error, if credentials configured)

    Args:
        file_source: FileSource instance for downloading GDELT files
        bigquery_source: Optional BigQuerySource instance for fallback queries
        settings: Optional GDELTSettings for configuration (currently unused but
            reserved for future features like caching)
        fallback_enabled: Whether to fallback to BigQuery on errors (default: True)
        error_policy: How to handle errors - 'raise', 'warn', or 'skip' (default: 'warn')

    Note:
        BigQuery fallback only activates if both fallback_enabled=True AND
        bigquery_source is provided AND credentials are configured.

    Example:
        Basic GKG query:

        >>> from datetime import date
        >>> from py_gdelt.filters import GKGFilter, DateRange
        >>> from py_gdelt.endpoints.gkg import GKGEndpoint
        >>> from py_gdelt.sources.files import FileSource
        >>>
        >>> async def main():
        ...     async with FileSource() as file_source:
        ...         endpoint = GKGEndpoint(file_source=file_source)
        ...         filter_obj = GKGFilter(
        ...             date_range=DateRange(start=date(2024, 1, 1)),
        ...             themes=["ENV_CLIMATECHANGE"]
        ...         )
        ...         result = await endpoint.query(filter_obj)
        ...         for record in result:
        ...             print(record.record_id, record.source_url)

        Streaming large result sets:

        >>> async def stream_example():
        ...     async with FileSource() as file_source:
        ...         endpoint = GKGEndpoint(file_source=file_source)
        ...         filter_obj = GKGFilter(
        ...             date_range=DateRange(start=date(2024, 1, 1)),
        ...             country="USA"
        ...         )
        ...         async for record in endpoint.stream(filter_obj):
        ...             print(record.record_id, record.primary_theme)

        Synchronous usage:

        >>> endpoint = GKGEndpoint(file_source=file_source)
        >>> result = endpoint.query_sync(filter_obj)
        >>> for record in result:
        ...     print(record.record_id)
    """

    def __init__(
        self,
        file_source: FileSource,
        bigquery_source: BigQuerySource | None = None,
        *,
        settings: GDELTSettings | None = None,
        fallback_enabled: bool = True,
        error_policy: ErrorPolicy = "warn",
    ) -> None:
        from py_gdelt.sources.fetcher import DataFetcher

        self._settings = settings
        self._fetcher: Any = DataFetcher(
            file_source=file_source,
            bigquery_source=bigquery_source,
            fallback_enabled=fallback_enabled,
            error_policy=error_policy,
        )

        logger.debug(
            "GKGEndpoint initialized (fallback_enabled=%s, error_policy=%s)",
            fallback_enabled,
            error_policy,
        )

    async def query(
        self,
        filter_obj: GKGFilter,
        *,
        use_bigquery: bool = False,
    ) -> FetchResult[GKGRecord]:
        """Query GKG data with automatic fallback and return all results.

        This method fetches all matching GKG records and returns them as a FetchResult
        container. For large result sets, consider using stream() instead to avoid
        loading everything into memory.

        Files are always tried first (free, no credentials), with automatic fallback
        to BigQuery on rate limit/error if credentials are configured.

        Args:
            filter_obj: GKG filter with date range and query parameters
            use_bigquery: If True, skip files and use BigQuery directly (default: False)

        Returns:
            FetchResult[GKGRecord] containing all matching records and any failures

        Raises:
            RateLimitError: If rate limited and fallback not available/enabled
            APIError: If download fails and fallback not available/enabled
            ConfigurationError: If BigQuery requested but not configured

        Example:
            >>> from datetime import date
            >>> from py_gdelt.filters import GKGFilter, DateRange
            >>>
            >>> filter_obj = GKGFilter(
            ...     date_range=DateRange(start=date(2024, 1, 1)),
            ...     themes=["ECON_STOCKMARKET"],
            ...     min_tone=0.0,  # Only positive tone
            ... )
            >>> result = await endpoint.query(filter_obj)
            >>> print(f"Fetched {len(result)} records")
            >>> if not result.complete:
            ...     print(f"Warning: {result.total_failed} requests failed")
            >>> for record in result:
            ...     print(record.record_id, record.tone.tone if record.tone else None)
        """
        records: list[GKGRecord] = [
            record async for record in self.stream(filter_obj, use_bigquery=use_bigquery)
        ]

        logger.info("GKG query completed: %d records fetched", len(records))

        # Return FetchResult (failures tracked by DataFetcher error policy)
        return FetchResult(data=records, failed=[])

    async def stream(
        self,
        filter_obj: GKGFilter,
        *,
        use_bigquery: bool = False,
    ) -> AsyncIterator[GKGRecord]:
        """Stream GKG records with automatic fallback.

        This method streams GKG records one at a time, which is memory-efficient for
        large result sets. Records are converted from internal _RawGKG dataclass to
        public GKGRecord Pydantic model at the yield boundary.

        Files are always tried first (free, no credentials), with automatic fallback
        to BigQuery on rate limit/error if credentials are configured.

        Args:
            filter_obj: GKG filter with date range and query parameters
            use_bigquery: If True, skip files and use BigQuery directly (default: False)

        Yields:
            GKGRecord: Individual GKG records matching the filter criteria

        Raises:
            RateLimitError: If rate limited and fallback not available/enabled
            APIError: If download fails and fallback not available/enabled
            ConfigurationError: If BigQuery requested but not configured

        Example:
            >>> from datetime import date
            >>> from py_gdelt.filters import GKGFilter, DateRange
            >>>
            >>> filter_obj = GKGFilter(
            ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7)),
            ...     organizations=["United Nations"],
            ... )
            >>> count = 0
            >>> async for record in endpoint.stream(filter_obj):
            ...     print(f"Processing {record.record_id}")
            ...     count += 1
            ...     if count >= 1000:
            ...         break  # Stop after 1000 records
        """
        logger.debug("Starting GKG stream for filter: %s", filter_obj)

        # Use DataFetcher to fetch raw GKG records
        async for raw_gkg in self._fetcher.fetch_gkg(filter_obj, use_bigquery=use_bigquery):
            # Convert _RawGKG to GKGRecord at yield boundary
            try:
                record = GKGRecord.from_raw(raw_gkg)
                yield record
            except Exception as e:  # noqa: BLE001
                # Error boundary: log conversion errors but continue processing other records
                logger.warning("Failed to convert raw GKG record to GKGRecord: %s", e)
                continue

    def query_sync(
        self,
        filter_obj: GKGFilter,
        *,
        use_bigquery: bool = False,
    ) -> FetchResult[GKGRecord]:
        """Synchronous wrapper for query().

        This is a convenience method for synchronous code that internally uses
        asyncio.run() to execute the async query() method.

        Args:
            filter_obj: GKG filter with date range and query parameters
            use_bigquery: If True, skip files and use BigQuery directly (default: False)

        Returns:
            FetchResult[GKGRecord] containing all matching records and any failures

        Raises:
            RateLimitError: If rate limited and fallback not available/enabled
            APIError: If download fails and fallback not available/enabled
            ConfigurationError: If BigQuery requested but not configured
            RuntimeError: If called from within an existing event loop

        Example:
            >>> from datetime import date
            >>> from py_gdelt.filters import GKGFilter, DateRange
            >>>
            >>> # Synchronous usage (no async/await needed)
            >>> endpoint = GKGEndpoint(file_source=file_source)
            >>> filter_obj = GKGFilter(
            ...     date_range=DateRange(start=date(2024, 1, 1))
            ... )
            >>> result = endpoint.query_sync(filter_obj)
            >>> for record in result:
            ...     print(record.record_id)
        """
        return asyncio.run(self.query(filter_obj, use_bigquery=use_bigquery))

    def stream_sync(
        self,
        filter_obj: GKGFilter,
        *,
        use_bigquery: bool = False,
    ) -> Iterator[GKGRecord]:
        """Synchronous wrapper for stream().

        This method provides a synchronous iterator interface over async streaming.
        It internally manages the event loop and yields records one at a time.

        Note: This creates a new event loop for each iteration, which has some overhead.
        For better performance, use the async stream() method directly if possible.

        Args:
            filter_obj: GKG filter with date range and query parameters
            use_bigquery: If True, skip files and use BigQuery directly (default: False)

        Returns:
            Iterator of GKGRecord instances for each matching record

        Raises:
            RateLimitError: If rate limited and fallback not available/enabled
            APIError: If download fails and fallback not available/enabled
            ConfigurationError: If BigQuery requested but not configured
            RuntimeError: If called from within an existing event loop

        Example:
            >>> from datetime import date
            >>> from py_gdelt.filters import GKGFilter, DateRange
            >>>
            >>> # Synchronous streaming (no async/await needed)
            >>> endpoint = GKGEndpoint(file_source=file_source)
            >>> filter_obj = GKGFilter(
            ...     date_range=DateRange(start=date(2024, 1, 1))
            ... )
            >>> for record in endpoint.stream_sync(filter_obj):
            ...     print(record.record_id)
            ...     if record.has_quotations:
            ...         print(f"  {len(record.quotations)} quotations found")
        """

        async def _async_generator() -> AsyncIterator[GKGRecord]:
            """Internal async generator for sync wrapper."""
            async for record in self.stream(filter_obj, use_bigquery=use_bigquery):
                yield record

        # Run async generator and yield results synchronously
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            async_gen = _async_generator()
            while True:
                try:
                    record = loop.run_until_complete(async_gen.__anext__())
                    yield record
                except StopAsyncIteration:
                    break
        finally:
            loop.close()
```

#### `query(filter_obj, *, use_bigquery=False)`

Query GKG data with automatic fallback and return all results.

This method fetches all matching GKG records and returns them as a FetchResult container. For large result sets, consider using stream() instead to avoid loading everything into memory.

Files are always tried first (free, no credentials), with automatic fallback to BigQuery on rate limit/error if credentials are configured.

Parameters:

| Name           | Type        | Description                                                    | Default    |
| -------------- | ----------- | -------------------------------------------------------------- | ---------- |
| `filter_obj`   | `GKGFilter` | GKG filter with date range and query parameters                | *required* |
| `use_bigquery` | `bool`      | If True, skip files and use BigQuery directly (default: False) | `False`    |

Returns:

| Type                     | Description                                                             |
| ------------------------ | ----------------------------------------------------------------------- |
| `FetchResult[GKGRecord]` | FetchResult[GKGRecord] containing all matching records and any failures |

Raises:

| Type                 | Description                                          |
| -------------------- | ---------------------------------------------------- |
| `RateLimitError`     | If rate limited and fallback not available/enabled   |
| `APIError`           | If download fails and fallback not available/enabled |
| `ConfigurationError` | If BigQuery requested but not configured             |

Example

> > > from datetime import date from py_gdelt.filters import GKGFilter, DateRange
> > >
> > > filter_obj = GKGFilter( ... date_range=DateRange(start=date(2024, 1, 1)), ... themes=["ECON_STOCKMARKET"], ... min_tone=0.0, # Only positive tone ... ) result = await endpoint.query(filter_obj) print(f"Fetched {len(result)} records") if not result.complete: ... print(f"Warning: {result.total_failed} requests failed") for record in result: ... print(record.record_id, record.tone.tone if record.tone else None)

Source code in `src/py_gdelt/endpoints/gkg.py`

```
async def query(
    self,
    filter_obj: GKGFilter,
    *,
    use_bigquery: bool = False,
) -> FetchResult[GKGRecord]:
    """Query GKG data with automatic fallback and return all results.

    This method fetches all matching GKG records and returns them as a FetchResult
    container. For large result sets, consider using stream() instead to avoid
    loading everything into memory.

    Files are always tried first (free, no credentials), with automatic fallback
    to BigQuery on rate limit/error if credentials are configured.

    Args:
        filter_obj: GKG filter with date range and query parameters
        use_bigquery: If True, skip files and use BigQuery directly (default: False)

    Returns:
        FetchResult[GKGRecord] containing all matching records and any failures

    Raises:
        RateLimitError: If rate limited and fallback not available/enabled
        APIError: If download fails and fallback not available/enabled
        ConfigurationError: If BigQuery requested but not configured

    Example:
        >>> from datetime import date
        >>> from py_gdelt.filters import GKGFilter, DateRange
        >>>
        >>> filter_obj = GKGFilter(
        ...     date_range=DateRange(start=date(2024, 1, 1)),
        ...     themes=["ECON_STOCKMARKET"],
        ...     min_tone=0.0,  # Only positive tone
        ... )
        >>> result = await endpoint.query(filter_obj)
        >>> print(f"Fetched {len(result)} records")
        >>> if not result.complete:
        ...     print(f"Warning: {result.total_failed} requests failed")
        >>> for record in result:
        ...     print(record.record_id, record.tone.tone if record.tone else None)
    """
    records: list[GKGRecord] = [
        record async for record in self.stream(filter_obj, use_bigquery=use_bigquery)
    ]

    logger.info("GKG query completed: %d records fetched", len(records))

    # Return FetchResult (failures tracked by DataFetcher error policy)
    return FetchResult(data=records, failed=[])
```

#### `stream(filter_obj, *, use_bigquery=False)`

Stream GKG records with automatic fallback.

This method streams GKG records one at a time, which is memory-efficient for large result sets. Records are converted from internal \_RawGKG dataclass to public GKGRecord Pydantic model at the yield boundary.

Files are always tried first (free, no credentials), with automatic fallback to BigQuery on rate limit/error if credentials are configured.

Parameters:

| Name           | Type        | Description                                                    | Default    |
| -------------- | ----------- | -------------------------------------------------------------- | ---------- |
| `filter_obj`   | `GKGFilter` | GKG filter with date range and query parameters                | *required* |
| `use_bigquery` | `bool`      | If True, skip files and use BigQuery directly (default: False) | `False`    |

Yields:

| Name        | Type                       | Description                                         |
| ----------- | -------------------------- | --------------------------------------------------- |
| `GKGRecord` | `AsyncIterator[GKGRecord]` | Individual GKG records matching the filter criteria |

Raises:

| Type                 | Description                                          |
| -------------------- | ---------------------------------------------------- |
| `RateLimitError`     | If rate limited and fallback not available/enabled   |
| `APIError`           | If download fails and fallback not available/enabled |
| `ConfigurationError` | If BigQuery requested but not configured             |

Example

> > > from datetime import date from py_gdelt.filters import GKGFilter, DateRange
> > >
> > > filter_obj = GKGFilter( ... date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7)), ... organizations=["United Nations"], ... ) count = 0 async for record in endpoint.stream(filter_obj): ... print(f"Processing {record.record_id}") ... count += 1 ... if count >= 1000: ... break # Stop after 1000 records

Source code in `src/py_gdelt/endpoints/gkg.py`

```
async def stream(
    self,
    filter_obj: GKGFilter,
    *,
    use_bigquery: bool = False,
) -> AsyncIterator[GKGRecord]:
    """Stream GKG records with automatic fallback.

    This method streams GKG records one at a time, which is memory-efficient for
    large result sets. Records are converted from internal _RawGKG dataclass to
    public GKGRecord Pydantic model at the yield boundary.

    Files are always tried first (free, no credentials), with automatic fallback
    to BigQuery on rate limit/error if credentials are configured.

    Args:
        filter_obj: GKG filter with date range and query parameters
        use_bigquery: If True, skip files and use BigQuery directly (default: False)

    Yields:
        GKGRecord: Individual GKG records matching the filter criteria

    Raises:
        RateLimitError: If rate limited and fallback not available/enabled
        APIError: If download fails and fallback not available/enabled
        ConfigurationError: If BigQuery requested but not configured

    Example:
        >>> from datetime import date
        >>> from py_gdelt.filters import GKGFilter, DateRange
        >>>
        >>> filter_obj = GKGFilter(
        ...     date_range=DateRange(start=date(2024, 1, 1), end=date(2024, 1, 7)),
        ...     organizations=["United Nations"],
        ... )
        >>> count = 0
        >>> async for record in endpoint.stream(filter_obj):
        ...     print(f"Processing {record.record_id}")
        ...     count += 1
        ...     if count >= 1000:
        ...         break  # Stop after 1000 records
    """
    logger.debug("Starting GKG stream for filter: %s", filter_obj)

    # Use DataFetcher to fetch raw GKG records
    async for raw_gkg in self._fetcher.fetch_gkg(filter_obj, use_bigquery=use_bigquery):
        # Convert _RawGKG to GKGRecord at yield boundary
        try:
            record = GKGRecord.from_raw(raw_gkg)
            yield record
        except Exception as e:  # noqa: BLE001
            # Error boundary: log conversion errors but continue processing other records
            logger.warning("Failed to convert raw GKG record to GKGRecord: %s", e)
            continue
```

#### `query_sync(filter_obj, *, use_bigquery=False)`

Synchronous wrapper for query().

This is a convenience method for synchronous code that internally uses asyncio.run() to execute the async query() method.

Parameters:

| Name           | Type        | Description                                                    | Default    |
| -------------- | ----------- | -------------------------------------------------------------- | ---------- |
| `filter_obj`   | `GKGFilter` | GKG filter with date range and query parameters                | *required* |
| `use_bigquery` | `bool`      | If True, skip files and use BigQuery directly (default: False) | `False`    |

Returns:

| Type                     | Description                                                             |
| ------------------------ | ----------------------------------------------------------------------- |
| `FetchResult[GKGRecord]` | FetchResult[GKGRecord] containing all matching records and any failures |

Raises:

| Type                 | Description                                          |
| -------------------- | ---------------------------------------------------- |
| `RateLimitError`     | If rate limited and fallback not available/enabled   |
| `APIError`           | If download fails and fallback not available/enabled |
| `ConfigurationError` | If BigQuery requested but not configured             |
| `RuntimeError`       | If called from within an existing event loop         |

Example

> > > from datetime import date from py_gdelt.filters import GKGFilter, DateRange
> > >
> > > ##### Synchronous usage (no async/await needed)
> > >
> > > endpoint = GKGEndpoint(file_source=file_source) filter_obj = GKGFilter( ... date_range=DateRange(start=date(2024, 1, 1)) ... ) result = endpoint.query_sync(filter_obj) for record in result: ... print(record.record_id)

Source code in `src/py_gdelt/endpoints/gkg.py`

```
def query_sync(
    self,
    filter_obj: GKGFilter,
    *,
    use_bigquery: bool = False,
) -> FetchResult[GKGRecord]:
    """Synchronous wrapper for query().

    This is a convenience method for synchronous code that internally uses
    asyncio.run() to execute the async query() method.

    Args:
        filter_obj: GKG filter with date range and query parameters
        use_bigquery: If True, skip files and use BigQuery directly (default: False)

    Returns:
        FetchResult[GKGRecord] containing all matching records and any failures

    Raises:
        RateLimitError: If rate limited and fallback not available/enabled
        APIError: If download fails and fallback not available/enabled
        ConfigurationError: If BigQuery requested but not configured
        RuntimeError: If called from within an existing event loop

    Example:
        >>> from datetime import date
        >>> from py_gdelt.filters import GKGFilter, DateRange
        >>>
        >>> # Synchronous usage (no async/await needed)
        >>> endpoint = GKGEndpoint(file_source=file_source)
        >>> filter_obj = GKGFilter(
        ...     date_range=DateRange(start=date(2024, 1, 1))
        ... )
        >>> result = endpoint.query_sync(filter_obj)
        >>> for record in result:
        ...     print(record.record_id)
    """
    return asyncio.run(self.query(filter_obj, use_bigquery=use_bigquery))
```

#### `stream_sync(filter_obj, *, use_bigquery=False)`

Synchronous wrapper for stream().

This method provides a synchronous iterator interface over async streaming. It internally manages the event loop and yields records one at a time.

Note: This creates a new event loop for each iteration, which has some overhead. For better performance, use the async stream() method directly if possible.

Parameters:

| Name           | Type        | Description                                                    | Default    |
| -------------- | ----------- | -------------------------------------------------------------- | ---------- |
| `filter_obj`   | `GKGFilter` | GKG filter with date range and query parameters                | *required* |
| `use_bigquery` | `bool`      | If True, skip files and use BigQuery directly (default: False) | `False`    |

Returns:

| Type                  | Description                                              |
| --------------------- | -------------------------------------------------------- |
| `Iterator[GKGRecord]` | Iterator of GKGRecord instances for each matching record |

Raises:

| Type                 | Description                                          |
| -------------------- | ---------------------------------------------------- |
| `RateLimitError`     | If rate limited and fallback not available/enabled   |
| `APIError`           | If download fails and fallback not available/enabled |
| `ConfigurationError` | If BigQuery requested but not configured             |
| `RuntimeError`       | If called from within an existing event loop         |

Example

> > > from datetime import date from py_gdelt.filters import GKGFilter, DateRange
> > >
> > > ##### Synchronous streaming (no async/await needed)
> > >
> > > endpoint = GKGEndpoint(file_source=file_source) filter_obj = GKGFilter( ... date_range=DateRange(start=date(2024, 1, 1)) ... ) for record in endpoint.stream_sync(filter_obj): ... print(record.record_id) ... if record.has_quotations: ... print(f" {len(record.quotations)} quotations found")

Source code in `src/py_gdelt/endpoints/gkg.py`

```
def stream_sync(
    self,
    filter_obj: GKGFilter,
    *,
    use_bigquery: bool = False,
) -> Iterator[GKGRecord]:
    """Synchronous wrapper for stream().

    This method provides a synchronous iterator interface over async streaming.
    It internally manages the event loop and yields records one at a time.

    Note: This creates a new event loop for each iteration, which has some overhead.
    For better performance, use the async stream() method directly if possible.

    Args:
        filter_obj: GKG filter with date range and query parameters
        use_bigquery: If True, skip files and use BigQuery directly (default: False)

    Returns:
        Iterator of GKGRecord instances for each matching record

    Raises:
        RateLimitError: If rate limited and fallback not available/enabled
        APIError: If download fails and fallback not available/enabled
        ConfigurationError: If BigQuery requested but not configured
        RuntimeError: If called from within an existing event loop

    Example:
        >>> from datetime import date
        >>> from py_gdelt.filters import GKGFilter, DateRange
        >>>
        >>> # Synchronous streaming (no async/await needed)
        >>> endpoint = GKGEndpoint(file_source=file_source)
        >>> filter_obj = GKGFilter(
        ...     date_range=DateRange(start=date(2024, 1, 1))
        ... )
        >>> for record in endpoint.stream_sync(filter_obj):
        ...     print(record.record_id)
        ...     if record.has_quotations:
        ...         print(f"  {len(record.quotations)} quotations found")
    """

    async def _async_generator() -> AsyncIterator[GKGRecord]:
        """Internal async generator for sync wrapper."""
        async for record in self.stream(filter_obj, use_bigquery=use_bigquery):
            yield record

    # Run async generator and yield results synchronously
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        async_gen = _async_generator()
        while True:
            try:
                record = loop.run_until_complete(async_gen.__anext__())
                yield record
            except StopAsyncIteration:
                break
    finally:
        loop.close()
```

### NGramsEndpoint

### `NGramsEndpoint`

Endpoint for querying GDELT NGrams 3.0 data.

Provides access to GDELT's NGrams dataset, which tracks word and phrase occurrences across global news with contextual information. NGrams are file-based only (no BigQuery support).

The endpoint uses DataFetcher for orchestrated file downloads with automatic retry, error handling, and intelligent caching. Internal \_RawNGram dataclass instances are converted to Pydantic NGramRecord models at the yield boundary.

Parameters:

| Name          | Type            | Description | Default                                                                                                                 |
| ------------- | --------------- | ----------- | ----------------------------------------------------------------------------------------------------------------------- |
| `settings`    | \`GDELTSettings | None\`      | Configuration settings. If None, uses defaults.                                                                         |
| `file_source` | \`FileSource    | None\`      | Optional shared FileSource. If None, creates owned instance. When provided, the source lifecycle is managed externally. |

Example

Batch query with filtering:

> > > from py_gdelt.filters import NGramsFilter, DateRange from datetime import date async with NGramsEndpoint() as endpoint: ... filter_obj = NGramsFilter( ... date_range=DateRange(start=date(2024, 1, 1)), ... language="en", ... ngram="climate", ... ) ... result = await endpoint.query(filter_obj) ... print(f"Found {len(result)} records")

Streaming for large datasets:

> > > async with NGramsEndpoint() as endpoint: ... filter_obj = NGramsFilter( ... date_range=DateRange( ... start=date(2024, 1, 1), ... end=date(2024, 1, 7) ... ), ... language="en", ... ) ... async for record in endpoint.stream(filter_obj): ... if record.is_early_in_article: ... print(f"Early: {record.ngram} in {record.url}")

Source code in `src/py_gdelt/endpoints/ngrams.py`

```
class NGramsEndpoint:
    """Endpoint for querying GDELT NGrams 3.0 data.

    Provides access to GDELT's NGrams dataset, which tracks word and phrase
    occurrences across global news with contextual information. NGrams are
    file-based only (no BigQuery support).

    The endpoint uses DataFetcher for orchestrated file downloads with automatic
    retry, error handling, and intelligent caching. Internal _RawNGram dataclass
    instances are converted to Pydantic NGramRecord models at the yield boundary.

    Args:
        settings: Configuration settings. If None, uses defaults.
        file_source: Optional shared FileSource. If None, creates owned instance.
                    When provided, the source lifecycle is managed externally.

    Example:
        Batch query with filtering:

        >>> from py_gdelt.filters import NGramsFilter, DateRange
        >>> from datetime import date
        >>> async with NGramsEndpoint() as endpoint:
        ...     filter_obj = NGramsFilter(
        ...         date_range=DateRange(start=date(2024, 1, 1)),
        ...         language="en",
        ...         ngram="climate",
        ...     )
        ...     result = await endpoint.query(filter_obj)
        ...     print(f"Found {len(result)} records")

        Streaming for large datasets:

        >>> async with NGramsEndpoint() as endpoint:
        ...     filter_obj = NGramsFilter(
        ...         date_range=DateRange(
        ...             start=date(2024, 1, 1),
        ...             end=date(2024, 1, 7)
        ...         ),
        ...         language="en",
        ...     )
        ...     async for record in endpoint.stream(filter_obj):
        ...         if record.is_early_in_article:
        ...             print(f"Early: {record.ngram} in {record.url}")
    """

    def __init__(
        self,
        settings: GDELTSettings | None = None,
        file_source: FileSource | None = None,
    ) -> None:
        self.settings = settings or GDELTSettings()

        if file_source is not None:
            self._file_source = file_source
            self._owns_sources = False
        else:
            self._file_source = FileSource(settings=self.settings)
            self._owns_sources = True

        # Create DataFetcher with file source only (NGrams don't support BigQuery)
        self._fetcher = DataFetcher(
            file_source=self._file_source,
            bigquery_source=None,
            fallback_enabled=False,
            error_policy="warn",
        )

        # Create parser instance
        self._parser = NGramsParser()

    async def close(self) -> None:
        """Close resources if we own them.

        Only closes resources that were created by this instance.
        Shared resources are not closed to allow reuse.
        """
        if self._owns_sources:
            # FileSource uses context manager protocol, manually call __aexit__
            await self._file_source.__aexit__(None, None, None)

    async def __aenter__(self) -> NGramsEndpoint:
        """Async context manager entry.

        Returns:
            Self for use in async with statement.
        """
        return self

    async def __aexit__(self, *args: object) -> None:
        """Async context manager exit - close resources.

        Args:
            *args: Exception info (unused, but required by protocol).
        """
        await self.close()

    async def query(self, filter_obj: NGramsFilter) -> FetchResult[NGramRecord]:
        """Query NGrams data and return all results.

        Fetches all NGram records matching the filter criteria and returns them
        as a FetchResult. This method collects all records in memory before returning,
        so use stream() for large result sets to avoid memory issues.

        Args:
            filter_obj: Filter with date range and optional ngram/language constraints

        Returns:
            FetchResult containing list of NGramRecord instances and any failed requests

        Raises:
            RateLimitError: If rate limited and retries exhausted
            APIError: If downloads fail
            DataError: If file parsing fails

        Example:
            >>> filter_obj = NGramsFilter(
            ...     date_range=DateRange(start=date(2024, 1, 1)),
            ...     language="en",
            ...     min_position=0,
            ...     max_position=20,
            ... )
            >>> result = await endpoint.query(filter_obj)
            >>> print(f"Found {len(result)} records in article headlines")
        """
        # Stream all records and collect them
        records: list[NGramRecord] = [record async for record in self.stream(filter_obj)]

        logger.info("Collected %d NGram records from query", len(records))

        # Return FetchResult (no failed tracking for now - DataFetcher handles errors)
        return FetchResult(data=records, failed=[])

    async def stream(self, filter_obj: NGramsFilter) -> AsyncIterator[NGramRecord]:
        """Stream NGrams data record by record.

        Yields NGram records one at a time, converting internal _RawNGram dataclass
        instances to Pydantic NGramRecord models at the yield boundary. This method
        is memory-efficient for large result sets.

        Client-side filtering is applied for ngram text, language, and position
        constraints since file downloads provide all records for a date range.

        Args:
            filter_obj: Filter with date range and optional ngram/language constraints

        Yields:
            NGramRecord: Individual NGram records matching the filter criteria

        Raises:
            RateLimitError: If rate limited and retries exhausted
            APIError: If downloads fail
            DataError: If file parsing fails

        Example:
            >>> filter_obj = NGramsFilter(
            ...     date_range=DateRange(start=date(2024, 1, 1)),
            ...     ngram="climate",
            ...     language="en",
            ... )
            >>> async for record in endpoint.stream(filter_obj):
            ...     print(f"{record.ngram}: {record.context}")
        """
        # Use DataFetcher's fetch_ngrams method to get raw records
        async for raw_record in self._fetcher.fetch_ngrams(filter_obj):
            # Convert _RawNGram to NGramRecord (type conversion happens here)
            try:
                record = NGramRecord.from_raw(raw_record)
            except Exception as e:  # noqa: BLE001
                logger.warning(
                    "Failed to convert raw ngram to NGramRecord: %s - Skipping",
                    e,
                )
                continue

            # Apply client-side filtering
            if not self._matches_filter(record, filter_obj):
                continue

            yield record

    def _matches_filter(self, record: NGramRecord, filter_obj: NGramsFilter) -> bool:
        """Check if record matches filter criteria.

        Applies client-side filtering for ngram text, language, and position
        constraints. Date filtering is handled by DataFetcher (file selection).

        Args:
            record: NGramRecord to check
            filter_obj: Filter criteria

        Returns:
            True if record matches all filter criteria, False otherwise
        """
        # Filter by ngram text (case-insensitive substring match)
        if filter_obj.ngram is not None and filter_obj.ngram.lower() not in record.ngram.lower():
            return False

        # Filter by language (exact match)
        if filter_obj.language is not None and record.language != filter_obj.language:
            return False

        # Filter by position (article decile)
        if filter_obj.min_position is not None and record.position < filter_obj.min_position:
            return False

        return not (
            filter_obj.max_position is not None and record.position > filter_obj.max_position
        )

    def query_sync(self, filter_obj: NGramsFilter) -> FetchResult[NGramRecord]:
        """Synchronous wrapper for query().

        Runs the async query() method in a new event loop. This is a convenience
        method for synchronous code, but async methods are preferred when possible.

        Args:
            filter_obj: Filter with date range and optional constraints

        Returns:
            FetchResult containing list of NGramRecord instances

        Raises:
            RateLimitError: If rate limited and retries exhausted
            APIError: If downloads fail
            DataError: If file parsing fails

        Example:
            >>> filter_obj = NGramsFilter(
            ...     date_range=DateRange(start=date(2024, 1, 1)),
            ...     language="en",
            ... )
            >>> result = endpoint.query_sync(filter_obj)
            >>> print(f"Found {len(result)} records")
        """
        return asyncio.run(self.query(filter_obj))

    def stream_sync(self, filter_obj: NGramsFilter) -> Iterator[NGramRecord]:
        """Synchronous wrapper for stream().

        Yields NGram records from the async stream() method in a blocking manner.
        This is a convenience method for synchronous code, but async methods are
        preferred when possible.

        Args:
            filter_obj: Filter with date range and optional constraints

        Yields:
            NGramRecord: Individual NGram records matching the filter criteria

        Raises:
            RuntimeError: If called from within a running event loop.
            RateLimitError: If rate limited and retries exhausted.
            APIError: If downloads fail.
            DataError: If file parsing fails.

        Note:
            This method cannot be called from within an async context (e.g., inside
            an async function or running event loop). Doing so will raise RuntimeError.
            Use the async stream() method instead. This method creates its own event
            loop internally.

        Example:
            >>> filter_obj = NGramsFilter(
            ...     date_range=DateRange(start=date(2024, 1, 1)),
            ...     ngram="climate",
            ... )
            >>> for record in endpoint.stream_sync(filter_obj):
            ...     print(f"{record.ngram}: {record.url}")
        """
        # Manual event loop management is required for async generators.
        # Unlike query_sync() which uses asyncio.run() for a single coroutine,
        # stream_sync() must iterate through an async generator step-by-step.
        # asyncio.run() cannot handle async generators - it expects a coroutine
        # that returns a value, not one that yields multiple values.

        # Check if we're already in an async context - this would cause issues
        try:
            asyncio.get_running_loop()
            has_running_loop = True
        except RuntimeError:
            has_running_loop = False

        if has_running_loop:
            msg = "stream_sync() cannot be called from a running event loop. Use stream() instead."
            raise RuntimeError(msg)

        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            async_gen = self.stream(filter_obj)
            while True:
                try:
                    record = loop.run_until_complete(async_gen.__anext__())
                    yield record
                except StopAsyncIteration:
                    break
        finally:
            loop.close()
```

#### `close()`

Close resources if we own them.

Only closes resources that were created by this instance. Shared resources are not closed to allow reuse.

Source code in `src/py_gdelt/endpoints/ngrams.py`

```
async def close(self) -> None:
    """Close resources if we own them.

    Only closes resources that were created by this instance.
    Shared resources are not closed to allow reuse.
    """
    if self._owns_sources:
        # FileSource uses context manager protocol, manually call __aexit__
        await self._file_source.__aexit__(None, None, None)
```

#### `__aenter__()`

Async context manager entry.

Returns:

| Type             | Description                           |
| ---------------- | ------------------------------------- |
| `NGramsEndpoint` | Self for use in async with statement. |

Source code in `src/py_gdelt/endpoints/ngrams.py`

```
async def __aenter__(self) -> NGramsEndpoint:
    """Async context manager entry.

    Returns:
        Self for use in async with statement.
    """
    return self
```

#### `__aexit__(*args)`

Async context manager exit - close resources.

Parameters:

| Name    | Type     | Description                                        | Default |
| ------- | -------- | -------------------------------------------------- | ------- |
| `*args` | `object` | Exception info (unused, but required by protocol). | `()`    |

Source code in `src/py_gdelt/endpoints/ngrams.py`

```
async def __aexit__(self, *args: object) -> None:
    """Async context manager exit - close resources.

    Args:
        *args: Exception info (unused, but required by protocol).
    """
    await self.close()
```

#### `query(filter_obj)`

Query NGrams data and return all results.

Fetches all NGram records matching the filter criteria and returns them as a FetchResult. This method collects all records in memory before returning, so use stream() for large result sets to avoid memory issues.

Parameters:

| Name         | Type           | Description                                                    | Default    |
| ------------ | -------------- | -------------------------------------------------------------- | ---------- |
| `filter_obj` | `NGramsFilter` | Filter with date range and optional ngram/language constraints | *required* |

Returns:

| Type                       | Description                                                                  |
| -------------------------- | ---------------------------------------------------------------------------- |
| `FetchResult[NGramRecord]` | FetchResult containing list of NGramRecord instances and any failed requests |

Raises:

| Type             | Description                           |
| ---------------- | ------------------------------------- |
| `RateLimitError` | If rate limited and retries exhausted |
| `APIError`       | If downloads fail                     |
| `DataError`      | If file parsing fails                 |

Example

> > > filter_obj = NGramsFilter( ... date_range=DateRange(start=date(2024, 1, 1)), ... language="en", ... min_position=0, ... max_position=20, ... ) result = await endpoint.query(filter_obj) print(f"Found {len(result)} records in article headlines")

Source code in `src/py_gdelt/endpoints/ngrams.py`

```
async def query(self, filter_obj: NGramsFilter) -> FetchResult[NGramRecord]:
    """Query NGrams data and return all results.

    Fetches all NGram records matching the filter criteria and returns them
    as a FetchResult. This method collects all records in memory before returning,
    so use stream() for large result sets to avoid memory issues.

    Args:
        filter_obj: Filter with date range and optional ngram/language constraints

    Returns:
        FetchResult containing list of NGramRecord instances and any failed requests

    Raises:
        RateLimitError: If rate limited and retries exhausted
        APIError: If downloads fail
        DataError: If file parsing fails

    Example:
        >>> filter_obj = NGramsFilter(
        ...     date_range=DateRange(start=date(2024, 1, 1)),
        ...     language="en",
        ...     min_position=0,
        ...     max_position=20,
        ... )
        >>> result = await endpoint.query(filter_obj)
        >>> print(f"Found {len(result)} records in article headlines")
    """
    # Stream all records and collect them
    records: list[NGramRecord] = [record async for record in self.stream(filter_obj)]

    logger.info("Collected %d NGram records from query", len(records))

    # Return FetchResult (no failed tracking for now - DataFetcher handles errors)
    return FetchResult(data=records, failed=[])
```

#### `stream(filter_obj)`

Stream NGrams data record by record.

Yields NGram records one at a time, converting internal \_RawNGram dataclass instances to Pydantic NGramRecord models at the yield boundary. This method is memory-efficient for large result sets.

Client-side filtering is applied for ngram text, language, and position constraints since file downloads provide all records for a date range.

Parameters:

| Name         | Type           | Description                                                    | Default    |
| ------------ | -------------- | -------------------------------------------------------------- | ---------- |
| `filter_obj` | `NGramsFilter` | Filter with date range and optional ngram/language constraints | *required* |

Yields:

| Name          | Type                         | Description                                           |
| ------------- | ---------------------------- | ----------------------------------------------------- |
| `NGramRecord` | `AsyncIterator[NGramRecord]` | Individual NGram records matching the filter criteria |

Raises:

| Type             | Description                           |
| ---------------- | ------------------------------------- |
| `RateLimitError` | If rate limited and retries exhausted |
| `APIError`       | If downloads fail                     |
| `DataError`      | If file parsing fails                 |

Example

> > > filter_obj = NGramsFilter( ... date_range=DateRange(start=date(2024, 1, 1)), ... ngram="climate", ... language="en", ... ) async for record in endpoint.stream(filter_obj): ... print(f"{record.ngram}: {record.context}")

Source code in `src/py_gdelt/endpoints/ngrams.py`

```
async def stream(self, filter_obj: NGramsFilter) -> AsyncIterator[NGramRecord]:
    """Stream NGrams data record by record.

    Yields NGram records one at a time, converting internal _RawNGram dataclass
    instances to Pydantic NGramRecord models at the yield boundary. This method
    is memory-efficient for large result sets.

    Client-side filtering is applied for ngram text, language, and position
    constraints since file downloads provide all records for a date range.

    Args:
        filter_obj: Filter with date range and optional ngram/language constraints

    Yields:
        NGramRecord: Individual NGram records matching the filter criteria

    Raises:
        RateLimitError: If rate limited and retries exhausted
        APIError: If downloads fail
        DataError: If file parsing fails

    Example:
        >>> filter_obj = NGramsFilter(
        ...     date_range=DateRange(start=date(2024, 1, 1)),
        ...     ngram="climate",
        ...     language="en",
        ... )
        >>> async for record in endpoint.stream(filter_obj):
        ...     print(f"{record.ngram}: {record.context}")
    """
    # Use DataFetcher's fetch_ngrams method to get raw records
    async for raw_record in self._fetcher.fetch_ngrams(filter_obj):
        # Convert _RawNGram to NGramRecord (type conversion happens here)
        try:
            record = NGramRecord.from_raw(raw_record)
        except Exception as e:  # noqa: BLE001
            logger.warning(
                "Failed to convert raw ngram to NGramRecord: %s - Skipping",
                e,
            )
            continue

        # Apply client-side filtering
        if not self._matches_filter(record, filter_obj):
            continue

        yield record
```

#### `query_sync(filter_obj)`

Synchronous wrapper for query().

Runs the async query() method in a new event loop. This is a convenience method for synchronous code, but async methods are preferred when possible.

Parameters:

| Name         | Type           | Description                                     | Default    |
| ------------ | -------------- | ----------------------------------------------- | ---------- |
| `filter_obj` | `NGramsFilter` | Filter with date range and optional constraints | *required* |

Returns:

| Type                       | Description                                          |
| -------------------------- | ---------------------------------------------------- |
| `FetchResult[NGramRecord]` | FetchResult containing list of NGramRecord instances |

Raises:

| Type             | Description                           |
| ---------------- | ------------------------------------- |
| `RateLimitError` | If rate limited and retries exhausted |
| `APIError`       | If downloads fail                     |
| `DataError`      | If file parsing fails                 |

Example

> > > filter_obj = NGramsFilter( ... date_range=DateRange(start=date(2024, 1, 1)), ... language="en", ... ) result = endpoint.query_sync(filter_obj) print(f"Found {len(result)} records")

Source code in `src/py_gdelt/endpoints/ngrams.py`

```
def query_sync(self, filter_obj: NGramsFilter) -> FetchResult[NGramRecord]:
    """Synchronous wrapper for query().

    Runs the async query() method in a new event loop. This is a convenience
    method for synchronous code, but async methods are preferred when possible.

    Args:
        filter_obj: Filter with date range and optional constraints

    Returns:
        FetchResult containing list of NGramRecord instances

    Raises:
        RateLimitError: If rate limited and retries exhausted
        APIError: If downloads fail
        DataError: If file parsing fails

    Example:
        >>> filter_obj = NGramsFilter(
        ...     date_range=DateRange(start=date(2024, 1, 1)),
        ...     language="en",
        ... )
        >>> result = endpoint.query_sync(filter_obj)
        >>> print(f"Found {len(result)} records")
    """
    return asyncio.run(self.query(filter_obj))
```

#### `stream_sync(filter_obj)`

Synchronous wrapper for stream().

Yields NGram records from the async stream() method in a blocking manner. This is a convenience method for synchronous code, but async methods are preferred when possible.

Parameters:

| Name         | Type           | Description                                     | Default    |
| ------------ | -------------- | ----------------------------------------------- | ---------- |
| `filter_obj` | `NGramsFilter` | Filter with date range and optional constraints | *required* |

Yields:

| Name          | Type          | Description                                           |
| ------------- | ------------- | ----------------------------------------------------- |
| `NGramRecord` | `NGramRecord` | Individual NGram records matching the filter criteria |

Raises:

| Type             | Description                                 |
| ---------------- | ------------------------------------------- |
| `RuntimeError`   | If called from within a running event loop. |
| `RateLimitError` | If rate limited and retries exhausted.      |
| `APIError`       | If downloads fail.                          |
| `DataError`      | If file parsing fails.                      |

Note

This method cannot be called from within an async context (e.g., inside an async function or running event loop). Doing so will raise RuntimeError. Use the async stream() method instead. This method creates its own event loop internally.

Example

> > > filter_obj = NGramsFilter( ... date_range=DateRange(start=date(2024, 1, 1)), ... ngram="climate", ... ) for record in endpoint.stream_sync(filter_obj): ... print(f"{record.ngram}: {record.url}")

Source code in `src/py_gdelt/endpoints/ngrams.py`

```
def stream_sync(self, filter_obj: NGramsFilter) -> Iterator[NGramRecord]:
    """Synchronous wrapper for stream().

    Yields NGram records from the async stream() method in a blocking manner.
    This is a convenience method for synchronous code, but async methods are
    preferred when possible.

    Args:
        filter_obj: Filter with date range and optional constraints

    Yields:
        NGramRecord: Individual NGram records matching the filter criteria

    Raises:
        RuntimeError: If called from within a running event loop.
        RateLimitError: If rate limited and retries exhausted.
        APIError: If downloads fail.
        DataError: If file parsing fails.

    Note:
        This method cannot be called from within an async context (e.g., inside
        an async function or running event loop). Doing so will raise RuntimeError.
        Use the async stream() method instead. This method creates its own event
        loop internally.

    Example:
        >>> filter_obj = NGramsFilter(
        ...     date_range=DateRange(start=date(2024, 1, 1)),
        ...     ngram="climate",
        ... )
        >>> for record in endpoint.stream_sync(filter_obj):
        ...     print(f"{record.ngram}: {record.url}")
    """
    # Manual event loop management is required for async generators.
    # Unlike query_sync() which uses asyncio.run() for a single coroutine,
    # stream_sync() must iterate through an async generator step-by-step.
    # asyncio.run() cannot handle async generators - it expects a coroutine
    # that returns a value, not one that yields multiple values.

    # Check if we're already in an async context - this would cause issues
    try:
        asyncio.get_running_loop()
        has_running_loop = True
    except RuntimeError:
        has_running_loop = False

    if has_running_loop:
        msg = "stream_sync() cannot be called from a running event loop. Use stream() instead."
        raise RuntimeError(msg)

    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        async_gen = self.stream(filter_obj)
        while True:
            try:
                record = loop.run_until_complete(async_gen.__anext__())
                yield record
            except StopAsyncIteration:
                break
    finally:
        loop.close()
```

## REST API Endpoints

### DocEndpoint

### `DocEndpoint`

Bases: `BaseEndpoint`

DOC 2.0 API endpoint for searching GDELT articles.

The DOC API provides full-text search across GDELT's monitored news sources with support for various output modes (article lists, timelines, galleries) and flexible filtering by time, source, language, and relevance.

Attributes:

| Name       | Type | Description                       |
| ---------- | ---- | --------------------------------- |
| `BASE_URL` |      | Base URL for the DOC API endpoint |

Example

Basic article search:

> > > async with DocEndpoint() as doc: ... articles = await doc.search("climate change", max_results=100) ... for article in articles: ... print(article.title, article.url)

Using filters for advanced queries:

> > > from py_gdelt.filters import DocFilter async with DocEndpoint() as doc: ... filter = DocFilter( ... query="elections", ... timespan="7d", ... source_country="US", ... sort_by="relevance" ... ) ... articles = await doc.query(filter)

Getting timeline data:

> > > async with DocEndpoint() as doc: ... timeline = await doc.timeline("protests", timespan="30d") ... for point in timeline.points: ... print(point.date, point.value)

Source code in `src/py_gdelt/endpoints/doc.py`

```
class DocEndpoint(BaseEndpoint):
    """
    DOC 2.0 API endpoint for searching GDELT articles.

    The DOC API provides full-text search across GDELT's monitored news sources
    with support for various output modes (article lists, timelines, galleries)
    and flexible filtering by time, source, language, and relevance.

    Attributes:
        BASE_URL: Base URL for the DOC API endpoint

    Example:
        Basic article search:

        >>> async with DocEndpoint() as doc:
        ...     articles = await doc.search("climate change", max_results=100)
        ...     for article in articles:
        ...         print(article.title, article.url)

        Using filters for advanced queries:

        >>> from py_gdelt.filters import DocFilter
        >>> async with DocEndpoint() as doc:
        ...     filter = DocFilter(
        ...         query="elections",
        ...         timespan="7d",
        ...         source_country="US",
        ...         sort_by="relevance"
        ...     )
        ...     articles = await doc.query(filter)

        Getting timeline data:

        >>> async with DocEndpoint() as doc:
        ...     timeline = await doc.timeline("protests", timespan="30d")
        ...     for point in timeline.points:
        ...         print(point.date, point.value)
    """

    BASE_URL = "https://api.gdeltproject.org/api/v2/doc/doc"

    async def _build_url(self, **kwargs: Any) -> str:
        """Build DOC API URL.

        The DOC API uses a fixed URL with all parameters passed as query strings.

        Args:
            **kwargs: Unused, but required by BaseEndpoint interface.

        Returns:
            Base URL for DOC API requests.
        """
        return self.BASE_URL

    def _build_params(self, query_filter: DocFilter) -> dict[str, str]:
        """
        Build query parameters from DocFilter.

        Converts the DocFilter model into URL query parameters expected by
        the DOC API, including proper format conversion for dates and
        mapping of sort values to API parameter names.

        Args:
            query_filter: Query filter with search parameters.

        Returns:
            Dict of query parameters ready for API request.

        Example:
            >>> filter = DocFilter(query="test", timespan="24h", sort_by="relevance")
            >>> endpoint = DocEndpoint()
            >>> params = endpoint._build_params(filter)
            >>> params["query"]
            'test'
            >>> params["sort"]
            'rel'
        """
        params: dict[str, str] = {
            "query": query_filter.query,
            "format": "json",
            "mode": query_filter.mode,
            "maxrecords": str(query_filter.max_results),
        }

        # Map sort values to API parameter names
        sort_map = {
            "date": "date",
            "relevance": "rel",
            "tone": "tonedesc",
        }
        params["sort"] = sort_map[query_filter.sort_by]

        # Time constraints - timespan takes precedence over datetime range
        if query_filter.timespan:
            params["timespan"] = query_filter.timespan
        elif query_filter.start_datetime:
            params["startdatetime"] = query_filter.start_datetime.strftime("%Y%m%d%H%M%S")
            if query_filter.end_datetime:
                params["enddatetime"] = query_filter.end_datetime.strftime("%Y%m%d%H%M%S")

        # Source filters
        if query_filter.source_language:
            params["sourcelang"] = query_filter.source_language
        if query_filter.source_country:
            params["sourcecountry"] = query_filter.source_country

        return params

    async def search(
        self,
        query: str,
        *,
        timespan: str | None = None,
        max_results: int = 250,
        sort_by: Literal["date", "relevance", "tone"] = "date",
        source_language: str | None = None,
        source_country: str | None = None,
    ) -> list[Article]:
        """
        Search for articles matching a query.

        This is a convenience method that constructs a DocFilter internally.
        For more control over query parameters, use query() with a DocFilter directly.

        Args:
            query: Search query string (supports boolean operators, phrases).
            timespan: Time range like "24h", "7d", "30d". If None, searches all time.
            max_results: Maximum results to return (1-250, default: 250).
            sort_by: Sort order - "date", "relevance", or "tone" (default: "date").
            source_language: Filter by source language (ISO 639 code).
            source_country: Filter by source country (FIPS country code).

        Returns:
            List of Article objects matching the query.

        Raises:
            APIError: On HTTP errors or invalid responses.
            APIUnavailableError: When API is down or unreachable.
            RateLimitError: When rate limited by the API.

        Example:
            >>> async with DocEndpoint() as doc:
            ...     # Search recent articles about climate
            ...     articles = await doc.search(
            ...         "climate change",
            ...         timespan="7d",
            ...         max_results=50,
            ...         sort_by="relevance"
            ...     )
            ...     # Filter by country
            ...     us_articles = await doc.search(
            ...         "elections",
            ...         source_country="US",
            ...         timespan="24h"
            ...     )
        """
        query_filter = DocFilter(
            query=query,
            timespan=timespan,
            max_results=max_results,
            sort_by=sort_by,
            source_language=source_language,
            source_country=source_country,
        )
        return await self.query(query_filter)

    async def query(self, query_filter: DocFilter) -> list[Article]:
        """
        Query the DOC API with a filter.

        Executes a search using a pre-configured DocFilter object, providing
        full control over all query parameters.

        Args:
            query_filter: DocFilter with query parameters and constraints.

        Returns:
            List of Article objects matching the filter criteria.

        Raises:
            APIError: On HTTP errors or invalid responses.
            APIUnavailableError: When API is down or unreachable.
            RateLimitError: When rate limited by the API.

        Example:
            >>> from py_gdelt.filters import DocFilter
            >>> from datetime import datetime
            >>> async with DocEndpoint() as doc:
            ...     # Complex query with datetime range
            ...     doc_filter = DocFilter(
            ...         query='"machine learning" AND python',
            ...         start_datetime=datetime(2024, 1, 1),
            ...         end_datetime=datetime(2024, 1, 31),
            ...         source_country="US",
            ...         max_results=100,
            ...         sort_by="relevance"
            ...     )
            ...     articles = await doc.query(doc_filter)
        """
        from py_gdelt.models.articles import Article

        params = self._build_params(query_filter)
        url = await self._build_url()

        data = await self._get_json(url, params=params)

        # Parse response - handle both empty and populated responses
        return [Article.model_validate(item) for item in data.get("articles", [])]

    async def timeline(
        self,
        query: str,
        *,
        timespan: str | None = "7d",
    ) -> Timeline:
        """
        Get timeline data for a query.

        Returns time series data showing article volume over time for a given
        search query. Useful for visualizing trends and tracking story evolution.

        Args:
            query: Search query string.
            timespan: Time range to analyze (default: "7d" - 7 days).
                     Common values: "24h", "7d", "30d", "3mon".

        Returns:
            Timeline object with time series data points.

        Raises:
            APIError: On HTTP errors or invalid responses.
            APIUnavailableError: When API is down or unreachable.
            RateLimitError: When rate limited by the API.

        Example:
            >>> async with DocEndpoint() as doc:
            ...     # Get article volume over last month
            ...     timeline = await doc.timeline("protests", timespan="30d")
            ...     for point in timeline.points:
            ...         print(f"{point.date}: {point.value} articles")
        """
        from py_gdelt.models.articles import Timeline

        query_filter = DocFilter(
            query=query,
            timespan=timespan,
            mode="timelinevol",  # GDELT API uses 'timelinevol', not 'timeline'
        )

        params = self._build_params(query_filter)
        url = await self._build_url()

        data = await self._get_json(url, params=params)
        return Timeline.model_validate(data)
```

#### `search(query, *, timespan=None, max_results=250, sort_by='date', source_language=None, source_country=None)`

Search for articles matching a query.

This is a convenience method that constructs a DocFilter internally. For more control over query parameters, use query() with a DocFilter directly.

Parameters:

| Name              | Type                                   | Description                                                    | Default                                                         |
| ----------------- | -------------------------------------- | -------------------------------------------------------------- | --------------------------------------------------------------- |
| `query`           | `str`                                  | Search query string (supports boolean operators, phrases).     | *required*                                                      |
| `timespan`        | \`str                                  | None\`                                                         | Time range like "24h", "7d", "30d". If None, searches all time. |
| `max_results`     | `int`                                  | Maximum results to return (1-250, default: 250).               | `250`                                                           |
| `sort_by`         | `Literal['date', 'relevance', 'tone']` | Sort order - "date", "relevance", or "tone" (default: "date"). | `'date'`                                                        |
| `source_language` | \`str                                  | None\`                                                         | Filter by source language (ISO 639 code).                       |
| `source_country`  | \`str                                  | None\`                                                         | Filter by source country (FIPS country code).                   |

Returns:

| Type            | Description                                 |
| --------------- | ------------------------------------------- |
| `list[Article]` | List of Article objects matching the query. |

Raises:

| Type                  | Description                          |
| --------------------- | ------------------------------------ |
| `APIError`            | On HTTP errors or invalid responses. |
| `APIUnavailableError` | When API is down or unreachable.     |
| `RateLimitError`      | When rate limited by the API.        |

Example

> > > async with DocEndpoint() as doc: ... # Search recent articles about climate ... articles = await doc.search( ... "climate change", ... timespan="7d", ... max_results=50, ... sort_by="relevance" ... ) ... # Filter by country ... us_articles = await doc.search( ... "elections", ... source_country="US", ... timespan="24h" ... )

Source code in `src/py_gdelt/endpoints/doc.py`

```
async def search(
    self,
    query: str,
    *,
    timespan: str | None = None,
    max_results: int = 250,
    sort_by: Literal["date", "relevance", "tone"] = "date",
    source_language: str | None = None,
    source_country: str | None = None,
) -> list[Article]:
    """
    Search for articles matching a query.

    This is a convenience method that constructs a DocFilter internally.
    For more control over query parameters, use query() with a DocFilter directly.

    Args:
        query: Search query string (supports boolean operators, phrases).
        timespan: Time range like "24h", "7d", "30d". If None, searches all time.
        max_results: Maximum results to return (1-250, default: 250).
        sort_by: Sort order - "date", "relevance", or "tone" (default: "date").
        source_language: Filter by source language (ISO 639 code).
        source_country: Filter by source country (FIPS country code).

    Returns:
        List of Article objects matching the query.

    Raises:
        APIError: On HTTP errors or invalid responses.
        APIUnavailableError: When API is down or unreachable.
        RateLimitError: When rate limited by the API.

    Example:
        >>> async with DocEndpoint() as doc:
        ...     # Search recent articles about climate
        ...     articles = await doc.search(
        ...         "climate change",
        ...         timespan="7d",
        ...         max_results=50,
        ...         sort_by="relevance"
        ...     )
        ...     # Filter by country
        ...     us_articles = await doc.search(
        ...         "elections",
        ...         source_country="US",
        ...         timespan="24h"
        ...     )
    """
    query_filter = DocFilter(
        query=query,
        timespan=timespan,
        max_results=max_results,
        sort_by=sort_by,
        source_language=source_language,
        source_country=source_country,
    )
    return await self.query(query_filter)
```

#### `query(query_filter)`

Query the DOC API with a filter.

Executes a search using a pre-configured DocFilter object, providing full control over all query parameters.

Parameters:

| Name           | Type        | Description                                      | Default    |
| -------------- | ----------- | ------------------------------------------------ | ---------- |
| `query_filter` | `DocFilter` | DocFilter with query parameters and constraints. | *required* |

Returns:

| Type            | Description                                           |
| --------------- | ----------------------------------------------------- |
| `list[Article]` | List of Article objects matching the filter criteria. |

Raises:

| Type                  | Description                          |
| --------------------- | ------------------------------------ |
| `APIError`            | On HTTP errors or invalid responses. |
| `APIUnavailableError` | When API is down or unreachable.     |
| `RateLimitError`      | When rate limited by the API.        |

Example

> > > from py_gdelt.filters import DocFilter from datetime import datetime async with DocEndpoint() as doc: ... # Complex query with datetime range ... doc_filter = DocFilter( ... query='"machine learning" AND python', ... start_datetime=datetime(2024, 1, 1), ... end_datetime=datetime(2024, 1, 31), ... source_country="US", ... max_results=100, ... sort_by="relevance" ... ) ... articles = await doc.query(doc_filter)

Source code in `src/py_gdelt/endpoints/doc.py`

```
async def query(self, query_filter: DocFilter) -> list[Article]:
    """
    Query the DOC API with a filter.

    Executes a search using a pre-configured DocFilter object, providing
    full control over all query parameters.

    Args:
        query_filter: DocFilter with query parameters and constraints.

    Returns:
        List of Article objects matching the filter criteria.

    Raises:
        APIError: On HTTP errors or invalid responses.
        APIUnavailableError: When API is down or unreachable.
        RateLimitError: When rate limited by the API.

    Example:
        >>> from py_gdelt.filters import DocFilter
        >>> from datetime import datetime
        >>> async with DocEndpoint() as doc:
        ...     # Complex query with datetime range
        ...     doc_filter = DocFilter(
        ...         query='"machine learning" AND python',
        ...         start_datetime=datetime(2024, 1, 1),
        ...         end_datetime=datetime(2024, 1, 31),
        ...         source_country="US",
        ...         max_results=100,
        ...         sort_by="relevance"
        ...     )
        ...     articles = await doc.query(doc_filter)
    """
    from py_gdelt.models.articles import Article

    params = self._build_params(query_filter)
    url = await self._build_url()

    data = await self._get_json(url, params=params)

    # Parse response - handle both empty and populated responses
    return [Article.model_validate(item) for item in data.get("articles", [])]
```

#### `timeline(query, *, timespan='7d')`

Get timeline data for a query.

Returns time series data showing article volume over time for a given search query. Useful for visualizing trends and tracking story evolution.

Parameters:

| Name       | Type  | Description          | Default                                                                                    |
| ---------- | ----- | -------------------- | ------------------------------------------------------------------------------------------ |
| `query`    | `str` | Search query string. | *required*                                                                                 |
| `timespan` | \`str | None\`               | Time range to analyze (default: "7d" - 7 days). Common values: "24h", "7d", "30d", "3mon". |

Returns:

| Type       | Description                                   |
| ---------- | --------------------------------------------- |
| `Timeline` | Timeline object with time series data points. |

Raises:

| Type                  | Description                          |
| --------------------- | ------------------------------------ |
| `APIError`            | On HTTP errors or invalid responses. |
| `APIUnavailableError` | When API is down or unreachable.     |
| `RateLimitError`      | When rate limited by the API.        |

Example

> > > async with DocEndpoint() as doc: ... # Get article volume over last month ... timeline = await doc.timeline("protests", timespan="30d") ... for point in timeline.points: ... print(f"{point.date}: {point.value} articles")

Source code in `src/py_gdelt/endpoints/doc.py`

```
async def timeline(
    self,
    query: str,
    *,
    timespan: str | None = "7d",
) -> Timeline:
    """
    Get timeline data for a query.

    Returns time series data showing article volume over time for a given
    search query. Useful for visualizing trends and tracking story evolution.

    Args:
        query: Search query string.
        timespan: Time range to analyze (default: "7d" - 7 days).
                 Common values: "24h", "7d", "30d", "3mon".

    Returns:
        Timeline object with time series data points.

    Raises:
        APIError: On HTTP errors or invalid responses.
        APIUnavailableError: When API is down or unreachable.
        RateLimitError: When rate limited by the API.

    Example:
        >>> async with DocEndpoint() as doc:
        ...     # Get article volume over last month
        ...     timeline = await doc.timeline("protests", timespan="30d")
        ...     for point in timeline.points:
        ...         print(f"{point.date}: {point.value} articles")
    """
    from py_gdelt.models.articles import Timeline

    query_filter = DocFilter(
        query=query,
        timespan=timespan,
        mode="timelinevol",  # GDELT API uses 'timelinevol', not 'timeline'
    )

    params = self._build_params(query_filter)
    url = await self._build_url()

    data = await self._get_json(url, params=params)
    return Timeline.model_validate(data)
```

### GeoEndpoint

### `GeoEndpoint`

Bases: `BaseEndpoint`

GEO 2.0 API endpoint for geographic article data.

Returns locations mentioned in news articles matching a query. Supports time-based filtering and geographic bounds.

Example

async with GeoEndpoint() as geo: result = await geo.search("earthquake", max_points=100) for point in result.points: print(f"{point.name}: {point.count} articles")

Attributes:

| Name       | Type | Description      |
| ---------- | ---- | ---------------- |
| `BASE_URL` |      | GEO API base URL |

Source code in `src/py_gdelt/endpoints/geo.py`

```
class GeoEndpoint(BaseEndpoint):
    """GEO 2.0 API endpoint for geographic article data.

    Returns locations mentioned in news articles matching a query.
    Supports time-based filtering and geographic bounds.

    Example:
        async with GeoEndpoint() as geo:
            result = await geo.search("earthquake", max_points=100)
            for point in result.points:
                print(f"{point.name}: {point.count} articles")

    Attributes:
        BASE_URL: GEO API base URL
    """

    BASE_URL = "https://api.gdeltproject.org/api/v2/geo/geo"

    async def _build_url(self, **kwargs: Any) -> str:
        """Build GEO API URL.

        The GEO API uses a fixed base URL with query parameters.

        Args:
            **kwargs: Unused, kept for BaseEndpoint compatibility

        Returns:
            Base URL for GEO API
        """
        return self.BASE_URL

    def _build_params(self, query_filter: GeoFilter) -> dict[str, str]:
        """Build query parameters from GeoFilter.

        Args:
            query_filter: GeoFilter with query parameters

        Returns:
            Dict of URL query parameters
        """
        params: dict[str, str] = {
            "query": query_filter.query,
            "format": "GeoJSON",  # GDELT GEO API requires exact case
            "maxpoints": str(query_filter.max_results),
        }

        if query_filter.timespan:
            params["timespan"] = query_filter.timespan

        # Add bounding box if provided (format: lon1,lat1,lon2,lat2)
        if query_filter.bounding_box:
            min_lat, min_lon, max_lat, max_lon = query_filter.bounding_box
            params["BBOX"] = f"{min_lon},{min_lat},{max_lon},{max_lat}"

        return params

    async def search(
        self,
        query: str,
        *,
        timespan: str | None = None,
        max_points: int = 250,
        bounding_box: tuple[float, float, float, float] | None = None,
    ) -> GeoResult:
        """Search for geographic locations in news.

        Args:
            query: Search query (full text search)
            timespan: Time range (e.g., "24h", "7d", "1m")
            max_points: Maximum points to return (1-250)
            bounding_box: Optional (min_lat, min_lon, max_lat, max_lon)

        Returns:
            GeoResult with list of GeoPoints

        Example:
            async with GeoEndpoint() as geo:
                result = await geo.search(
                    "earthquake",
                    timespan="7d",
                    max_points=50
                )
                print(f"Found {len(result.points)} locations")
        """
        query_filter = GeoFilter(
            query=query,
            timespan=timespan,
            max_results=min(max_points, 250),  # Cap at filter max
            bounding_box=bounding_box,
        )
        return await self.query(query_filter)

    async def query(self, query_filter: GeoFilter) -> GeoResult:
        """Query the GEO API with a filter.

        Args:
            query_filter: GeoFilter with query parameters

        Returns:
            GeoResult containing geographic points

        Raises:
            APIError: On request failure
            RateLimitError: On rate limit
            APIUnavailableError: On server error
        """
        params = self._build_params(query_filter)
        url = await self._build_url()

        data = await self._get_json(url, params=params)

        # Parse GeoJSON features or raw points
        points: list[GeoPoint] = []

        if "features" in data:
            # GeoJSON format
            for feature in data["features"]:
                coords = feature.get("geometry", {}).get("coordinates", [])
                props = feature.get("properties", {})
                if len(coords) >= 2:
                    points.append(
                        GeoPoint(
                            lon=coords[0],
                            lat=coords[1],
                            name=props.get("name"),
                            count=props.get("count", 1),
                            url=props.get("url"),
                        ),
                    )
        elif "points" in data:
            # Plain JSON format
            points.extend([GeoPoint.model_validate(item) for item in data["points"]])

        return GeoResult(
            points=points,
            total_count=data.get("count", len(points)),
        )

    async def to_geojson(
        self,
        query: str,
        *,
        timespan: str | None = None,
        max_points: int = 250,
    ) -> dict[str, Any]:
        """Get raw GeoJSON response.

        Useful for direct use with mapping libraries (Leaflet, Folium, etc).

        Args:
            query: Search query
            timespan: Time range (e.g., "24h", "7d")
            max_points: Maximum points (1-250)

        Returns:
            Raw GeoJSON dict (FeatureCollection)

        Example:
            async with GeoEndpoint() as geo:
                geojson = await geo.to_geojson("climate change", timespan="30d")
                # Pass directly to mapping library
                folium.GeoJson(geojson).add_to(map)
        """
        query_filter = GeoFilter(
            query=query,
            timespan=timespan,
            max_results=min(max_points, 250),
        )

        params = self._build_params(query_filter)
        params["format"] = "geojson"
        url = await self._build_url()

        result = await self._get_json(url, params=params)
        return cast("dict[str, Any]", result)
```

#### `search(query, *, timespan=None, max_points=250, bounding_box=None)`

Search for geographic locations in news.

Parameters:

| Name           | Type                                | Description                      | Default                                       |
| -------------- | ----------------------------------- | -------------------------------- | --------------------------------------------- |
| `query`        | `str`                               | Search query (full text search)  | *required*                                    |
| `timespan`     | \`str                               | None\`                           | Time range (e.g., "24h", "7d", "1m")          |
| `max_points`   | `int`                               | Maximum points to return (1-250) | `250`                                         |
| `bounding_box` | \`tuple[float, float, float, float] | None\`                           | Optional (min_lat, min_lon, max_lat, max_lon) |

Returns:

| Type        | Description                      |
| ----------- | -------------------------------- |
| `GeoResult` | GeoResult with list of GeoPoints |

Example

async with GeoEndpoint() as geo: result = await geo.search( "earthquake", timespan="7d", max_points=50 ) print(f"Found {len(result.points)} locations")

Source code in `src/py_gdelt/endpoints/geo.py`

```
async def search(
    self,
    query: str,
    *,
    timespan: str | None = None,
    max_points: int = 250,
    bounding_box: tuple[float, float, float, float] | None = None,
) -> GeoResult:
    """Search for geographic locations in news.

    Args:
        query: Search query (full text search)
        timespan: Time range (e.g., "24h", "7d", "1m")
        max_points: Maximum points to return (1-250)
        bounding_box: Optional (min_lat, min_lon, max_lat, max_lon)

    Returns:
        GeoResult with list of GeoPoints

    Example:
        async with GeoEndpoint() as geo:
            result = await geo.search(
                "earthquake",
                timespan="7d",
                max_points=50
            )
            print(f"Found {len(result.points)} locations")
    """
    query_filter = GeoFilter(
        query=query,
        timespan=timespan,
        max_results=min(max_points, 250),  # Cap at filter max
        bounding_box=bounding_box,
    )
    return await self.query(query_filter)
```

#### `query(query_filter)`

Query the GEO API with a filter.

Parameters:

| Name           | Type        | Description                     | Default    |
| -------------- | ----------- | ------------------------------- | ---------- |
| `query_filter` | `GeoFilter` | GeoFilter with query parameters | *required* |

Returns:

| Type        | Description                            |
| ----------- | -------------------------------------- |
| `GeoResult` | GeoResult containing geographic points |

Raises:

| Type                  | Description        |
| --------------------- | ------------------ |
| `APIError`            | On request failure |
| `RateLimitError`      | On rate limit      |
| `APIUnavailableError` | On server error    |

Source code in `src/py_gdelt/endpoints/geo.py`

```
async def query(self, query_filter: GeoFilter) -> GeoResult:
    """Query the GEO API with a filter.

    Args:
        query_filter: GeoFilter with query parameters

    Returns:
        GeoResult containing geographic points

    Raises:
        APIError: On request failure
        RateLimitError: On rate limit
        APIUnavailableError: On server error
    """
    params = self._build_params(query_filter)
    url = await self._build_url()

    data = await self._get_json(url, params=params)

    # Parse GeoJSON features or raw points
    points: list[GeoPoint] = []

    if "features" in data:
        # GeoJSON format
        for feature in data["features"]:
            coords = feature.get("geometry", {}).get("coordinates", [])
            props = feature.get("properties", {})
            if len(coords) >= 2:
                points.append(
                    GeoPoint(
                        lon=coords[0],
                        lat=coords[1],
                        name=props.get("name"),
                        count=props.get("count", 1),
                        url=props.get("url"),
                    ),
                )
    elif "points" in data:
        # Plain JSON format
        points.extend([GeoPoint.model_validate(item) for item in data["points"]])

    return GeoResult(
        points=points,
        total_count=data.get("count", len(points)),
    )
```

#### `to_geojson(query, *, timespan=None, max_points=250)`

Get raw GeoJSON response.

Useful for direct use with mapping libraries (Leaflet, Folium, etc).

Parameters:

| Name         | Type  | Description            | Default                        |
| ------------ | ----- | ---------------------- | ------------------------------ |
| `query`      | `str` | Search query           | *required*                     |
| `timespan`   | \`str | None\`                 | Time range (e.g., "24h", "7d") |
| `max_points` | `int` | Maximum points (1-250) | `250`                          |

Returns:

| Type             | Description                          |
| ---------------- | ------------------------------------ |
| `dict[str, Any]` | Raw GeoJSON dict (FeatureCollection) |

Example

async with GeoEndpoint() as geo: geojson = await geo.to_geojson("climate change", timespan="30d")

# Pass directly to mapping library

folium.GeoJson(geojson).add_to(map)

Source code in `src/py_gdelt/endpoints/geo.py`

```
async def to_geojson(
    self,
    query: str,
    *,
    timespan: str | None = None,
    max_points: int = 250,
) -> dict[str, Any]:
    """Get raw GeoJSON response.

    Useful for direct use with mapping libraries (Leaflet, Folium, etc).

    Args:
        query: Search query
        timespan: Time range (e.g., "24h", "7d")
        max_points: Maximum points (1-250)

    Returns:
        Raw GeoJSON dict (FeatureCollection)

    Example:
        async with GeoEndpoint() as geo:
            geojson = await geo.to_geojson("climate change", timespan="30d")
            # Pass directly to mapping library
            folium.GeoJson(geojson).add_to(map)
    """
    query_filter = GeoFilter(
        query=query,
        timespan=timespan,
        max_results=min(max_points, 250),
    )

    params = self._build_params(query_filter)
    params["format"] = "geojson"
    url = await self._build_url()

    result = await self._get_json(url, params=params)
    return cast("dict[str, Any]", result)
```

### ContextEndpoint

### `ContextEndpoint`

Bases: `BaseEndpoint`

Context 2.0 API endpoint for contextual analysis.

Provides contextual information about search terms including related themes, entities, and sentiment analysis.

Attributes:

| Name       | Type | Description                           |
| ---------- | ---- | ------------------------------------- |
| `BASE_URL` |      | Base URL for the Context API endpoint |

Example

async with ContextEndpoint() as ctx: result = await ctx.analyze("climate change") for theme in result.themes\[:5\]: print(f"{theme.theme}: {theme.count} mentions")

Source code in `src/py_gdelt/endpoints/context.py`

```
class ContextEndpoint(BaseEndpoint):
    """Context 2.0 API endpoint for contextual analysis.

    Provides contextual information about search terms including
    related themes, entities, and sentiment analysis.

    Attributes:
        BASE_URL: Base URL for the Context API endpoint

    Example:
        async with ContextEndpoint() as ctx:
            result = await ctx.analyze("climate change")
            for theme in result.themes[:5]:
                print(f"{theme.theme}: {theme.count} mentions")
    """

    BASE_URL = "https://api.gdeltproject.org/api/v2/context/context"

    async def __aenter__(self) -> ContextEndpoint:
        """Async context manager entry.

        Returns:
            Self for use in async with statement.
        """
        await super().__aenter__()
        return self

    async def _build_url(self, **kwargs: Any) -> str:
        """Build Context API URL.

        The Context API uses a single endpoint URL with query parameters.

        Args:
            **kwargs: Unused, included for BaseEndpoint compatibility.

        Returns:
            Base URL for Context API.
        """
        return self.BASE_URL

    def _build_params(
        self,
        query: str,
        timespan: str | None = None,
    ) -> dict[str, str]:
        """Build query parameters for Context API request.

        Args:
            query: Search term to analyze
            timespan: Time range (e.g., "24h", "7d", "30d")

        Returns:
            Dictionary of query parameters for the API request.
        """
        params: dict[str, str] = {
            "query": query,
            "format": "json",
            "mode": "artlist",  # GDELT Context API only supports 'artlist' mode
        }

        if timespan:
            params["timespan"] = timespan

        return params

    async def analyze(
        self,
        query: str,
        *,
        timespan: str | None = None,
    ) -> ContextResult:
        """Get contextual analysis for a search term.

        Retrieves comprehensive contextual information including themes, entities,
        tone analysis, and related queries for the specified search term.

        Args:
            query: Search term to analyze
            timespan: Time range (e.g., "24h", "7d", "30d")

        Returns:
            ContextResult with themes, entities, and tone analysis

        Raises:
            RateLimitError: On 429 response
            APIUnavailableError: On 5xx response or connection error
            APIError: On other HTTP errors or invalid JSON
        """
        params = self._build_params(query, timespan)
        url = await self._build_url()

        data = await self._get_json(url, params=params)

        # Parse themes
        themes: list[ContextTheme] = [
            ContextTheme(
                theme=item.get("theme", ""),
                count=item.get("count", 0),
                score=item.get("score"),
            )
            for item in data.get("themes", [])
        ]

        # Parse entities
        entities: list[ContextEntity] = [
            ContextEntity(
                name=item.get("name", ""),
                entity_type=item.get("type", "UNKNOWN"),
                count=item.get("count", 0),
            )
            for item in data.get("entities", [])
        ]

        # Parse tone
        tone: ContextTone | None = None
        if "tone" in data:
            t = data["tone"]
            tone = ContextTone(
                average_tone=t.get("average", 0.0),
                positive_count=t.get("positive", 0),
                negative_count=t.get("negative", 0),
                neutral_count=t.get("neutral", 0),
            )

        # Parse related queries
        related = data.get("related_queries", [])
        related_queries = [str(q) for q in related] if isinstance(related, list) else []

        return ContextResult(
            query=query,
            article_count=data.get("article_count", 0),
            themes=themes,
            entities=entities,
            tone=tone,
            related_queries=related_queries,
        )

    async def get_themes(
        self,
        query: str,
        *,
        timespan: str | None = None,
        limit: int = 10,
    ) -> list[ContextTheme]:
        """Get top themes for a search term.

        Convenience method that returns just themes sorted by count.

        Args:
            query: Search term
            timespan: Time range
            limit: Max themes to return

        Returns:
            List of top themes sorted by count (descending)

        Raises:
            RateLimitError: On 429 response
            APIUnavailableError: On 5xx response or connection error
            APIError: On other HTTP errors or invalid JSON
        """
        result = await self.analyze(query, timespan=timespan)
        sorted_themes = sorted(result.themes, key=lambda t: t.count, reverse=True)
        return sorted_themes[:limit]

    async def get_entities(
        self,
        query: str,
        *,
        timespan: str | None = None,
        entity_type: str | None = None,
        limit: int = 10,
    ) -> list[ContextEntity]:
        """Get top entities for a search term.

        Convenience method that returns entities, optionally filtered by type
        and sorted by count.

        Args:
            query: Search term
            timespan: Time range
            entity_type: Filter by type (PERSON, ORG, LOCATION)
            limit: Max entities to return

        Returns:
            List of top entities sorted by count (descending)

        Raises:
            RateLimitError: On 429 response
            APIUnavailableError: On 5xx response or connection error
            APIError: On other HTTP errors or invalid JSON
        """
        result = await self.analyze(query, timespan=timespan)

        entities = result.entities
        if entity_type:
            entities = [e for e in entities if e.entity_type == entity_type]

        sorted_entities = sorted(entities, key=lambda e: e.count, reverse=True)
        return sorted_entities[:limit]
```

#### `__aenter__()`

Async context manager entry.

Returns:

| Type              | Description                           |
| ----------------- | ------------------------------------- |
| `ContextEndpoint` | Self for use in async with statement. |

Source code in `src/py_gdelt/endpoints/context.py`

```
async def __aenter__(self) -> ContextEndpoint:
    """Async context manager entry.

    Returns:
        Self for use in async with statement.
    """
    await super().__aenter__()
    return self
```

#### `analyze(query, *, timespan=None)`

Get contextual analysis for a search term.

Retrieves comprehensive contextual information including themes, entities, tone analysis, and related queries for the specified search term.

Parameters:

| Name       | Type  | Description            | Default                               |
| ---------- | ----- | ---------------------- | ------------------------------------- |
| `query`    | `str` | Search term to analyze | *required*                            |
| `timespan` | \`str | None\`                 | Time range (e.g., "24h", "7d", "30d") |

Returns:

| Type            | Description                                            |
| --------------- | ------------------------------------------------------ |
| `ContextResult` | ContextResult with themes, entities, and tone analysis |

Raises:

| Type                  | Description                          |
| --------------------- | ------------------------------------ |
| `RateLimitError`      | On 429 response                      |
| `APIUnavailableError` | On 5xx response or connection error  |
| `APIError`            | On other HTTP errors or invalid JSON |

Source code in `src/py_gdelt/endpoints/context.py`

```
async def analyze(
    self,
    query: str,
    *,
    timespan: str | None = None,
) -> ContextResult:
    """Get contextual analysis for a search term.

    Retrieves comprehensive contextual information including themes, entities,
    tone analysis, and related queries for the specified search term.

    Args:
        query: Search term to analyze
        timespan: Time range (e.g., "24h", "7d", "30d")

    Returns:
        ContextResult with themes, entities, and tone analysis

    Raises:
        RateLimitError: On 429 response
        APIUnavailableError: On 5xx response or connection error
        APIError: On other HTTP errors or invalid JSON
    """
    params = self._build_params(query, timespan)
    url = await self._build_url()

    data = await self._get_json(url, params=params)

    # Parse themes
    themes: list[ContextTheme] = [
        ContextTheme(
            theme=item.get("theme", ""),
            count=item.get("count", 0),
            score=item.get("score"),
        )
        for item in data.get("themes", [])
    ]

    # Parse entities
    entities: list[ContextEntity] = [
        ContextEntity(
            name=item.get("name", ""),
            entity_type=item.get("type", "UNKNOWN"),
            count=item.get("count", 0),
        )
        for item in data.get("entities", [])
    ]

    # Parse tone
    tone: ContextTone | None = None
    if "tone" in data:
        t = data["tone"]
        tone = ContextTone(
            average_tone=t.get("average", 0.0),
            positive_count=t.get("positive", 0),
            negative_count=t.get("negative", 0),
            neutral_count=t.get("neutral", 0),
        )

    # Parse related queries
    related = data.get("related_queries", [])
    related_queries = [str(q) for q in related] if isinstance(related, list) else []

    return ContextResult(
        query=query,
        article_count=data.get("article_count", 0),
        themes=themes,
        entities=entities,
        tone=tone,
        related_queries=related_queries,
    )
```

#### `get_themes(query, *, timespan=None, limit=10)`

Get top themes for a search term.

Convenience method that returns just themes sorted by count.

Parameters:

| Name       | Type  | Description          | Default    |
| ---------- | ----- | -------------------- | ---------- |
| `query`    | `str` | Search term          | *required* |
| `timespan` | \`str | None\`               | Time range |
| `limit`    | `int` | Max themes to return | `10`       |

Returns:

| Type                 | Description                                     |
| -------------------- | ----------------------------------------------- |
| `list[ContextTheme]` | List of top themes sorted by count (descending) |

Raises:

| Type                  | Description                          |
| --------------------- | ------------------------------------ |
| `RateLimitError`      | On 429 response                      |
| `APIUnavailableError` | On 5xx response or connection error  |
| `APIError`            | On other HTTP errors or invalid JSON |

Source code in `src/py_gdelt/endpoints/context.py`

```
async def get_themes(
    self,
    query: str,
    *,
    timespan: str | None = None,
    limit: int = 10,
) -> list[ContextTheme]:
    """Get top themes for a search term.

    Convenience method that returns just themes sorted by count.

    Args:
        query: Search term
        timespan: Time range
        limit: Max themes to return

    Returns:
        List of top themes sorted by count (descending)

    Raises:
        RateLimitError: On 429 response
        APIUnavailableError: On 5xx response or connection error
        APIError: On other HTTP errors or invalid JSON
    """
    result = await self.analyze(query, timespan=timespan)
    sorted_themes = sorted(result.themes, key=lambda t: t.count, reverse=True)
    return sorted_themes[:limit]
```

#### `get_entities(query, *, timespan=None, entity_type=None, limit=10)`

Get top entities for a search term.

Convenience method that returns entities, optionally filtered by type and sorted by count.

Parameters:

| Name          | Type  | Description            | Default                                |
| ------------- | ----- | ---------------------- | -------------------------------------- |
| `query`       | `str` | Search term            | *required*                             |
| `timespan`    | \`str | None\`                 | Time range                             |
| `entity_type` | \`str | None\`                 | Filter by type (PERSON, ORG, LOCATION) |
| `limit`       | `int` | Max entities to return | `10`                                   |

Returns:

| Type                  | Description                                       |
| --------------------- | ------------------------------------------------- |
| `list[ContextEntity]` | List of top entities sorted by count (descending) |

Raises:

| Type                  | Description                          |
| --------------------- | ------------------------------------ |
| `RateLimitError`      | On 429 response                      |
| `APIUnavailableError` | On 5xx response or connection error  |
| `APIError`            | On other HTTP errors or invalid JSON |

Source code in `src/py_gdelt/endpoints/context.py`

```
async def get_entities(
    self,
    query: str,
    *,
    timespan: str | None = None,
    entity_type: str | None = None,
    limit: int = 10,
) -> list[ContextEntity]:
    """Get top entities for a search term.

    Convenience method that returns entities, optionally filtered by type
    and sorted by count.

    Args:
        query: Search term
        timespan: Time range
        entity_type: Filter by type (PERSON, ORG, LOCATION)
        limit: Max entities to return

    Returns:
        List of top entities sorted by count (descending)

    Raises:
        RateLimitError: On 429 response
        APIUnavailableError: On 5xx response or connection error
        APIError: On other HTTP errors or invalid JSON
    """
    result = await self.analyze(query, timespan=timespan)

    entities = result.entities
    if entity_type:
        entities = [e for e in entities if e.entity_type == entity_type]

    sorted_entities = sorted(entities, key=lambda e: e.count, reverse=True)
    return sorted_entities[:limit]
```

### TVEndpoint

### `TVEndpoint`

Bases: `BaseEndpoint`

TV API endpoint for television news monitoring.

Searches transcripts from major US television networks including CNN, Fox News, MSNBC, and others. Provides three query modes:

- Clip gallery: Individual video clips matching query
- Timeline: Time series of mention frequency
- Station chart: Breakdown by network

The endpoint handles date formatting, parameter building, and response parsing automatically.

Attributes:

| Name       | Type | Description                     |
| ---------- | ---- | ------------------------------- |
| `BASE_URL` |      | API endpoint URL for TV queries |

Example

async with TVEndpoint() as tv: clips = await tv.search("election", station="CNN") for clip in clips: print(f"{clip.show_name}: {clip.snippet}")

Source code in `src/py_gdelt/endpoints/tv.py`

```
class TVEndpoint(BaseEndpoint):
    """TV API endpoint for television news monitoring.

    Searches transcripts from major US television networks including CNN,
    Fox News, MSNBC, and others. Provides three query modes:
    - Clip gallery: Individual video clips matching query
    - Timeline: Time series of mention frequency
    - Station chart: Breakdown by network

    The endpoint handles date formatting, parameter building, and response
    parsing automatically.

    Attributes:
        BASE_URL: API endpoint URL for TV queries

    Example:
        async with TVEndpoint() as tv:
            clips = await tv.search("election", station="CNN")
            for clip in clips:
                print(f"{clip.show_name}: {clip.snippet}")
    """

    BASE_URL = "https://api.gdeltproject.org/api/v2/tv/tv"

    async def _build_url(self, **kwargs: Any) -> str:
        """Build the request URL.

        TV API uses a fixed URL with query parameters.

        Args:
            **kwargs: Unused, but required by BaseEndpoint interface.

        Returns:
            The base TV API URL.
        """
        return self.BASE_URL

    def _build_params(self, query_filter: TVFilter) -> dict[str, str]:
        """Build query parameters from TVFilter.

        Constructs query parameters for the TV API from a TVFilter object.
        Handles both timespan and datetime range parameters, station/market
        filtering, and output mode selection.

        Note: GDELT TV API requires station to be in the query string itself
        (e.g., "election station:CNN") rather than as a separate parameter.

        Args:
            query_filter: Validated TV filter object

        Returns:
            Dictionary of query parameters ready for HTTP request
        """
        # Build query string - GDELT TV API requires station in query
        query = query_filter.query
        if query_filter.station:
            query = f"{query} station:{query_filter.station}"
        if query_filter.market:
            query = f"{query} market:{query_filter.market}"

        params: dict[str, str] = {
            "query": query,
            "format": "json",
            "mode": query_filter.mode,
            "maxrecords": str(query_filter.max_results),
        }

        # Convert timespan to explicit datetime range (GDELT TV API TIMESPAN is unreliable)
        if query_filter.timespan:
            delta = _parse_timespan(query_filter.timespan)
            if delta:
                end_dt = datetime.now(UTC)
                start_dt = end_dt - delta
                params["STARTDATETIME"] = start_dt.strftime("%Y%m%d%H%M%S")
                params["ENDDATETIME"] = end_dt.strftime("%Y%m%d%H%M%S")
        elif query_filter.start_datetime:
            params["STARTDATETIME"] = query_filter.start_datetime.strftime("%Y%m%d%H%M%S")
            if query_filter.end_datetime:
                params["ENDDATETIME"] = query_filter.end_datetime.strftime("%Y%m%d%H%M%S")

        return params

    async def search(
        self,
        query: str,
        *,
        timespan: str | None = None,
        start_datetime: datetime | None = None,
        end_datetime: datetime | None = None,
        station: str | None = None,
        market: str | None = None,
        max_results: int = 250,
    ) -> list[TVClip]:
        """Search TV transcripts for clips.

        Searches television news transcripts and returns matching video clips
        with metadata and text excerpts.

        Args:
            query: Search query (keywords, phrases, or boolean expressions)
            timespan: Time range (e.g., "24h", "7d", "30d")
            start_datetime: Start of date range (alternative to timespan)
            end_datetime: End of date range (alternative to timespan)
            station: Filter by station (CNN, FOXNEWS, MSNBC, etc.)
            market: Filter by market (National, Philadelphia, etc.)
            max_results: Maximum clips to return (1-250)

        Returns:
            List of TVClip objects matching the query

        Raises:
            APIError: If the API returns an error
            RateLimitError: If rate limit is exceeded
            APIUnavailableError: If the API is unavailable

        Example:
            clips = await tv.search("climate change", station="CNN", timespan="7d")
        """
        query_filter = TVFilter(
            query=query,
            timespan=timespan,
            start_datetime=start_datetime,
            end_datetime=end_datetime,
            station=station,
            market=market,
            max_results=max_results,
            mode="ClipGallery",
        )
        return await self.query_clips(query_filter)

    async def query_clips(self, query_filter: TVFilter) -> list[TVClip]:
        """Query for TV clips with a filter.

        Lower-level method that accepts a TVFilter object for more control
        over query parameters.

        Args:
            query_filter: TVFilter object with query parameters

        Returns:
            List of TVClip objects

        Raises:
            APIError: If the API returns an error
            RateLimitError: If rate limit is exceeded
            APIUnavailableError: If the API is unavailable
        """
        params = self._build_params(query_filter)
        params["mode"] = "ClipGallery"
        url = await self._build_url()

        data = await self._get_json(url, params=params)

        clips: list[TVClip] = [
            TVClip(
                station=item.get("station", ""),
                show_name=item.get("show"),
                clip_url=item.get("url"),
                preview_url=item.get("preview"),
                date=try_parse_gdelt_datetime(item.get("date")),
                duration_seconds=item.get("duration"),
                snippet=item.get("snippet"),
            )
            for item in data.get("clips", [])
        ]

        return clips

    async def timeline(
        self,
        query: str,
        *,
        timespan: str | None = "7d",
        start_datetime: datetime | None = None,
        end_datetime: datetime | None = None,
        station: str | None = None,
    ) -> TVTimeline:
        """Get timeline of TV mentions.

        Returns a time series showing when a topic was mentioned on television,
        useful for tracking coverage patterns over time.

        Args:
            query: Search query
            timespan: Time range (default: "7d")
            start_datetime: Start of date range (alternative to timespan)
            end_datetime: End of date range (alternative to timespan)
            station: Optional station filter

        Returns:
            TVTimeline with time series data

        Raises:
            APIError: If the API returns an error
            RateLimitError: If rate limit is exceeded
            APIUnavailableError: If the API is unavailable

        Example:
            timeline = await tv.timeline("election", timespan="30d")
            for point in timeline.points:
                print(f"{point.date}: {point.count} mentions")
        """
        query_filter = TVFilter(
            query=query,
            timespan=timespan if not start_datetime else None,
            start_datetime=start_datetime,
            end_datetime=end_datetime,
            station=station,
            mode="TimelineVol",
        )

        params = self._build_params(query_filter)
        url = await self._build_url()

        data = await self._get_json(url, params=params)

        points: list[TVTimelinePoint] = [
            TVTimelinePoint(
                date=item.get("date", ""),
                station=item.get("station"),
                count=item.get("count", 0),
            )
            for item in data.get("timeline", [])
        ]

        return TVTimeline(points=points)

    async def station_chart(
        self,
        query: str,
        *,
        timespan: str | None = "7d",
        start_datetime: datetime | None = None,
        end_datetime: datetime | None = None,
    ) -> TVStationChart:
        """Get station comparison chart.

        Shows which stations covered a topic the most, useful for understanding
        which networks are focusing on particular stories.

        Args:
            query: Search query
            timespan: Time range (default: "7d")
            start_datetime: Start of date range (alternative to timespan)
            end_datetime: End of date range (alternative to timespan)

        Returns:
            TVStationChart with station breakdown

        Raises:
            APIError: If the API returns an error
            RateLimitError: If rate limit is exceeded
            APIUnavailableError: If the API is unavailable

        Example:
            chart = await tv.station_chart("healthcare")
            for station in chart.stations:
                print(f"{station.station}: {station.percentage}%")
        """
        query_filter = TVFilter(
            query=query,
            timespan=timespan if not start_datetime else None,
            start_datetime=start_datetime,
            end_datetime=end_datetime,
            mode="StationChart",
        )

        params = self._build_params(query_filter)
        url = await self._build_url()

        data = await self._get_json(url, params=params)

        stations: list[TVStationData] = []
        if "stations" in data:
            total = sum(s.get("count", 0) for s in data["stations"])
            for item in data["stations"]:
                count = item.get("count", 0)
                stations.append(
                    TVStationData(
                        station=item.get("station", ""),
                        count=count,
                        percentage=count / total * 100 if total > 0 else None,
                    ),
                )

        return TVStationChart(stations=stations)
```

#### `search(query, *, timespan=None, start_datetime=None, end_datetime=None, station=None, market=None, max_results=250)`

Search TV transcripts for clips.

Searches television news transcripts and returns matching video clips with metadata and text excerpts.

Parameters:

| Name             | Type       | Description                                              | Default                                         |
| ---------------- | ---------- | -------------------------------------------------------- | ----------------------------------------------- |
| `query`          | `str`      | Search query (keywords, phrases, or boolean expressions) | *required*                                      |
| `timespan`       | \`str      | None\`                                                   | Time range (e.g., "24h", "7d", "30d")           |
| `start_datetime` | \`datetime | None\`                                                   | Start of date range (alternative to timespan)   |
| `end_datetime`   | \`datetime | None\`                                                   | End of date range (alternative to timespan)     |
| `station`        | \`str      | None\`                                                   | Filter by station (CNN, FOXNEWS, MSNBC, etc.)   |
| `market`         | \`str      | None\`                                                   | Filter by market (National, Philadelphia, etc.) |
| `max_results`    | `int`      | Maximum clips to return (1-250)                          | `250`                                           |

Returns:

| Type           | Description                               |
| -------------- | ----------------------------------------- |
| `list[TVClip]` | List of TVClip objects matching the query |

Raises:

| Type                  | Description                 |
| --------------------- | --------------------------- |
| `APIError`            | If the API returns an error |
| `RateLimitError`      | If rate limit is exceeded   |
| `APIUnavailableError` | If the API is unavailable   |

Example

clips = await tv.search("climate change", station="CNN", timespan="7d")

Source code in `src/py_gdelt/endpoints/tv.py`

```
async def search(
    self,
    query: str,
    *,
    timespan: str | None = None,
    start_datetime: datetime | None = None,
    end_datetime: datetime | None = None,
    station: str | None = None,
    market: str | None = None,
    max_results: int = 250,
) -> list[TVClip]:
    """Search TV transcripts for clips.

    Searches television news transcripts and returns matching video clips
    with metadata and text excerpts.

    Args:
        query: Search query (keywords, phrases, or boolean expressions)
        timespan: Time range (e.g., "24h", "7d", "30d")
        start_datetime: Start of date range (alternative to timespan)
        end_datetime: End of date range (alternative to timespan)
        station: Filter by station (CNN, FOXNEWS, MSNBC, etc.)
        market: Filter by market (National, Philadelphia, etc.)
        max_results: Maximum clips to return (1-250)

    Returns:
        List of TVClip objects matching the query

    Raises:
        APIError: If the API returns an error
        RateLimitError: If rate limit is exceeded
        APIUnavailableError: If the API is unavailable

    Example:
        clips = await tv.search("climate change", station="CNN", timespan="7d")
    """
    query_filter = TVFilter(
        query=query,
        timespan=timespan,
        start_datetime=start_datetime,
        end_datetime=end_datetime,
        station=station,
        market=market,
        max_results=max_results,
        mode="ClipGallery",
    )
    return await self.query_clips(query_filter)
```

#### `query_clips(query_filter)`

Query for TV clips with a filter.

Lower-level method that accepts a TVFilter object for more control over query parameters.

Parameters:

| Name           | Type       | Description                           | Default    |
| -------------- | ---------- | ------------------------------------- | ---------- |
| `query_filter` | `TVFilter` | TVFilter object with query parameters | *required* |

Returns:

| Type           | Description            |
| -------------- | ---------------------- |
| `list[TVClip]` | List of TVClip objects |

Raises:

| Type                  | Description                 |
| --------------------- | --------------------------- |
| `APIError`            | If the API returns an error |
| `RateLimitError`      | If rate limit is exceeded   |
| `APIUnavailableError` | If the API is unavailable   |

Source code in `src/py_gdelt/endpoints/tv.py`

```
async def query_clips(self, query_filter: TVFilter) -> list[TVClip]:
    """Query for TV clips with a filter.

    Lower-level method that accepts a TVFilter object for more control
    over query parameters.

    Args:
        query_filter: TVFilter object with query parameters

    Returns:
        List of TVClip objects

    Raises:
        APIError: If the API returns an error
        RateLimitError: If rate limit is exceeded
        APIUnavailableError: If the API is unavailable
    """
    params = self._build_params(query_filter)
    params["mode"] = "ClipGallery"
    url = await self._build_url()

    data = await self._get_json(url, params=params)

    clips: list[TVClip] = [
        TVClip(
            station=item.get("station", ""),
            show_name=item.get("show"),
            clip_url=item.get("url"),
            preview_url=item.get("preview"),
            date=try_parse_gdelt_datetime(item.get("date")),
            duration_seconds=item.get("duration"),
            snippet=item.get("snippet"),
        )
        for item in data.get("clips", [])
    ]

    return clips
```

#### `timeline(query, *, timespan='7d', start_datetime=None, end_datetime=None, station=None)`

Get timeline of TV mentions.

Returns a time series showing when a topic was mentioned on television, useful for tracking coverage patterns over time.

Parameters:

| Name             | Type       | Description  | Default                                       |
| ---------------- | ---------- | ------------ | --------------------------------------------- |
| `query`          | `str`      | Search query | *required*                                    |
| `timespan`       | \`str      | None\`       | Time range (default: "7d")                    |
| `start_datetime` | \`datetime | None\`       | Start of date range (alternative to timespan) |
| `end_datetime`   | \`datetime | None\`       | End of date range (alternative to timespan)   |
| `station`        | \`str      | None\`       | Optional station filter                       |

Returns:

| Type         | Description                      |
| ------------ | -------------------------------- |
| `TVTimeline` | TVTimeline with time series data |

Raises:

| Type                  | Description                 |
| --------------------- | --------------------------- |
| `APIError`            | If the API returns an error |
| `RateLimitError`      | If rate limit is exceeded   |
| `APIUnavailableError` | If the API is unavailable   |

Example

timeline = await tv.timeline("election", timespan="30d") for point in timeline.points: print(f"{point.date}: {point.count} mentions")

Source code in `src/py_gdelt/endpoints/tv.py`

```
async def timeline(
    self,
    query: str,
    *,
    timespan: str | None = "7d",
    start_datetime: datetime | None = None,
    end_datetime: datetime | None = None,
    station: str | None = None,
) -> TVTimeline:
    """Get timeline of TV mentions.

    Returns a time series showing when a topic was mentioned on television,
    useful for tracking coverage patterns over time.

    Args:
        query: Search query
        timespan: Time range (default: "7d")
        start_datetime: Start of date range (alternative to timespan)
        end_datetime: End of date range (alternative to timespan)
        station: Optional station filter

    Returns:
        TVTimeline with time series data

    Raises:
        APIError: If the API returns an error
        RateLimitError: If rate limit is exceeded
        APIUnavailableError: If the API is unavailable

    Example:
        timeline = await tv.timeline("election", timespan="30d")
        for point in timeline.points:
            print(f"{point.date}: {point.count} mentions")
    """
    query_filter = TVFilter(
        query=query,
        timespan=timespan if not start_datetime else None,
        start_datetime=start_datetime,
        end_datetime=end_datetime,
        station=station,
        mode="TimelineVol",
    )

    params = self._build_params(query_filter)
    url = await self._build_url()

    data = await self._get_json(url, params=params)

    points: list[TVTimelinePoint] = [
        TVTimelinePoint(
            date=item.get("date", ""),
            station=item.get("station"),
            count=item.get("count", 0),
        )
        for item in data.get("timeline", [])
    ]

    return TVTimeline(points=points)
```

#### `station_chart(query, *, timespan='7d', start_datetime=None, end_datetime=None)`

Get station comparison chart.

Shows which stations covered a topic the most, useful for understanding which networks are focusing on particular stories.

Parameters:

| Name             | Type       | Description  | Default                                       |
| ---------------- | ---------- | ------------ | --------------------------------------------- |
| `query`          | `str`      | Search query | *required*                                    |
| `timespan`       | \`str      | None\`       | Time range (default: "7d")                    |
| `start_datetime` | \`datetime | None\`       | Start of date range (alternative to timespan) |
| `end_datetime`   | \`datetime | None\`       | End of date range (alternative to timespan)   |

Returns:

| Type             | Description                           |
| ---------------- | ------------------------------------- |
| `TVStationChart` | TVStationChart with station breakdown |

Raises:

| Type                  | Description                 |
| --------------------- | --------------------------- |
| `APIError`            | If the API returns an error |
| `RateLimitError`      | If rate limit is exceeded   |
| `APIUnavailableError` | If the API is unavailable   |

Example

chart = await tv.station_chart("healthcare") for station in chart.stations: print(f"{station.station}: {station.percentage}%")

Source code in `src/py_gdelt/endpoints/tv.py`

```
async def station_chart(
    self,
    query: str,
    *,
    timespan: str | None = "7d",
    start_datetime: datetime | None = None,
    end_datetime: datetime | None = None,
) -> TVStationChart:
    """Get station comparison chart.

    Shows which stations covered a topic the most, useful for understanding
    which networks are focusing on particular stories.

    Args:
        query: Search query
        timespan: Time range (default: "7d")
        start_datetime: Start of date range (alternative to timespan)
        end_datetime: End of date range (alternative to timespan)

    Returns:
        TVStationChart with station breakdown

    Raises:
        APIError: If the API returns an error
        RateLimitError: If rate limit is exceeded
        APIUnavailableError: If the API is unavailable

    Example:
        chart = await tv.station_chart("healthcare")
        for station in chart.stations:
            print(f"{station.station}: {station.percentage}%")
    """
    query_filter = TVFilter(
        query=query,
        timespan=timespan if not start_datetime else None,
        start_datetime=start_datetime,
        end_datetime=end_datetime,
        mode="StationChart",
    )

    params = self._build_params(query_filter)
    url = await self._build_url()

    data = await self._get_json(url, params=params)

    stations: list[TVStationData] = []
    if "stations" in data:
        total = sum(s.get("count", 0) for s in data["stations"])
        for item in data["stations"]:
            count = item.get("count", 0)
            stations.append(
                TVStationData(
                    station=item.get("station", ""),
                    count=count,
                    percentage=count / total * 100 if total > 0 else None,
                ),
            )

    return TVStationChart(stations=stations)
```

### TVAIEndpoint

### `TVAIEndpoint`

Bases: `BaseEndpoint`

TVAI API endpoint for AI-enhanced TV analysis.

Similar to TVEndpoint but uses AI-powered features for enhanced analysis. Uses the same data models and similar interface as TVEndpoint.

Attributes:

| Name       | Type | Description                       |
| ---------- | ---- | --------------------------------- |
| `BASE_URL` |      | API endpoint URL for TVAI queries |

Example

async with TVAIEndpoint() as tvai: clips = await tvai.search("artificial intelligence")

Source code in `src/py_gdelt/endpoints/tv.py`

```
class TVAIEndpoint(BaseEndpoint):
    """TVAI API endpoint for AI-enhanced TV analysis.

    Similar to TVEndpoint but uses AI-powered features for enhanced analysis.
    Uses the same data models and similar interface as TVEndpoint.

    Attributes:
        BASE_URL: API endpoint URL for TVAI queries

    Example:
        async with TVAIEndpoint() as tvai:
            clips = await tvai.search("artificial intelligence")
    """

    BASE_URL = "https://api.gdeltproject.org/api/v2/tvai/tvai"

    async def _build_url(self, **kwargs: Any) -> str:
        """Build the request URL.

        TVAI API uses a fixed URL with query parameters.

        Args:
            **kwargs: Unused, but required by BaseEndpoint interface.

        Returns:
            The base TVAI API URL.
        """
        return self.BASE_URL

    async def search(
        self,
        query: str,
        *,
        timespan: str | None = None,
        start_datetime: datetime | None = None,
        end_datetime: datetime | None = None,
        station: str | None = None,
        max_results: int = 250,
    ) -> list[TVClip]:
        """Search using AI-enhanced analysis.

        Searches television transcripts using AI-powered analysis for potentially
        better semantic matching and relevance.

        Args:
            query: Search query
            timespan: Time range (e.g., "24h", "7d")
            start_datetime: Start of date range (alternative to timespan)
            end_datetime: End of date range (alternative to timespan)
            station: Filter by station
            max_results: Maximum clips to return (1-250)

        Returns:
            List of TVClip objects

        Raises:
            APIError: If the API returns an error
            RateLimitError: If rate limit is exceeded
            APIUnavailableError: If the API is unavailable

        Example:
            clips = await tvai.search("machine learning", timespan="7d")
        """
        # Build query string - GDELT TV API requires station in query
        query_str = query
        if station:
            query_str = f"{query} station:{station}"

        params: dict[str, str] = {
            "query": query_str,
            "format": "json",
            "mode": "ClipGallery",
            "maxrecords": str(max_results),
        }

        # Use explicit datetime range if provided, otherwise convert timespan
        if start_datetime:
            params["STARTDATETIME"] = start_datetime.strftime("%Y%m%d%H%M%S")
            if end_datetime:
                params["ENDDATETIME"] = end_datetime.strftime("%Y%m%d%H%M%S")
        elif timespan:
            delta = _parse_timespan(timespan)
            if delta:
                end_dt = datetime.now(UTC)
                start_dt = end_dt - delta
                params["STARTDATETIME"] = start_dt.strftime("%Y%m%d%H%M%S")
                params["ENDDATETIME"] = end_dt.strftime("%Y%m%d%H%M%S")

        url = await self._build_url()
        data = await self._get_json(url, params=params)

        clips: list[TVClip] = [
            TVClip(
                station=item.get("station", ""),
                show_name=item.get("show"),
                clip_url=item.get("url"),
                preview_url=item.get("preview"),
                date=try_parse_gdelt_datetime(item.get("date")),
                duration_seconds=item.get("duration"),
                snippet=item.get("snippet"),
            )
            for item in data.get("clips", [])
        ]

        return clips
```

#### `search(query, *, timespan=None, start_datetime=None, end_datetime=None, station=None, max_results=250)`

Search using AI-enhanced analysis.

Searches television transcripts using AI-powered analysis for potentially better semantic matching and relevance.

Parameters:

| Name             | Type       | Description                     | Default                                       |
| ---------------- | ---------- | ------------------------------- | --------------------------------------------- |
| `query`          | `str`      | Search query                    | *required*                                    |
| `timespan`       | \`str      | None\`                          | Time range (e.g., "24h", "7d")                |
| `start_datetime` | \`datetime | None\`                          | Start of date range (alternative to timespan) |
| `end_datetime`   | \`datetime | None\`                          | End of date range (alternative to timespan)   |
| `station`        | \`str      | None\`                          | Filter by station                             |
| `max_results`    | `int`      | Maximum clips to return (1-250) | `250`                                         |

Returns:

| Type           | Description            |
| -------------- | ---------------------- |
| `list[TVClip]` | List of TVClip objects |

Raises:

| Type                  | Description                 |
| --------------------- | --------------------------- |
| `APIError`            | If the API returns an error |
| `RateLimitError`      | If rate limit is exceeded   |
| `APIUnavailableError` | If the API is unavailable   |

Example

clips = await tvai.search("machine learning", timespan="7d")

Source code in `src/py_gdelt/endpoints/tv.py`

```
async def search(
    self,
    query: str,
    *,
    timespan: str | None = None,
    start_datetime: datetime | None = None,
    end_datetime: datetime | None = None,
    station: str | None = None,
    max_results: int = 250,
) -> list[TVClip]:
    """Search using AI-enhanced analysis.

    Searches television transcripts using AI-powered analysis for potentially
    better semantic matching and relevance.

    Args:
        query: Search query
        timespan: Time range (e.g., "24h", "7d")
        start_datetime: Start of date range (alternative to timespan)
        end_datetime: End of date range (alternative to timespan)
        station: Filter by station
        max_results: Maximum clips to return (1-250)

    Returns:
        List of TVClip objects

    Raises:
        APIError: If the API returns an error
        RateLimitError: If rate limit is exceeded
        APIUnavailableError: If the API is unavailable

    Example:
        clips = await tvai.search("machine learning", timespan="7d")
    """
    # Build query string - GDELT TV API requires station in query
    query_str = query
    if station:
        query_str = f"{query} station:{station}"

    params: dict[str, str] = {
        "query": query_str,
        "format": "json",
        "mode": "ClipGallery",
        "maxrecords": str(max_results),
    }

    # Use explicit datetime range if provided, otherwise convert timespan
    if start_datetime:
        params["STARTDATETIME"] = start_datetime.strftime("%Y%m%d%H%M%S")
        if end_datetime:
            params["ENDDATETIME"] = end_datetime.strftime("%Y%m%d%H%M%S")
    elif timespan:
        delta = _parse_timespan(timespan)
        if delta:
            end_dt = datetime.now(UTC)
            start_dt = end_dt - delta
            params["STARTDATETIME"] = start_dt.strftime("%Y%m%d%H%M%S")
            params["ENDDATETIME"] = end_dt.strftime("%Y%m%d%H%M%S")

    url = await self._build_url()
    data = await self._get_json(url, params=params)

    clips: list[TVClip] = [
        TVClip(
            station=item.get("station", ""),
            show_name=item.get("show"),
            clip_url=item.get("url"),
            preview_url=item.get("preview"),
            date=try_parse_gdelt_datetime(item.get("date")),
            duration_seconds=item.get("duration"),
            snippet=item.get("snippet"),
        )
        for item in data.get("clips", [])
    ]

    return clips
```

# Filters API

Filters define query criteria for GDELT endpoints.

## Common Filters

### DateRange

### `DateRange`

Bases: `BaseModel`

Date range filter with validation.

Note

There is no enforced date range limit. File-based datasets (Events, GKG, Mentions, etc.) can span years of data. Use streaming methods for large date ranges to avoid memory issues.

REST APIs have their own limits enforced server-side:

- DOC 2.0: 1 year (with timespan=1y)
- GEO 2.0: 7 days
- Context 2.0: 72 hours

Source code in `src/py_gdelt/filters.py`

```
class DateRange(BaseModel):
    """Date range filter with validation.

    Note:
        There is no enforced date range limit. File-based datasets (Events, GKG,
        Mentions, etc.) can span years of data. Use streaming methods for large
        date ranges to avoid memory issues.

        REST APIs have their own limits enforced server-side:
        - DOC 2.0: 1 year (with timespan=1y)
        - GEO 2.0: 7 days
        - Context 2.0: 72 hours
    """

    start: date
    end: date | None = None

    @model_validator(mode="after")
    def validate_range(self) -> DateRange:
        """Ensure start <= end."""
        end = self.end or self.start
        if end < self.start:
            msg = "end date must be >= start date"
            raise ValueError(msg)
        return self

    @property
    def days(self) -> int:
        """Number of days in range."""
        end = self.end or self.start
        return (end - self.start).days + 1
```

#### `days`

Number of days in range.

#### `validate_range()`

Ensure start \<= end.

Source code in `src/py_gdelt/filters.py`

```
@model_validator(mode="after")
def validate_range(self) -> DateRange:
    """Ensure start <= end."""
    end = self.end or self.start
    if end < self.start:
        msg = "end date must be >= start date"
        raise ValueError(msg)
    return self
```

## File-Based Filters

### EventFilter

### `EventFilter`

Bases: `BaseModel`

Filter for Events/Mentions queries.

Source code in `src/py_gdelt/filters.py`

```
class EventFilter(BaseModel):
    """Filter for Events/Mentions queries."""

    date_range: DateRange

    # Actor filters (CAMEO country codes validated)
    actor1_country: str | None = None
    actor2_country: str | None = None

    # Event type filters (CAMEO event codes validated)
    event_code: str | None = None
    event_root_code: str | None = None
    event_base_code: str | None = None

    # Tone filter
    min_tone: float | None = None
    max_tone: float | None = None

    # Location filters
    action_country: str | None = None

    # Options
    include_translated: bool = True

    @field_validator("actor1_country", "actor2_country", "action_country", mode="before")
    @classmethod
    def validate_country_code(cls, v: str | None) -> str | None:
        """Validate and normalize country codes (accepts FIPS or ISO3)."""
        if v is None:
            return None
        from py_gdelt.lookups.countries import Countries

        countries = Countries()
        return countries.normalize(v)  # Returns FIPS, raises InvalidCodeError if invalid

    @field_validator("event_code", "event_root_code", "event_base_code", mode="before")
    @classmethod
    def validate_cameo_code(cls, v: str | None) -> str | None:
        """Validate CAMEO event codes."""
        if v is None:
            return None
        from py_gdelt.lookups.cameo import CAMEOCodes

        cameo = CAMEOCodes()
        try:
            cameo.validate(v)
        except InvalidCodeError:
            msg = f"Invalid CAMEO code: {v!r}"
            raise InvalidCodeError(msg, code=v, code_type="CAMEO") from None
        return v
```

#### `validate_country_code(v)`

Validate and normalize country codes (accepts FIPS or ISO3).

Source code in `src/py_gdelt/filters.py`

```
@field_validator("actor1_country", "actor2_country", "action_country", mode="before")
@classmethod
def validate_country_code(cls, v: str | None) -> str | None:
    """Validate and normalize country codes (accepts FIPS or ISO3)."""
    if v is None:
        return None
    from py_gdelt.lookups.countries import Countries

    countries = Countries()
    return countries.normalize(v)  # Returns FIPS, raises InvalidCodeError if invalid
```

#### `validate_cameo_code(v)`

Validate CAMEO event codes.

Source code in `src/py_gdelt/filters.py`

```
@field_validator("event_code", "event_root_code", "event_base_code", mode="before")
@classmethod
def validate_cameo_code(cls, v: str | None) -> str | None:
    """Validate CAMEO event codes."""
    if v is None:
        return None
    from py_gdelt.lookups.cameo import CAMEOCodes

    cameo = CAMEOCodes()
    try:
        cameo.validate(v)
    except InvalidCodeError:
        msg = f"Invalid CAMEO code: {v!r}"
        raise InvalidCodeError(msg, code=v, code_type="CAMEO") from None
    return v
```

### GKGFilter

### `GKGFilter`

Bases: `BaseModel`

Filter for GKG queries.

Source code in `src/py_gdelt/filters.py`

```
class GKGFilter(BaseModel):
    """Filter for GKG queries."""

    date_range: DateRange

    # Theme filters (validated against GKG themes)
    themes: list[str] | None = None
    theme_prefix: str | None = None

    # Entity filters
    persons: list[str] | None = None
    organizations: list[str] | None = None

    # Location
    country: str | None = None

    # Tone
    min_tone: float | None = None
    max_tone: float | None = None

    # Options
    include_translated: bool = True

    @field_validator("themes", mode="before")
    @classmethod
    def validate_themes(cls, v: list[str] | None) -> list[str] | None:
        """Validate GKG theme codes."""
        if v is None:
            return None
        from py_gdelt.lookups.themes import GKGThemes

        themes = GKGThemes()
        for theme in v:
            try:
                themes.validate(theme)
            except InvalidCodeError:
                msg = f"Invalid GKG theme: {theme!r}"
                raise InvalidCodeError(msg, code=theme, code_type="GKG theme") from None
        return v

    @field_validator("country", mode="before")
    @classmethod
    def validate_country(cls, v: str | None) -> str | None:
        """Validate and normalize country code (accepts FIPS or ISO3)."""
        if v is None:
            return None
        from py_gdelt.lookups.countries import Countries

        countries = Countries()
        return countries.normalize(v)  # Returns FIPS, raises InvalidCodeError if invalid
```

#### `validate_themes(v)`

Validate GKG theme codes.

Source code in `src/py_gdelt/filters.py`

```
@field_validator("themes", mode="before")
@classmethod
def validate_themes(cls, v: list[str] | None) -> list[str] | None:
    """Validate GKG theme codes."""
    if v is None:
        return None
    from py_gdelt.lookups.themes import GKGThemes

    themes = GKGThemes()
    for theme in v:
        try:
            themes.validate(theme)
        except InvalidCodeError:
            msg = f"Invalid GKG theme: {theme!r}"
            raise InvalidCodeError(msg, code=theme, code_type="GKG theme") from None
    return v
```

#### `validate_country(v)`

Validate and normalize country code (accepts FIPS or ISO3).

Source code in `src/py_gdelt/filters.py`

```
@field_validator("country", mode="before")
@classmethod
def validate_country(cls, v: str | None) -> str | None:
    """Validate and normalize country code (accepts FIPS or ISO3)."""
    if v is None:
        return None
    from py_gdelt.lookups.countries import Countries

    countries = Countries()
    return countries.normalize(v)  # Returns FIPS, raises InvalidCodeError if invalid
```

### NGramsFilter

### `NGramsFilter`

Bases: `BaseModel`

Filter for NGrams 3.0 queries.

Source code in `src/py_gdelt/filters.py`

```
class NGramsFilter(BaseModel):
    """Filter for NGrams 3.0 queries."""

    date_range: DateRange

    # NGram filtering
    ngram: str | None = None
    language: str | None = None

    # Position filtering (decile 0-90)
    min_position: int | None = Field(default=None, ge=0, le=90)
    max_position: int | None = Field(default=None, ge=0, le=90)

    @model_validator(mode="after")
    def validate_position_range(self) -> NGramsFilter:
        """Ensure min_position <= max_position."""
        if (
            self.min_position is not None
            and self.max_position is not None
            and self.min_position > self.max_position
        ):
            msg = "min_position must be <= max_position"
            raise ValueError(msg)
        return self
```

#### `validate_position_range()`

Ensure min_position \<= max_position.

Source code in `src/py_gdelt/filters.py`

```
@model_validator(mode="after")
def validate_position_range(self) -> NGramsFilter:
    """Ensure min_position <= max_position."""
    if (
        self.min_position is not None
        and self.max_position is not None
        and self.min_position > self.max_position
    ):
        msg = "min_position must be <= max_position"
        raise ValueError(msg)
    return self
```

## REST API Filters

### DocFilter

### `DocFilter`

Bases: `BaseModel`

Filter for DOC 2.0 API queries.

Source code in `src/py_gdelt/filters.py`

```
class DocFilter(BaseModel):
    """Filter for DOC 2.0 API queries."""

    query: str

    # Time constraints
    timespan: str | None = None
    start_datetime: datetime | None = None
    end_datetime: datetime | None = None

    # Source filtering
    source_country: str | None = None
    source_language: str | None = None

    # Result options
    max_results: int = Field(default=250, ge=1, le=250)
    sort_by: Literal["date", "relevance", "tone"] = "date"

    # Output mode
    mode: Literal["artlist", "artgallery", "timelinevol"] = "artlist"

    @model_validator(mode="after")
    def validate_time_constraints(self) -> DocFilter:
        """Ensure timespan XOR datetime range, not both."""
        if self.timespan and (self.start_datetime or self.end_datetime):
            msg = "Cannot specify both timespan and datetime range"
            raise ValueError(msg)
        return self

    @field_validator("source_country", mode="before")
    @classmethod
    def validate_source_country(cls, v: str | None) -> str | None:
        """Validate and normalize source country code (accepts FIPS or ISO3)."""
        if v is None:
            return None
        from py_gdelt.lookups.countries import Countries

        countries = Countries()
        return countries.normalize(v)  # Returns FIPS, raises InvalidCodeError if invalid
```

#### `validate_time_constraints()`

Ensure timespan XOR datetime range, not both.

Source code in `src/py_gdelt/filters.py`

```
@model_validator(mode="after")
def validate_time_constraints(self) -> DocFilter:
    """Ensure timespan XOR datetime range, not both."""
    if self.timespan and (self.start_datetime or self.end_datetime):
        msg = "Cannot specify both timespan and datetime range"
        raise ValueError(msg)
    return self
```

#### `validate_source_country(v)`

Validate and normalize source country code (accepts FIPS or ISO3).

Source code in `src/py_gdelt/filters.py`

```
@field_validator("source_country", mode="before")
@classmethod
def validate_source_country(cls, v: str | None) -> str | None:
    """Validate and normalize source country code (accepts FIPS or ISO3)."""
    if v is None:
        return None
    from py_gdelt.lookups.countries import Countries

    countries = Countries()
    return countries.normalize(v)  # Returns FIPS, raises InvalidCodeError if invalid
```

### GeoFilter

### `GeoFilter`

Bases: `BaseModel`

Filter for GEO 2.0 API queries.

Source code in `src/py_gdelt/filters.py`

```
class GeoFilter(BaseModel):
    """Filter for GEO 2.0 API queries."""

    query: str

    # Geographic bounds (optional)
    bounding_box: tuple[float, float, float, float] | None = None

    # Time
    timespan: str | None = None

    # Result options
    max_results: int = Field(default=250, ge=1, le=250)

    @field_validator("bounding_box", mode="before")
    @classmethod
    def validate_bbox(
        cls,
        v: tuple[float, float, float, float] | None,
    ) -> tuple[float, float, float, float] | None:
        """Validate bounding box coordinates."""
        if v is None:
            return None
        min_lat, min_lon, max_lat, max_lon = v
        if not (-90 <= min_lat <= 90 and -90 <= max_lat <= 90):
            msg = "Latitude must be between -90 and 90"
            raise ValueError(msg)
        if not (-180 <= min_lon <= 180 and -180 <= max_lon <= 180):
            msg = "Longitude must be between -180 and 180"
            raise ValueError(msg)
        if min_lat > max_lat:
            msg = "min_lat must be <= max_lat"
            raise ValueError(msg)
        if min_lon > max_lon:
            msg = "min_lon must be <= max_lon"
            raise ValueError(msg)
        return v
```

#### `validate_bbox(v)`

Validate bounding box coordinates.

Source code in `src/py_gdelt/filters.py`

```
@field_validator("bounding_box", mode="before")
@classmethod
def validate_bbox(
    cls,
    v: tuple[float, float, float, float] | None,
) -> tuple[float, float, float, float] | None:
    """Validate bounding box coordinates."""
    if v is None:
        return None
    min_lat, min_lon, max_lat, max_lon = v
    if not (-90 <= min_lat <= 90 and -90 <= max_lat <= 90):
        msg = "Latitude must be between -90 and 90"
        raise ValueError(msg)
    if not (-180 <= min_lon <= 180 and -180 <= max_lon <= 180):
        msg = "Longitude must be between -180 and 180"
        raise ValueError(msg)
    if min_lat > max_lat:
        msg = "min_lat must be <= max_lat"
        raise ValueError(msg)
    if min_lon > max_lon:
        msg = "min_lon must be <= max_lon"
        raise ValueError(msg)
    return v
```

# Data Models API

Data models represent GDELT records and results.

## Event Models

### `Event`

Bases: `BaseModel`

GDELT Event record.

Represents a single event in the GDELT Events database. Events capture who did what to whom, when, where, and how it was reported.

Source code in `src/py_gdelt/models/events.py`

```
class Event(BaseModel):
    """GDELT Event record.

    Represents a single event in the GDELT Events database. Events capture
    who did what to whom, when, where, and how it was reported.
    """

    # Identifiers
    global_event_id: int
    date: date  # Event date
    date_added: datetime | None = None  # When first recorded (UTC)
    source_url: str | None = None

    # Actors
    actor1: Actor | None = None
    actor2: Actor | None = None

    # Action
    event_code: str  # CAMEO code (string, not int, to preserve leading zeros)
    event_base_code: str
    event_root_code: str
    quad_class: int = Field(..., ge=1, le=4)  # 1-4
    goldstein_scale: float = Field(..., ge=-10, le=10)  # -10 to +10

    # Metrics
    num_mentions: int = Field(default=0, ge=0)
    num_sources: int = Field(default=0, ge=0)
    num_articles: int = Field(default=0, ge=0)
    avg_tone: float = 0.0
    is_root_event: bool = False

    # Geography (use Location from common.py)
    actor1_geo: Location | None = None
    actor2_geo: Location | None = None
    action_geo: Location | None = None

    # Metadata
    version: int = Field(default=2, ge=1, le=2)  # 1 or 2
    is_translated: bool = False
    original_record_id: str | None = None  # For translated records

    @classmethod
    def from_raw(cls, raw: _RawEvent) -> Event:
        """Convert internal _RawEvent to public Event model.

        Args:
            raw: Internal _RawEvent dataclass from TAB-delimited parsing.

        Returns:
            Event: Public Event model with proper type conversion.

        Raises:
            ValueError: If required fields are missing or invalid.
        """

        # Helper to safely parse int
        def _parse_int(value: str | None, default: int = 0) -> int:
            if not value or value == "":
                return default
            try:
                return int(value)
            except (ValueError, TypeError):
                return default

        # Helper to safely parse float
        def _parse_float(value: str | None, default: float = 0.0) -> float:
            if not value or value == "":
                return default
            try:
                return float(value)
            except (ValueError, TypeError):
                return default

        # Helper to safely parse bool
        def _parse_bool(value: str | None) -> bool:
            if not value or value == "":
                return False
            return value.strip() == "1"

        # Helper to create Location from geo fields
        def _make_location(
            geo_type: str | None,
            name: str | None,
            country_code: str | None,
            adm1_code: str | None,
            adm2_code: str | None,
            lat: str | None,
            lon: str | None,
            feature_id: str | None,
        ) -> Location | None:
            # If no geo data, return None
            if not any(
                [
                    geo_type,
                    name,
                    country_code,
                    adm1_code,
                    adm2_code,
                    lat,
                    lon,
                    feature_id,
                ],
            ):
                return None

            return Location(
                lat=_parse_float(lat) if lat else None,
                lon=_parse_float(lon) if lon else None,
                feature_id=feature_id if feature_id else None,
                name=name if name else None,
                country_code=country_code if country_code else None,
                adm1_code=adm1_code if adm1_code else None,
                adm2_code=adm2_code if adm2_code else None,
                geo_type=_parse_int(geo_type) if geo_type else None,
            )

        # Helper to create Actor
        def _make_actor(
            code: str | None,
            name: str | None,
            country_code: str | None,
            known_group_code: str | None,
            ethnic_code: str | None,
            religion1_code: str | None,
            religion2_code: str | None,
            type1_code: str | None,
            type2_code: str | None,
            type3_code: str | None,
        ) -> Actor | None:
            # If no actor data, return None
            if not any(
                [
                    code,
                    name,
                    country_code,
                    known_group_code,
                    ethnic_code,
                    religion1_code,
                    religion2_code,
                    type1_code,
                    type2_code,
                    type3_code,
                ],
            ):
                return None

            return Actor(
                code=code if code else None,
                name=name if name else None,
                country_code=country_code if country_code else None,
                known_group_code=known_group_code if known_group_code else None,
                ethnic_code=ethnic_code if ethnic_code else None,
                religion1_code=religion1_code if religion1_code else None,
                religion2_code=religion2_code if religion2_code else None,
                type1_code=type1_code if type1_code else None,
                type2_code=type2_code if type2_code else None,
                type3_code=type3_code if type3_code else None,
            )

        # Parse date from sql_date (YYYYMMDD)
        event_date = parse_gdelt_date(raw.sql_date)

        # Parse date_added (YYYYMMDDHHMMSS)
        date_added_dt: datetime | None = None
        if raw.date_added:
            with suppress(ValueError):
                date_added_dt = parse_gdelt_datetime(raw.date_added)

        # Create actors
        actor1 = _make_actor(
            raw.actor1_code,
            raw.actor1_name,
            raw.actor1_country_code,
            raw.actor1_known_group_code,
            raw.actor1_ethnic_code,
            raw.actor1_religion1_code,
            raw.actor1_religion2_code,
            raw.actor1_type1_code,
            raw.actor1_type2_code,
            raw.actor1_type3_code,
        )

        actor2 = _make_actor(
            raw.actor2_code,
            raw.actor2_name,
            raw.actor2_country_code,
            raw.actor2_known_group_code,
            raw.actor2_ethnic_code,
            raw.actor2_religion1_code,
            raw.actor2_religion2_code,
            raw.actor2_type1_code,
            raw.actor2_type2_code,
            raw.actor2_type3_code,
        )

        # Create locations
        actor1_geo = _make_location(
            raw.actor1_geo_type,
            raw.actor1_geo_fullname,
            raw.actor1_geo_country_code,
            raw.actor1_geo_adm1_code,
            raw.actor1_geo_adm2_code,
            raw.actor1_geo_lat,
            raw.actor1_geo_lon,
            raw.actor1_geo_feature_id,
        )

        actor2_geo = _make_location(
            raw.actor2_geo_type,
            raw.actor2_geo_fullname,
            raw.actor2_geo_country_code,
            raw.actor2_geo_adm1_code,
            raw.actor2_geo_adm2_code,
            raw.actor2_geo_lat,
            raw.actor2_geo_lon,
            raw.actor2_geo_feature_id,
        )

        action_geo = _make_location(
            raw.action_geo_type,
            raw.action_geo_fullname,
            raw.action_geo_country_code,
            raw.action_geo_adm1_code,
            raw.action_geo_adm2_code,
            raw.action_geo_lat,
            raw.action_geo_lon,
            raw.action_geo_feature_id,
        )

        return cls(
            global_event_id=_parse_int(raw.global_event_id),
            date=event_date,
            date_added=date_added_dt,
            source_url=raw.source_url if raw.source_url else None,
            actor1=actor1,
            actor2=actor2,
            event_code=raw.event_code,  # Keep as string to preserve leading zeros
            event_base_code=raw.event_base_code,
            event_root_code=raw.event_root_code,
            quad_class=_parse_int(raw.quad_class, default=1),
            goldstein_scale=_parse_float(raw.goldstein_scale, default=0.0),
            num_mentions=_parse_int(raw.num_mentions, default=0),
            num_sources=_parse_int(raw.num_sources, default=0),
            num_articles=_parse_int(raw.num_articles, default=0),
            avg_tone=_parse_float(raw.avg_tone, default=0.0),
            is_root_event=_parse_bool(raw.is_root_event),
            actor1_geo=actor1_geo,
            actor2_geo=actor2_geo,
            action_geo=action_geo,
            version=2,  # Default to v2, can be overridden
            is_translated=raw.is_translated,
            original_record_id=None,  # Not in raw data
        )

    @property
    def is_conflict(self) -> bool:
        """Check if event is conflict (quad_class 3 or 4).

        Returns:
            True if event is material conflict (3) or verbal conflict (4).
        """
        return self.quad_class in (3, 4)

    @property
    def is_cooperation(self) -> bool:
        """Check if event is cooperation (quad_class 1 or 2).

        Returns:
            True if event is verbal cooperation (1) or material cooperation (2).
        """
        return self.quad_class in (1, 2)
```

#### `is_conflict`

Check if event is conflict (quad_class 3 or 4).

Returns:

| Type   | Description                                                    |
| ------ | -------------------------------------------------------------- |
| `bool` | True if event is material conflict (3) or verbal conflict (4). |

#### `is_cooperation`

Check if event is cooperation (quad_class 1 or 2).

Returns:

| Type   | Description                                                          |
| ------ | -------------------------------------------------------------------- |
| `bool` | True if event is verbal cooperation (1) or material cooperation (2). |

#### `from_raw(raw)`

Convert internal \_RawEvent to public Event model.

Parameters:

| Name  | Type        | Description                                               | Default    |
| ----- | ----------- | --------------------------------------------------------- | ---------- |
| `raw` | `_RawEvent` | Internal \_RawEvent dataclass from TAB-delimited parsing. | *required* |

Returns:

| Name    | Type    | Description                                     |
| ------- | ------- | ----------------------------------------------- |
| `Event` | `Event` | Public Event model with proper type conversion. |

Raises:

| Type         | Description                                |
| ------------ | ------------------------------------------ |
| `ValueError` | If required fields are missing or invalid. |

Source code in `src/py_gdelt/models/events.py`

```
@classmethod
def from_raw(cls, raw: _RawEvent) -> Event:
    """Convert internal _RawEvent to public Event model.

    Args:
        raw: Internal _RawEvent dataclass from TAB-delimited parsing.

    Returns:
        Event: Public Event model with proper type conversion.

    Raises:
        ValueError: If required fields are missing or invalid.
    """

    # Helper to safely parse int
    def _parse_int(value: str | None, default: int = 0) -> int:
        if not value or value == "":
            return default
        try:
            return int(value)
        except (ValueError, TypeError):
            return default

    # Helper to safely parse float
    def _parse_float(value: str | None, default: float = 0.0) -> float:
        if not value or value == "":
            return default
        try:
            return float(value)
        except (ValueError, TypeError):
            return default

    # Helper to safely parse bool
    def _parse_bool(value: str | None) -> bool:
        if not value or value == "":
            return False
        return value.strip() == "1"

    # Helper to create Location from geo fields
    def _make_location(
        geo_type: str | None,
        name: str | None,
        country_code: str | None,
        adm1_code: str | None,
        adm2_code: str | None,
        lat: str | None,
        lon: str | None,
        feature_id: str | None,
    ) -> Location | None:
        # If no geo data, return None
        if not any(
            [
                geo_type,
                name,
                country_code,
                adm1_code,
                adm2_code,
                lat,
                lon,
                feature_id,
            ],
        ):
            return None

        return Location(
            lat=_parse_float(lat) if lat else None,
            lon=_parse_float(lon) if lon else None,
            feature_id=feature_id if feature_id else None,
            name=name if name else None,
            country_code=country_code if country_code else None,
            adm1_code=adm1_code if adm1_code else None,
            adm2_code=adm2_code if adm2_code else None,
            geo_type=_parse_int(geo_type) if geo_type else None,
        )

    # Helper to create Actor
    def _make_actor(
        code: str | None,
        name: str | None,
        country_code: str | None,
        known_group_code: str | None,
        ethnic_code: str | None,
        religion1_code: str | None,
        religion2_code: str | None,
        type1_code: str | None,
        type2_code: str | None,
        type3_code: str | None,
    ) -> Actor | None:
        # If no actor data, return None
        if not any(
            [
                code,
                name,
                country_code,
                known_group_code,
                ethnic_code,
                religion1_code,
                religion2_code,
                type1_code,
                type2_code,
                type3_code,
            ],
        ):
            return None

        return Actor(
            code=code if code else None,
            name=name if name else None,
            country_code=country_code if country_code else None,
            known_group_code=known_group_code if known_group_code else None,
            ethnic_code=ethnic_code if ethnic_code else None,
            religion1_code=religion1_code if religion1_code else None,
            religion2_code=religion2_code if religion2_code else None,
            type1_code=type1_code if type1_code else None,
            type2_code=type2_code if type2_code else None,
            type3_code=type3_code if type3_code else None,
        )

    # Parse date from sql_date (YYYYMMDD)
    event_date = parse_gdelt_date(raw.sql_date)

    # Parse date_added (YYYYMMDDHHMMSS)
    date_added_dt: datetime | None = None
    if raw.date_added:
        with suppress(ValueError):
            date_added_dt = parse_gdelt_datetime(raw.date_added)

    # Create actors
    actor1 = _make_actor(
        raw.actor1_code,
        raw.actor1_name,
        raw.actor1_country_code,
        raw.actor1_known_group_code,
        raw.actor1_ethnic_code,
        raw.actor1_religion1_code,
        raw.actor1_religion2_code,
        raw.actor1_type1_code,
        raw.actor1_type2_code,
        raw.actor1_type3_code,
    )

    actor2 = _make_actor(
        raw.actor2_code,
        raw.actor2_name,
        raw.actor2_country_code,
        raw.actor2_known_group_code,
        raw.actor2_ethnic_code,
        raw.actor2_religion1_code,
        raw.actor2_religion2_code,
        raw.actor2_type1_code,
        raw.actor2_type2_code,
        raw.actor2_type3_code,
    )

    # Create locations
    actor1_geo = _make_location(
        raw.actor1_geo_type,
        raw.actor1_geo_fullname,
        raw.actor1_geo_country_code,
        raw.actor1_geo_adm1_code,
        raw.actor1_geo_adm2_code,
        raw.actor1_geo_lat,
        raw.actor1_geo_lon,
        raw.actor1_geo_feature_id,
    )

    actor2_geo = _make_location(
        raw.actor2_geo_type,
        raw.actor2_geo_fullname,
        raw.actor2_geo_country_code,
        raw.actor2_geo_adm1_code,
        raw.actor2_geo_adm2_code,
        raw.actor2_geo_lat,
        raw.actor2_geo_lon,
        raw.actor2_geo_feature_id,
    )

    action_geo = _make_location(
        raw.action_geo_type,
        raw.action_geo_fullname,
        raw.action_geo_country_code,
        raw.action_geo_adm1_code,
        raw.action_geo_adm2_code,
        raw.action_geo_lat,
        raw.action_geo_lon,
        raw.action_geo_feature_id,
    )

    return cls(
        global_event_id=_parse_int(raw.global_event_id),
        date=event_date,
        date_added=date_added_dt,
        source_url=raw.source_url if raw.source_url else None,
        actor1=actor1,
        actor2=actor2,
        event_code=raw.event_code,  # Keep as string to preserve leading zeros
        event_base_code=raw.event_base_code,
        event_root_code=raw.event_root_code,
        quad_class=_parse_int(raw.quad_class, default=1),
        goldstein_scale=_parse_float(raw.goldstein_scale, default=0.0),
        num_mentions=_parse_int(raw.num_mentions, default=0),
        num_sources=_parse_int(raw.num_sources, default=0),
        num_articles=_parse_int(raw.num_articles, default=0),
        avg_tone=_parse_float(raw.avg_tone, default=0.0),
        is_root_event=_parse_bool(raw.is_root_event),
        actor1_geo=actor1_geo,
        actor2_geo=actor2_geo,
        action_geo=action_geo,
        version=2,  # Default to v2, can be overridden
        is_translated=raw.is_translated,
        original_record_id=None,  # Not in raw data
    )
```

### `Mention`

Bases: `BaseModel`

Mention of a GDELT event in a source.

Represents a single mention of an event in a news article. Each event can have many mentions across different sources and times.

Source code in `src/py_gdelt/models/events.py`

```
class Mention(BaseModel):
    """Mention of a GDELT event in a source.

    Represents a single mention of an event in a news article. Each event
    can have many mentions across different sources and times.
    """

    global_event_id: int
    event_time: datetime
    mention_time: datetime
    mention_type: int  # 1=WEB, 2=Citation, 3=CORE, etc.
    source_name: str
    identifier: str  # URL, DOI, or citation
    sentence_id: int
    actor1_char_offset: int | None = None
    actor2_char_offset: int | None = None
    action_char_offset: int | None = None
    in_raw_text: bool = False
    confidence: int = Field(..., ge=10, le=100)  # 10-100
    doc_length: int = Field(default=0, ge=0)
    doc_tone: float = 0.0
    translation_info: str | None = None

    @classmethod
    def from_raw(cls, raw: _RawMention) -> Mention:
        """Convert internal _RawMention to public Mention model.

        Args:
            raw: Internal _RawMention dataclass from TAB-delimited parsing.

        Returns:
            Mention: Public Mention model with proper type conversion.

        Raises:
            ValueError: If required fields are missing or invalid.
        """

        # Helper to safely parse int
        def _parse_int(value: str | None, default: int = 0) -> int:
            if not value or value == "":
                return default
            try:
                return int(value)
            except (ValueError, TypeError):
                return default

        # Helper to safely parse float
        def _parse_float(value: str | None, default: float = 0.0) -> float:
            if not value or value == "":
                return default
            try:
                return float(value)
            except (ValueError, TypeError):
                return default

        # Helper to safely parse bool
        def _parse_bool(value: str | None) -> bool:
            if not value or value == "":
                return False
            return value.strip() == "1"

        # Parse event_time (YYYYMMDDHHMMSS)
        event_time = parse_gdelt_datetime(raw.event_time_full)

        # Parse mention_time (YYYYMMDDHHMMSS)
        mention_time = parse_gdelt_datetime(raw.mention_time_full)

        # Parse char offsets (can be empty string)
        actor1_offset = (
            _parse_int(raw.actor1_char_offset)
            if raw.actor1_char_offset and raw.actor1_char_offset != ""
            else None
        )
        actor2_offset = (
            _parse_int(raw.actor2_char_offset)
            if raw.actor2_char_offset and raw.actor2_char_offset != ""
            else None
        )
        action_offset = (
            _parse_int(raw.action_char_offset)
            if raw.action_char_offset and raw.action_char_offset != ""
            else None
        )

        return cls(
            global_event_id=_parse_int(raw.global_event_id),
            event_time=event_time,
            mention_time=mention_time,
            mention_type=_parse_int(raw.mention_type, default=1),
            source_name=raw.mention_source_name,
            identifier=raw.mention_identifier,
            sentence_id=_parse_int(raw.sentence_id, default=0),
            actor1_char_offset=actor1_offset,
            actor2_char_offset=actor2_offset,
            action_char_offset=action_offset,
            in_raw_text=_parse_bool(raw.in_raw_text),
            confidence=_parse_int(raw.confidence, default=50),
            doc_length=_parse_int(raw.mention_doc_length, default=0),
            doc_tone=_parse_float(raw.mention_doc_tone, default=0.0),
            translation_info=(
                raw.mention_doc_translation_info if raw.mention_doc_translation_info else None
            ),
        )
```

#### `from_raw(raw)`

Convert internal \_RawMention to public Mention model.

Parameters:

| Name  | Type          | Description                                                 | Default    |
| ----- | ------------- | ----------------------------------------------------------- | ---------- |
| `raw` | `_RawMention` | Internal \_RawMention dataclass from TAB-delimited parsing. | *required* |

Returns:

| Name      | Type      | Description                                       |
| --------- | --------- | ------------------------------------------------- |
| `Mention` | `Mention` | Public Mention model with proper type conversion. |

Raises:

| Type         | Description                                |
| ------------ | ------------------------------------------ |
| `ValueError` | If required fields are missing or invalid. |

Source code in `src/py_gdelt/models/events.py`

```
@classmethod
def from_raw(cls, raw: _RawMention) -> Mention:
    """Convert internal _RawMention to public Mention model.

    Args:
        raw: Internal _RawMention dataclass from TAB-delimited parsing.

    Returns:
        Mention: Public Mention model with proper type conversion.

    Raises:
        ValueError: If required fields are missing or invalid.
    """

    # Helper to safely parse int
    def _parse_int(value: str | None, default: int = 0) -> int:
        if not value or value == "":
            return default
        try:
            return int(value)
        except (ValueError, TypeError):
            return default

    # Helper to safely parse float
    def _parse_float(value: str | None, default: float = 0.0) -> float:
        if not value or value == "":
            return default
        try:
            return float(value)
        except (ValueError, TypeError):
            return default

    # Helper to safely parse bool
    def _parse_bool(value: str | None) -> bool:
        if not value or value == "":
            return False
        return value.strip() == "1"

    # Parse event_time (YYYYMMDDHHMMSS)
    event_time = parse_gdelt_datetime(raw.event_time_full)

    # Parse mention_time (YYYYMMDDHHMMSS)
    mention_time = parse_gdelt_datetime(raw.mention_time_full)

    # Parse char offsets (can be empty string)
    actor1_offset = (
        _parse_int(raw.actor1_char_offset)
        if raw.actor1_char_offset and raw.actor1_char_offset != ""
        else None
    )
    actor2_offset = (
        _parse_int(raw.actor2_char_offset)
        if raw.actor2_char_offset and raw.actor2_char_offset != ""
        else None
    )
    action_offset = (
        _parse_int(raw.action_char_offset)
        if raw.action_char_offset and raw.action_char_offset != ""
        else None
    )

    return cls(
        global_event_id=_parse_int(raw.global_event_id),
        event_time=event_time,
        mention_time=mention_time,
        mention_type=_parse_int(raw.mention_type, default=1),
        source_name=raw.mention_source_name,
        identifier=raw.mention_identifier,
        sentence_id=_parse_int(raw.sentence_id, default=0),
        actor1_char_offset=actor1_offset,
        actor2_char_offset=actor2_offset,
        action_char_offset=action_offset,
        in_raw_text=_parse_bool(raw.in_raw_text),
        confidence=_parse_int(raw.confidence, default=50),
        doc_length=_parse_int(raw.mention_doc_length, default=0),
        doc_tone=_parse_float(raw.mention_doc_tone, default=0.0),
        translation_info=(
            raw.mention_doc_translation_info if raw.mention_doc_translation_info else None
        ),
    )
```

### `Actor`

Bases: `BaseModel`

Actor in a GDELT event.

Represents an entity (person, organization, country, etc.) participating in an event. Uses CAMEO actor codes for classification.

Source code in `src/py_gdelt/models/events.py`

```
class Actor(BaseModel):
    """Actor in a GDELT event.

    Represents an entity (person, organization, country, etc.) participating in an event.
    Uses CAMEO actor codes for classification.
    """

    code: str | None = None
    name: str | None = None
    country_code: str | None = None  # FIPS code
    known_group_code: str | None = None
    ethnic_code: str | None = None
    religion1_code: str | None = None
    religion2_code: str | None = None
    type1_code: str | None = None
    type2_code: str | None = None
    type3_code: str | None = None

    @property
    def is_state_actor(self) -> bool:
        """Check if actor is a state/government actor.

        Returns:
            True if actor has a country code but no known group code,
            indicating a state-level actor.
        """
        return self.country_code is not None and self.known_group_code is None
```

#### `is_state_actor`

Check if actor is a state/government actor.

Returns:

| Type   | Description                                               |
| ------ | --------------------------------------------------------- |
| `bool` | True if actor has a country code but no known group code, |
| `bool` | indicating a state-level actor.                           |

## Article Models

### `Article`

Bases: `BaseModel`

Article from GDELT DOC API.

Represents a news article monitored by GDELT.

Source code in `src/py_gdelt/models/articles.py`

```
class Article(BaseModel):
    """
    Article from GDELT DOC API.

    Represents a news article monitored by GDELT.
    """

    # Core fields (from API)
    url: str
    title: str | None = None
    seendate: str | None = None  # Raw GDELT date string (YYYYMMDDHHMMSS)

    # Source information
    domain: str | None = None
    source_country: str | None = Field(default=None, alias="sourcecountry")
    language: str | None = None

    # Content
    socialimage: str | None = None  # Preview image URL

    # Tone analysis (optional)
    tone: float | None = None

    # Sharing metrics (optional)
    share_count: int | None = Field(default=None, alias="sharecount")

    model_config = {"populate_by_name": True}

    @property
    def seen_datetime(self) -> datetime | None:
        """
        Parse seendate to datetime.

        Returns:
            datetime object or None if parsing fails
        """
        return try_parse_gdelt_datetime(self.seendate)

    @property
    def is_english(self) -> bool:
        """Check if article is in English."""
        if not self.language:
            return False
        return self.language.lower() in ("english", "en")

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for serialization."""
        return self.model_dump(by_alias=True)
```

#### `seen_datetime`

Parse seendate to datetime.

Returns:

| Type       | Description |
| ---------- | ----------- |
| \`datetime | None\`      |

#### `is_english`

Check if article is in English.

#### `to_dict()`

Convert to dictionary for serialization.

Source code in `src/py_gdelt/models/articles.py`

```
def to_dict(self) -> dict[str, Any]:
    """Convert to dictionary for serialization."""
    return self.model_dump(by_alias=True)
```

### `Timeline`

Bases: `BaseModel`

Timeline data from GDELT DOC API.

Contains time series data for article volume.

Source code in `src/py_gdelt/models/articles.py`

```
class Timeline(BaseModel):
    """
    Timeline data from GDELT DOC API.

    Contains time series data for article volume.
    """

    # The timeline data
    timeline: list[TimelinePoint] = Field(default_factory=list)

    # Metadata
    query: str | None = None
    total_articles: int | None = None

    @field_validator("timeline", mode="before")
    @classmethod
    def parse_timeline(cls, v: Any) -> list[TimelinePoint]:
        """Parse timeline from various formats.

        Handles both flat format and nested series format from timelinevol API:
        - Flat: [{"date": "...", "value": ...}, ...]
        - Nested: [{"series": "...", "data": [{"date": "...", "value": ...}]}]
        """
        if v is None:
            return []
        if isinstance(v, list):
            points: list[TimelinePoint] = []
            for item in v:
                if isinstance(item, TimelinePoint):
                    points.append(item)
                elif isinstance(item, dict):
                    # Check for nested series/data structure from timelinevol API
                    if "data" in item and isinstance(item["data"], list):
                        for dp in item["data"]:
                            if isinstance(dp, dict):
                                points.append(TimelinePoint.model_validate(dp))
                            else:
                                logger.warning(
                                    "Skipping non-dict timeline data point: %s", type(dp).__name__
                                )
                    else:
                        # Flat structure with date/value directly
                        points.append(TimelinePoint.model_validate(item))
            return points
        return []

    @property
    def points(self) -> list[TimelinePoint]:
        """Alias for timeline for cleaner access."""
        return self.timeline

    @property
    def dates(self) -> list[str]:
        """Get list of dates."""
        return [p.date for p in self.timeline]

    @property
    def values(self) -> list[float]:
        """Get list of values."""
        return [p.value for p in self.timeline]

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        return {
            "timeline": [p.model_dump() for p in self.timeline],
            "query": self.query,
            "total_articles": self.total_articles,
        }

    def to_series(self) -> dict[str, float]:
        """
        Convert to date:value mapping.

        Useful for quick lookups and plotting.
        """
        return {p.date: p.value for p in self.timeline}
```

#### `points`

Alias for timeline for cleaner access.

#### `dates`

Get list of dates.

#### `values`

Get list of values.

#### `parse_timeline(v)`

Parse timeline from various formats.

Handles both flat format and nested series format from timelinevol API:

- Flat: [{"date": "...", "value": ...}, ...]
- Nested: \[{"series": "...", "data": [{"date": "...", "value": ...}]}\]

Source code in `src/py_gdelt/models/articles.py`

```
@field_validator("timeline", mode="before")
@classmethod
def parse_timeline(cls, v: Any) -> list[TimelinePoint]:
    """Parse timeline from various formats.

    Handles both flat format and nested series format from timelinevol API:
    - Flat: [{"date": "...", "value": ...}, ...]
    - Nested: [{"series": "...", "data": [{"date": "...", "value": ...}]}]
    """
    if v is None:
        return []
    if isinstance(v, list):
        points: list[TimelinePoint] = []
        for item in v:
            if isinstance(item, TimelinePoint):
                points.append(item)
            elif isinstance(item, dict):
                # Check for nested series/data structure from timelinevol API
                if "data" in item and isinstance(item["data"], list):
                    for dp in item["data"]:
                        if isinstance(dp, dict):
                            points.append(TimelinePoint.model_validate(dp))
                        else:
                            logger.warning(
                                "Skipping non-dict timeline data point: %s", type(dp).__name__
                            )
                else:
                    # Flat structure with date/value directly
                    points.append(TimelinePoint.model_validate(item))
        return points
    return []
```

#### `to_dict()`

Convert to dictionary.

Source code in `src/py_gdelt/models/articles.py`

```
def to_dict(self) -> dict[str, Any]:
    """Convert to dictionary."""
    return {
        "timeline": [p.model_dump() for p in self.timeline],
        "query": self.query,
        "total_articles": self.total_articles,
    }
```

#### `to_series()`

Convert to date:value mapping.

Useful for quick lookups and plotting.

Source code in `src/py_gdelt/models/articles.py`

```
def to_series(self) -> dict[str, float]:
    """
    Convert to date:value mapping.

    Useful for quick lookups and plotting.
    """
    return {p.date: p.value for p in self.timeline}
```

### `TimelinePoint`

Bases: `BaseModel`

Single data point in a timeline.

Source code in `src/py_gdelt/models/articles.py`

```
class TimelinePoint(BaseModel):
    """Single data point in a timeline."""

    date: str
    value: float = Field(default=0, alias="count")

    # Optional breakdown
    tone: float | None = None

    model_config = {"populate_by_name": True}

    @property
    def parsed_date(self) -> datetime | None:
        """Parse date string to datetime."""
        return try_parse_gdelt_datetime(self.date)
```

#### `parsed_date`

Parse date string to datetime.

## GKG Models

### `GKGRecord`

Bases: `BaseModel`

GDELT Global Knowledge Graph record.

Represents enriched content analysis of a news article or document, including extracted themes, entities, locations, tone, and other metadata.

Attributes:

| Name                 | Type                  | Description                                                                         |
| -------------------- | --------------------- | ----------------------------------------------------------------------------------- |
| `record_id`          | `str`                 | Unique identifier in format "YYYYMMDDHHMMSS-seq" or with "-T" suffix for translated |
| `date`               | `datetime`            | Publication date/time                                                               |
| `source_url`         | `str`                 | URL of the source document                                                          |
| `source_name`        | `str`                 | Common name of the source                                                           |
| `source_collection`  | `int`                 | Source collection identifier (1=WEB, 2=Citation, etc.)                              |
| `themes`             | `list[EntityMention]` | Extracted themes/topics                                                             |
| `persons`            | `list[EntityMention]` | Extracted person names                                                              |
| `organizations`      | `list[EntityMention]` | Extracted organization names                                                        |
| `locations`          | `list[Location]`      | Extracted geographic locations                                                      |
| `tone`               | \`ToneScores          | None\`                                                                              |
| `gcam`               | `dict[str, float]`    | GCAM emotional dimension scores as dict                                             |
| `quotations`         | `list[Quotation]`     | Extracted quotations (v2.1+ only)                                                   |
| `amounts`            | `list[Amount]`        | Extracted numerical amounts (v2.1+ only)                                            |
| `sharing_image`      | \`str                 | None\`                                                                              |
| `all_names`          | `list[str]`           | All extracted names as flat list                                                    |
| `version`            | `int`                 | GDELT version (1 or 2)                                                              |
| `is_translated`      | `bool`                | Whether this is a translated document                                               |
| `original_record_id` | \`str                 | None\`                                                                              |
| `translation_info`   | \`str                 | None\`                                                                              |

Source code in `src/py_gdelt/models/gkg.py`

```
class GKGRecord(BaseModel):
    """GDELT Global Knowledge Graph record.

    Represents enriched content analysis of a news article or document, including
    extracted themes, entities, locations, tone, and other metadata.

    Attributes:
        record_id: Unique identifier in format "YYYYMMDDHHMMSS-seq" or with "-T" suffix for translated
        date: Publication date/time
        source_url: URL of the source document
        source_name: Common name of the source
        source_collection: Source collection identifier (1=WEB, 2=Citation, etc.)
        themes: Extracted themes/topics
        persons: Extracted person names
        organizations: Extracted organization names
        locations: Extracted geographic locations
        tone: Document tone analysis scores
        gcam: GCAM emotional dimension scores as dict
        quotations: Extracted quotations (v2.1+ only)
        amounts: Extracted numerical amounts (v2.1+ only)
        sharing_image: Primary sharing image URL if any
        all_names: All extracted names as flat list
        version: GDELT version (1 or 2)
        is_translated: Whether this is a translated document
        original_record_id: Original record ID if translated
        translation_info: Translation metadata string
    """

    # Identifiers
    record_id: str
    date: datetime
    source_url: str
    source_name: str
    source_collection: int

    # Extracted entities
    themes: list[EntityMention] = Field(default_factory=list)
    persons: list[EntityMention] = Field(default_factory=list)
    organizations: list[EntityMention] = Field(default_factory=list)
    locations: list[Location] = Field(default_factory=list)

    # Tone
    tone: ToneScores | None = None

    # GCAM emotional dimensions
    gcam: dict[str, float] = Field(default_factory=dict)

    # V2.1+ fields
    quotations: list[Quotation] = Field(default_factory=list)
    amounts: list[Amount] = Field(default_factory=list)

    # Extra fields
    sharing_image: str | None = None
    all_names: list[str] = Field(default_factory=list)

    # Metadata
    version: int = 2
    is_translated: bool = False
    original_record_id: str | None = None
    translation_info: str | None = None

    @classmethod
    def from_raw(cls, raw: _RawGKG) -> GKGRecord:
        """Convert internal _RawGKG to public GKGRecord model.

        This method handles parsing the complex delimited fields from GKG v2.1 format:
        - Themes: semicolon-delimited "theme,offset" pairs
        - GCAM: semicolon-delimited "key:value" pairs
        - Quotations: pipe-delimited records with format "offset#length#verb#quote"
        - Amounts: semicolon-delimited "amount,object,offset" triples
        - Locations: semicolon-delimited with multiple sub-fields
        - Tone: comma-separated values

        Args:
            raw: Internal _RawGKG dataclass from TSV parsing

        Returns:
            Validated GKGRecord with all fields parsed and typed
        """
        # Detect translation
        is_translated = raw.gkg_record_id.endswith("-T")
        original_record_id = None
        if is_translated:
            original_record_id = raw.gkg_record_id[:-2]

        # Parse date (format: YYYYMMDDHHMMSS)
        date = parse_gdelt_datetime(raw.date)

        # Parse themes from V2 enhanced (preferred) or V1
        themes = _parse_themes(raw.themes_v2_enhanced or raw.themes_v1)

        # Parse persons
        persons = _parse_entities(raw.persons_v2_enhanced or raw.persons_v1, entity_type="PERSON")

        # Parse organizations
        organizations = _parse_entities(
            raw.organizations_v2_enhanced or raw.organizations_v1,
            entity_type="ORG",
        )

        # Parse locations
        locations = _parse_locations(raw.locations_v2_enhanced or raw.locations_v1)

        # Parse tone
        tone = _parse_tone(raw.tone) if raw.tone else None

        # Parse GCAM
        gcam = _parse_gcam(raw.gcam) if raw.gcam else {}

        # Parse quotations (v2.1+)
        quotations = _parse_quotations(raw.quotations) if raw.quotations else []

        # Parse amounts (v2.1+)
        amounts = _parse_amounts(raw.amounts) if raw.amounts else []

        # Parse all_names
        all_names = (
            [name.strip() for name in raw.all_names.split(";") if name.strip()]
            if raw.all_names
            else []
        )

        # Determine version (if v2 enhanced fields exist, it's v2)
        version = 2 if raw.themes_v2_enhanced else 1

        return cls(
            record_id=raw.gkg_record_id,
            date=date,
            source_url=raw.document_identifier,
            source_name=raw.source_common_name,
            source_collection=int(raw.source_collection_id),
            themes=themes,
            persons=persons,
            organizations=organizations,
            locations=locations,
            tone=tone,
            gcam=gcam,
            quotations=quotations,
            amounts=amounts,
            sharing_image=raw.sharing_image,
            all_names=all_names,
            version=version,
            is_translated=is_translated,
            original_record_id=original_record_id,
            translation_info=raw.translation_info,
        )

    @property
    def primary_theme(self) -> str | None:
        """Get the first/primary theme if any.

        Returns:
            The name of the first theme, or None if no themes exist
        """
        if not self.themes:
            return None
        return self.themes[0].name

    @property
    def has_quotations(self) -> bool:
        """Check if record has extracted quotations.

        Returns:
            True if one or more quotations were extracted
        """
        return len(self.quotations) > 0
```

#### `primary_theme`

Get the first/primary theme if any.

Returns:

| Type  | Description |
| ----- | ----------- |
| \`str | None\`      |

#### `has_quotations`

Check if record has extracted quotations.

Returns:

| Type   | Description                                   |
| ------ | --------------------------------------------- |
| `bool` | True if one or more quotations were extracted |

#### `from_raw(raw)`

Convert internal \_RawGKG to public GKGRecord model.

This method handles parsing the complex delimited fields from GKG v2.1 format:

- Themes: semicolon-delimited "theme,offset" pairs
- GCAM: semicolon-delimited "key:value" pairs
- Quotations: pipe-delimited records with format "offset#length#verb#quote"
- Amounts: semicolon-delimited "amount,object,offset" triples
- Locations: semicolon-delimited with multiple sub-fields
- Tone: comma-separated values

Parameters:

| Name  | Type      | Description                                  | Default    |
| ----- | --------- | -------------------------------------------- | ---------- |
| `raw` | `_RawGKG` | Internal \_RawGKG dataclass from TSV parsing | *required* |

Returns:

| Type        | Description                                          |
| ----------- | ---------------------------------------------------- |
| `GKGRecord` | Validated GKGRecord with all fields parsed and typed |

Source code in `src/py_gdelt/models/gkg.py`

```
@classmethod
def from_raw(cls, raw: _RawGKG) -> GKGRecord:
    """Convert internal _RawGKG to public GKGRecord model.

    This method handles parsing the complex delimited fields from GKG v2.1 format:
    - Themes: semicolon-delimited "theme,offset" pairs
    - GCAM: semicolon-delimited "key:value" pairs
    - Quotations: pipe-delimited records with format "offset#length#verb#quote"
    - Amounts: semicolon-delimited "amount,object,offset" triples
    - Locations: semicolon-delimited with multiple sub-fields
    - Tone: comma-separated values

    Args:
        raw: Internal _RawGKG dataclass from TSV parsing

    Returns:
        Validated GKGRecord with all fields parsed and typed
    """
    # Detect translation
    is_translated = raw.gkg_record_id.endswith("-T")
    original_record_id = None
    if is_translated:
        original_record_id = raw.gkg_record_id[:-2]

    # Parse date (format: YYYYMMDDHHMMSS)
    date = parse_gdelt_datetime(raw.date)

    # Parse themes from V2 enhanced (preferred) or V1
    themes = _parse_themes(raw.themes_v2_enhanced or raw.themes_v1)

    # Parse persons
    persons = _parse_entities(raw.persons_v2_enhanced or raw.persons_v1, entity_type="PERSON")

    # Parse organizations
    organizations = _parse_entities(
        raw.organizations_v2_enhanced or raw.organizations_v1,
        entity_type="ORG",
    )

    # Parse locations
    locations = _parse_locations(raw.locations_v2_enhanced or raw.locations_v1)

    # Parse tone
    tone = _parse_tone(raw.tone) if raw.tone else None

    # Parse GCAM
    gcam = _parse_gcam(raw.gcam) if raw.gcam else {}

    # Parse quotations (v2.1+)
    quotations = _parse_quotations(raw.quotations) if raw.quotations else []

    # Parse amounts (v2.1+)
    amounts = _parse_amounts(raw.amounts) if raw.amounts else []

    # Parse all_names
    all_names = (
        [name.strip() for name in raw.all_names.split(";") if name.strip()]
        if raw.all_names
        else []
    )

    # Determine version (if v2 enhanced fields exist, it's v2)
    version = 2 if raw.themes_v2_enhanced else 1

    return cls(
        record_id=raw.gkg_record_id,
        date=date,
        source_url=raw.document_identifier,
        source_name=raw.source_common_name,
        source_collection=int(raw.source_collection_id),
        themes=themes,
        persons=persons,
        organizations=organizations,
        locations=locations,
        tone=tone,
        gcam=gcam,
        quotations=quotations,
        amounts=amounts,
        sharing_image=raw.sharing_image,
        all_names=all_names,
        version=version,
        is_translated=is_translated,
        original_record_id=original_record_id,
        translation_info=raw.translation_info,
    )
```

### `Quotation`

Bases: `BaseModel`

Quote extracted from a GKG document.

Source code in `src/py_gdelt/models/gkg.py`

```
class Quotation(BaseModel):
    """Quote extracted from a GKG document."""

    offset: int
    length: int
    verb: str
    quote: str
```

### `Amount`

Bases: `BaseModel`

Numerical amount extracted from a GKG document.

Source code in `src/py_gdelt/models/gkg.py`

```
class Amount(BaseModel):
    """Numerical amount extracted from a GKG document."""

    amount: float
    object: str
    offset: int
```

### `TVGKGRecord`

Bases: `BaseModel`

TV-GKG record with timecode mapping support.

Uses composition instead of inheritance from GKGRecord. Reuses GKG parsing logic but creates independent model.

Note

The following GKG fields are always empty in TV-GKG:

- sharing_image, related_images, social_image_embeds, social_video_embeds
- quotations, all_names, dates, amounts, translation_info

Special field

- timecode_mappings: Parsed from CHARTIMECODEOFFSETTOC in extras

Attributes:

| Name                  | Type                    | Description                                   |
| --------------------- | ----------------------- | --------------------------------------------- |
| `gkg_record_id`       | `str`                   | Unique record identifier.                     |
| `date`                | `datetime`              | Timestamp of the analysis.                    |
| `source_identifier`   | `str`                   | Source collection identifier.                 |
| `document_identifier` | `str`                   | URL or identifier of the source document.     |
| `themes`              | `list[str]`             | List of detected themes.                      |
| `locations`           | `list[str]`             | List of detected locations.                   |
| `persons`             | `list[str]`             | List of detected persons.                     |
| `organizations`       | `list[str]`             | List of detected organizations.               |
| `tone`                | \`float                 | None\`                                        |
| `extras`              | `str`                   | Raw extras field content.                     |
| `timecode_mappings`   | `list[TimecodeMapping]` | Parsed character offset to timecode mappings. |

Source code in `src/py_gdelt/models/gkg.py`

```
class TVGKGRecord(BaseModel):
    """TV-GKG record with timecode mapping support.

    Uses composition instead of inheritance from GKGRecord.
    Reuses GKG parsing logic but creates independent model.

    Note:
        The following GKG fields are always empty in TV-GKG:
        - sharing_image, related_images, social_image_embeds, social_video_embeds
        - quotations, all_names, dates, amounts, translation_info

    Special field:
        - timecode_mappings: Parsed from CHARTIMECODEOFFSETTOC in extras

    Attributes:
        gkg_record_id: Unique record identifier.
        date: Timestamp of the analysis.
        source_identifier: Source collection identifier.
        document_identifier: URL or identifier of the source document.
        themes: List of detected themes.
        locations: List of detected locations.
        persons: List of detected persons.
        organizations: List of detected organizations.
        tone: Average tone score.
        extras: Raw extras field content.
        timecode_mappings: Parsed character offset to timecode mappings.
    """

    gkg_record_id: str
    date: datetime
    source_identifier: str
    document_identifier: str
    themes: list[str] = Field(default_factory=list)
    locations: list[str] = Field(default_factory=list)
    persons: list[str] = Field(default_factory=list)
    organizations: list[str] = Field(default_factory=list)
    tone: float | None = None
    extras: str = ""
    timecode_mappings: list[TimecodeMapping] = Field(default_factory=list)

    @classmethod
    def from_raw(cls, raw: _RawGKG) -> TVGKGRecord:
        """Convert raw GKG to TV-GKG with timecode extraction.

        Args:
            raw: Internal raw GKG representation.

        Returns:
            Validated TVGKGRecord instance.

        Raises:
            ValueError: If date parsing fails.
        """
        timecodes = cls._parse_timecode_toc(raw.extras_xml)
        return cls(
            gkg_record_id=raw.gkg_record_id,
            date=parse_gdelt_datetime(raw.date),
            source_identifier=raw.source_common_name or "",
            document_identifier=raw.document_identifier or "",
            themes=_parse_semicolon_delimited(raw.themes_v1),
            locations=_parse_semicolon_delimited(raw.locations_v1),
            persons=_parse_semicolon_delimited(raw.persons_v1),
            organizations=_parse_semicolon_delimited(raw.organizations_v1),
            tone=_parse_tone_simple(raw.tone),
            extras=raw.extras_xml or "",
            timecode_mappings=timecodes,
        )

    @staticmethod
    def _parse_timecode_toc(extras: str | None) -> list[TimecodeMapping]:
        """Parse CHARTIMECODEOFFSETTOC:offset:timecode;offset:timecode;...

        Real format discovered: offset:timecode pairs separated by semicolons.

        Args:
            extras: Raw extras XML/text field from GKG record.

        Returns:
            List of TimecodeMapping instances.
        """
        if not extras:
            return []
        # Look for CHARTIMECODEOFFSETTOC block in extras
        for block in extras.split("<SPECIAL>"):
            if block.startswith("CHARTIMECODEOFFSETTOC:"):
                content = block[len("CHARTIMECODEOFFSETTOC:") :]
                mappings: list[TimecodeMapping] = []
                for raw_entry in content.split(";"):
                    entry = raw_entry.strip()
                    if ":" in entry:
                        parts = entry.split(":", 1)
                        try:
                            mappings.append(
                                TimecodeMapping(
                                    char_offset=int(parts[0]),
                                    timecode=parts[1],
                                ),
                            )
                        except (ValueError, IndexError):
                            logger.debug("Failed to parse timecode entry: %r", entry)
                            continue
                logger.debug("Parsed %d timecode mappings", len(mappings))
                return mappings
        # No CHARTIMECODEOFFSETTOC block found - this is normal for non-TV records
        if "<SPECIAL>" in extras:
            logger.debug("SPECIAL blocks present but no CHARTIMECODEOFFSETTOC found")
        return []
```

#### `from_raw(raw)`

Convert raw GKG to TV-GKG with timecode extraction.

Parameters:

| Name  | Type      | Description                      | Default    |
| ----- | --------- | -------------------------------- | ---------- |
| `raw` | `_RawGKG` | Internal raw GKG representation. | *required* |

Returns:

| Type          | Description                     |
| ------------- | ------------------------------- |
| `TVGKGRecord` | Validated TVGKGRecord instance. |

Raises:

| Type         | Description            |
| ------------ | ---------------------- |
| `ValueError` | If date parsing fails. |

Source code in `src/py_gdelt/models/gkg.py`

```
@classmethod
def from_raw(cls, raw: _RawGKG) -> TVGKGRecord:
    """Convert raw GKG to TV-GKG with timecode extraction.

    Args:
        raw: Internal raw GKG representation.

    Returns:
        Validated TVGKGRecord instance.

    Raises:
        ValueError: If date parsing fails.
    """
    timecodes = cls._parse_timecode_toc(raw.extras_xml)
    return cls(
        gkg_record_id=raw.gkg_record_id,
        date=parse_gdelt_datetime(raw.date),
        source_identifier=raw.source_common_name or "",
        document_identifier=raw.document_identifier or "",
        themes=_parse_semicolon_delimited(raw.themes_v1),
        locations=_parse_semicolon_delimited(raw.locations_v1),
        persons=_parse_semicolon_delimited(raw.persons_v1),
        organizations=_parse_semicolon_delimited(raw.organizations_v1),
        tone=_parse_tone_simple(raw.tone),
        extras=raw.extras_xml or "",
        timecode_mappings=timecodes,
    )
```

### `TimecodeMapping`

Bases: `NamedTuple`

Character offset to video timecode mapping (lightweight).

Used in TV-GKG to map text positions to video timestamps.

Attributes:

| Name          | Type  | Description                         |
| ------------- | ----- | ----------------------------------- |
| `char_offset` | `int` | Character offset in the transcript. |
| `timecode`    | `str` | Video timecode (e.g., "00:01:23").  |

Source code in `src/py_gdelt/models/gkg.py`

```
class TimecodeMapping(NamedTuple):
    """Character offset to video timecode mapping (lightweight).

    Used in TV-GKG to map text positions to video timestamps.

    Attributes:
        char_offset: Character offset in the transcript.
        timecode: Video timecode (e.g., "00:01:23").
    """

    char_offset: int
    timecode: str
```

## NGrams Models

### `NGramRecord`

Bases: `BaseModel`

GDELT NGram 3.0 record.

Represents an n-gram (word or phrase) occurrence in web content, including context and source information.

Source code in `src/py_gdelt/models/ngrams.py`

```
class NGramRecord(BaseModel):
    """GDELT NGram 3.0 record.

    Represents an n-gram (word or phrase) occurrence in web content,
    including context and source information.
    """

    date: datetime
    ngram: str  # Word or character
    language: str  # ISO 639-1/2
    segment_type: int  # 1=space-delimited, 2=scriptio continua
    position: int  # Article decile (0-90, where 0 = first 10% of article)
    pre_context: str  # ~7 words before
    post_context: str  # ~7 words after
    url: str

    @classmethod
    def from_raw(cls, raw: _RawNGram) -> NGramRecord:
        """Convert internal _RawNGram to public NGramRecord model.

        Args:
            raw: Internal raw ngram representation with string fields

        Returns:
            Validated NGramRecord instance

        Raises:
            ValueError: If date parsing or type conversion fails
        """
        return cls(
            date=parse_gdelt_datetime(raw.date),
            ngram=raw.ngram,
            language=raw.language,
            segment_type=int(raw.segment_type),
            position=int(raw.position),
            pre_context=raw.pre_context,
            post_context=raw.post_context,
            url=raw.url,
        )

    @property
    def context(self) -> str:
        """Get full context (pre + ngram + post).

        Returns:
            Full context string with ngram surrounded by pre and post context
        """
        return f"{self.pre_context} {self.ngram} {self.post_context}"

    @property
    def is_early_in_article(self) -> bool:
        """Check if ngram appears in first 30% of article.

        Returns:
            True if position <= 20 (first 30% of article)
        """
        return self.position <= 20

    @property
    def is_late_in_article(self) -> bool:
        """Check if ngram appears in last 30% of article.

        Returns:
            True if position >= 70 (last 30% of article)
        """
        return self.position >= 70
```

#### `context`

Get full context (pre + ngram + post).

Returns:

| Type  | Description                                                       |
| ----- | ----------------------------------------------------------------- |
| `str` | Full context string with ngram surrounded by pre and post context |

#### `is_early_in_article`

Check if ngram appears in first 30% of article.

Returns:

| Type   | Description                                    |
| ------ | ---------------------------------------------- |
| `bool` | True if position \<= 20 (first 30% of article) |

#### `is_late_in_article`

Check if ngram appears in last 30% of article.

Returns:

| Type   | Description                                  |
| ------ | -------------------------------------------- |
| `bool` | True if position >= 70 (last 30% of article) |

#### `from_raw(raw)`

Convert internal \_RawNGram to public NGramRecord model.

Parameters:

| Name  | Type        | Description                                          | Default    |
| ----- | ----------- | ---------------------------------------------------- | ---------- |
| `raw` | `_RawNGram` | Internal raw ngram representation with string fields | *required* |

Returns:

| Type          | Description                    |
| ------------- | ------------------------------ |
| `NGramRecord` | Validated NGramRecord instance |

Raises:

| Type         | Description                              |
| ------------ | ---------------------------------------- |
| `ValueError` | If date parsing or type conversion fails |

Source code in `src/py_gdelt/models/ngrams.py`

```
@classmethod
def from_raw(cls, raw: _RawNGram) -> NGramRecord:
    """Convert internal _RawNGram to public NGramRecord model.

    Args:
        raw: Internal raw ngram representation with string fields

    Returns:
        Validated NGramRecord instance

    Raises:
        ValueError: If date parsing or type conversion fails
    """
    return cls(
        date=parse_gdelt_datetime(raw.date),
        ngram=raw.ngram,
        language=raw.language,
        segment_type=int(raw.segment_type),
        position=int(raw.position),
        pre_context=raw.pre_context,
        post_context=raw.post_context,
        url=raw.url,
    )
```

### `BroadcastNGramRecord`

Bases: `BaseModel`

Broadcast NGram frequency record (TV or Radio).

Unified model for both TV and Radio NGrams since schemas are compatible. TV NGrams: 5 columns (DATE, STATION, HOUR, WORD, COUNT) Radio NGrams: 6 columns (DATE, STATION, HOUR, NGRAM, COUNT, SHOW)

Attributes:

| Name      | Type              | Description                           |
| --------- | ----------------- | ------------------------------------- |
| `date`    | `date`            | Date of the broadcast.                |
| `station` | `str`             | Station identifier (e.g., CNN, KQED). |
| `hour`    | `int`             | Hour of broadcast (0-23).             |
| `ngram`   | `str`             | Word or phrase.                       |
| `count`   | `int`             | Frequency count.                      |
| `show`    | \`str             | None\`                                |
| `source`  | `BroadcastSource` | Indicates origin (tv or radio).       |

Source code in `src/py_gdelt/models/ngrams.py`

```
class BroadcastNGramRecord(BaseModel):
    """Broadcast NGram frequency record (TV or Radio).

    Unified model for both TV and Radio NGrams since schemas are compatible.
    TV NGrams: 5 columns (DATE, STATION, HOUR, WORD, COUNT)
    Radio NGrams: 6 columns (DATE, STATION, HOUR, NGRAM, COUNT, SHOW)

    Attributes:
        date: Date of the broadcast.
        station: Station identifier (e.g., CNN, KQED).
        hour: Hour of broadcast (0-23).
        ngram: Word or phrase.
        count: Frequency count.
        show: Show name (Radio only, None for TV).
        source: Indicates origin (tv or radio).
    """

    date: date_type
    station: str
    hour: int = Field(ge=0, le=23)
    ngram: str
    count: int = Field(ge=0)
    show: str | None = None
    source: BroadcastSource

    @classmethod
    def from_raw(cls, raw: _RawBroadcastNGram, source: BroadcastSource) -> BroadcastNGramRecord:
        """Convert internal _RawBroadcastNGram to public model.

        Args:
            raw: Internal raw broadcast ngram representation.
            source: Source type (TV or Radio).

        Returns:
            Validated BroadcastNGramRecord instance.

        Raises:
            ValueError: If date parsing or type conversion fails.
        """
        return cls(
            date=parse_gdelt_date(raw.date),
            station=raw.station,
            hour=int(raw.hour),
            ngram=raw.ngram,
            count=int(raw.count),
            show=raw.show if raw.show else None,
            source=source,
        )
```

#### `from_raw(raw, source)`

Convert internal \_RawBroadcastNGram to public model.

Parameters:

| Name     | Type                 | Description                                  | Default    |
| -------- | -------------------- | -------------------------------------------- | ---------- |
| `raw`    | `_RawBroadcastNGram` | Internal raw broadcast ngram representation. | *required* |
| `source` | `BroadcastSource`    | Source type (TV or Radio).                   | *required* |

Returns:

| Type                   | Description                              |
| ---------------------- | ---------------------------------------- |
| `BroadcastNGramRecord` | Validated BroadcastNGramRecord instance. |

Raises:

| Type         | Description                               |
| ------------ | ----------------------------------------- |
| `ValueError` | If date parsing or type conversion fails. |

Source code in `src/py_gdelt/models/ngrams.py`

```
@classmethod
def from_raw(cls, raw: _RawBroadcastNGram, source: BroadcastSource) -> BroadcastNGramRecord:
    """Convert internal _RawBroadcastNGram to public model.

    Args:
        raw: Internal raw broadcast ngram representation.
        source: Source type (TV or Radio).

    Returns:
        Validated BroadcastNGramRecord instance.

    Raises:
        ValueError: If date parsing or type conversion fails.
    """
    return cls(
        date=parse_gdelt_date(raw.date),
        station=raw.station,
        hour=int(raw.hour),
        ngram=raw.ngram,
        count=int(raw.count),
        show=raw.show if raw.show else None,
        source=source,
    )
```

### `BroadcastSource`

Bases: `str`, `Enum`

Source type for broadcast NGrams.

Source code in `src/py_gdelt/models/ngrams.py`

```
class BroadcastSource(str, Enum):
    """Source type for broadcast NGrams."""

    TV = "tv"
    RADIO = "radio"
```

### `TVNGramRecord = BroadcastNGramRecord`

### `RadioNGramRecord = BroadcastNGramRecord`

## VGKG Models

### `VGKGRecord`

Bases: `BaseModel`

Visual GKG record with Cloud Vision annotations.

Nested structures (labels, faces, etc.) use TypedDict for performance. See tests/benchmarks/test_bench_vgkg_parsing.py for rationale.

Attributes:

| Name                   | Type                       | Description                                     |
| ---------------------- | -------------------------- | ----------------------------------------------- |
| `date`                 | `datetime`                 | Timestamp of the analysis.                      |
| `document_identifier`  | `str`                      | Source article URL.                             |
| `image_url`            | `str`                      | URL of the analyzed image.                      |
| `labels`               | `list[VisionLabelDict]`    | List of detected labels with confidence scores. |
| `logos`                | `list[VisionLabelDict]`    | List of detected logos.                         |
| `web_entities`         | `list[VisionLabelDict]`    | List of web entities matched to the image.      |
| `safe_search`          | \`SafeSearchDict           | None\`                                          |
| `faces`                | `list[FaceAnnotationDict]` | List of detected faces with pose information.   |
| `ocr_text`             | `str`                      | Text extracted via OCR.                         |
| `landmark_annotations` | `list[VisionLabelDict]`    | List of detected landmarks.                     |
| `domain`               | `str`                      | Domain of the source article.                   |
| `raw_json`             | `str`                      | Full Cloud Vision API JSON response.            |

Source code in `src/py_gdelt/models/vgkg.py`

```
class VGKGRecord(BaseModel):
    """Visual GKG record with Cloud Vision annotations.

    Nested structures (labels, faces, etc.) use TypedDict for performance.
    See tests/benchmarks/test_bench_vgkg_parsing.py for rationale.

    Attributes:
        date: Timestamp of the analysis.
        document_identifier: Source article URL.
        image_url: URL of the analyzed image.
        labels: List of detected labels with confidence scores.
        logos: List of detected logos.
        web_entities: List of web entities matched to the image.
        safe_search: SafeSearch annotation scores.
        faces: List of detected faces with pose information.
        ocr_text: Text extracted via OCR.
        landmark_annotations: List of detected landmarks.
        domain: Domain of the source article.
        raw_json: Full Cloud Vision API JSON response.
    """

    date: datetime
    document_identifier: str
    image_url: str
    labels: list[VisionLabelDict] = Field(default_factory=list)
    logos: list[VisionLabelDict] = Field(default_factory=list)
    web_entities: list[VisionLabelDict] = Field(default_factory=list)
    safe_search: SafeSearchDict | None = None
    faces: list[FaceAnnotationDict] = Field(default_factory=list)
    ocr_text: str = ""
    landmark_annotations: list[VisionLabelDict] = Field(default_factory=list)
    domain: str = ""
    raw_json: str = ""  # Keep as string - users can parse with json.loads() if needed

    @classmethod
    def from_raw(cls, raw: _RawVGKG) -> VGKGRecord:
        """Convert internal _RawVGKG to validated VGKGRecord model.

        Args:
            raw: Internal raw VGKG representation with string fields.

        Returns:
            Validated VGKGRecord instance.

        Raises:
            ValueError: If date parsing or type conversion fails.
        """
        return cls(
            date=parse_gdelt_datetime(raw.date),
            document_identifier=raw.document_identifier,
            image_url=raw.image_url,
            labels=cls._parse_labels(raw.labels),
            logos=cls._parse_labels(raw.logos),
            web_entities=cls._parse_labels(raw.web_entities),
            safe_search=cls._parse_safe_search(raw.safe_search),
            faces=cls._parse_faces(raw.faces),
            ocr_text=raw.ocr_text or "",
            landmark_annotations=cls._parse_labels(raw.landmark_annotations),
            domain=raw.domain or "",
            raw_json=raw.raw_json or "",
        )

    @classmethod
    def _parse_labels(cls, raw: str) -> list[VisionLabelDict]:
        """Parse Label<FIELD>Confidence<FIELD>MID<RECORD>... format.

        Args:
            raw: Delimited string with label records.

        Returns:
            List of VisionLabelDict objects.
        """
        if not raw:
            return []
        labels: list[VisionLabelDict] = []
        for record in raw.split(_RECORD_DELIM):
            if not record.strip():
                continue
            fields = record.split(_FIELD_DELIM)
            if len(fields) >= 2:
                try:
                    labels.append(
                        {
                            "description": fields[0],
                            "confidence": float(fields[1]) if fields[1] else 0.0,
                            "mid": fields[2] if len(fields) > 2 and fields[2] else None,
                        }
                    )
                except (ValueError, IndexError):
                    continue
        return labels

    @classmethod
    def _parse_safe_search(cls, raw: str) -> SafeSearchDict | None:
        """Parse safe_search field (4 integers).

        Args:
            raw: Delimited string with 4 safe search scores.

        Returns:
            SafeSearchDict or None if parsing fails.
        """
        if not raw:
            return None
        fields = raw.split(_FIELD_DELIM)
        if len(fields) < 4:
            return None
        try:
            return {
                "adult": int(fields[0]) if fields[0] else -1,
                "spoof": int(fields[1]) if fields[1] else -1,
                "medical": int(fields[2]) if fields[2] else -1,
                "violence": int(fields[3]) if fields[3] else -1,
            }
        except (ValueError, IndexError):
            return None

    @classmethod
    def _parse_faces(cls, raw: str) -> list[FaceAnnotationDict]:
        """Parse faces field (roll/pan/tilt angles, NOT emotions).

        Args:
            raw: Delimited string with face records.

        Returns:
            List of FaceAnnotationDict objects.
        """
        if not raw:
            return []
        faces: list[FaceAnnotationDict] = []
        for record in raw.split(_RECORD_DELIM):
            if not record.strip():
                continue
            fields = record.split(_FIELD_DELIM)
            if len(fields) >= 5:
                try:
                    faces.append(
                        {
                            "confidence": float(fields[0]) if fields[0] else 0.0,
                            "roll": float(fields[1]) if fields[1] else 0.0,
                            "pan": float(fields[2]) if fields[2] else 0.0,
                            "tilt": float(fields[3]) if fields[3] else 0.0,
                            "detection_confidence": float(fields[4]) if fields[4] else 0.0,
                            "bounding_box": fields[5] if len(fields) > 5 and fields[5] else None,
                        }
                    )
                except (ValueError, IndexError):
                    continue
        return faces
```

#### `from_raw(raw)`

Convert internal \_RawVGKG to validated VGKGRecord model.

Parameters:

| Name  | Type       | Description                                          | Default    |
| ----- | ---------- | ---------------------------------------------------- | ---------- |
| `raw` | `_RawVGKG` | Internal raw VGKG representation with string fields. | *required* |

Returns:

| Type         | Description                    |
| ------------ | ------------------------------ |
| `VGKGRecord` | Validated VGKGRecord instance. |

Raises:

| Type         | Description                               |
| ------------ | ----------------------------------------- |
| `ValueError` | If date parsing or type conversion fails. |

Source code in `src/py_gdelt/models/vgkg.py`

```
@classmethod
def from_raw(cls, raw: _RawVGKG) -> VGKGRecord:
    """Convert internal _RawVGKG to validated VGKGRecord model.

    Args:
        raw: Internal raw VGKG representation with string fields.

    Returns:
        Validated VGKGRecord instance.

    Raises:
        ValueError: If date parsing or type conversion fails.
    """
    return cls(
        date=parse_gdelt_datetime(raw.date),
        document_identifier=raw.document_identifier,
        image_url=raw.image_url,
        labels=cls._parse_labels(raw.labels),
        logos=cls._parse_labels(raw.logos),
        web_entities=cls._parse_labels(raw.web_entities),
        safe_search=cls._parse_safe_search(raw.safe_search),
        faces=cls._parse_faces(raw.faces),
        ocr_text=raw.ocr_text or "",
        landmark_annotations=cls._parse_labels(raw.landmark_annotations),
        domain=raw.domain or "",
        raw_json=raw.raw_json or "",
    )
```

### `VisionLabelDict`

Bases: `TypedDict`

Google Cloud Vision label annotation (lightweight).

Used for labels, logos, web_entities, and landmark_annotations fields. TypedDict avoids Pydantic validation overhead for nested structures.

Source code in `src/py_gdelt/models/vgkg.py`

```
class VisionLabelDict(TypedDict):
    """Google Cloud Vision label annotation (lightweight).

    Used for labels, logos, web_entities, and landmark_annotations fields.
    TypedDict avoids Pydantic validation overhead for nested structures.
    """

    description: str
    confidence: float
    mid: str | None  # Knowledge Graph MID - useful for entity linking
```

### `SafeSearchDict`

Bases: `TypedDict`

SafeSearch detection results.

Values are integers from Cloud Vision API:

- UNKNOWN = -1
- VERY_UNLIKELY = 0
- UNLIKELY = 1
- POSSIBLE = 2
- LIKELY = 3
- VERY_LIKELY = 4

Source code in `src/py_gdelt/models/vgkg.py`

```
class SafeSearchDict(TypedDict):
    """SafeSearch detection results.

    Values are integers from Cloud Vision API:
    - UNKNOWN = -1
    - VERY_UNLIKELY = 0
    - UNLIKELY = 1
    - POSSIBLE = 2
    - LIKELY = 3
    - VERY_LIKELY = 4
    """

    adult: int
    spoof: int
    medical: int
    violence: int
```

### `FaceAnnotationDict`

Bases: `TypedDict`

Detected face with pose angles.

Contains pose information (roll/pan/tilt angles), NOT emotion scores. Angles are in degrees relative to the camera.

Source code in `src/py_gdelt/models/vgkg.py`

```
class FaceAnnotationDict(TypedDict):
    """Detected face with pose angles.

    Contains pose information (roll/pan/tilt angles), NOT emotion scores.
    Angles are in degrees relative to the camera.
    """

    confidence: float
    roll: float  # Head roll angle in degrees
    pan: float  # Head pan angle in degrees
    tilt: float  # Head tilt angle in degrees
    detection_confidence: float
    bounding_box: str | None  # Format: "x1,y1,x2,y2"
```

## Graph Models

### `Entity`

Bases: `SchemaEvolutionMixin`, `BaseModel`

Entity extracted from GEG (Global Entity Graph).

Represents a named entity with metadata and knowledge graph links.

Attributes:

| Name                  | Type    | Description |
| --------------------- | ------- | ----------- |
| `name`                | `str`   | Entity name |
| `entity_type`         | `str`   | Entity type |
| `salience`            | \`float | None\`      |
| `wikipedia_url`       | \`str   | None\`      |
| `knowledge_graph_mid` | \`str   | None\`      |

Source code in `src/py_gdelt/models/graphs.py`

```
class Entity(SchemaEvolutionMixin, BaseModel):
    """Entity extracted from GEG (Global Entity Graph).

    Represents a named entity with metadata and knowledge graph links.

    Attributes:
        name: Entity name
        entity_type: Entity type
        salience: Salience score
        wikipedia_url: Wikipedia URL if available
        knowledge_graph_mid: Google Knowledge Graph MID if available
    """

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    name: str
    entity_type: str = Field(alias="type")
    salience: float | None = None
    wikipedia_url: str | None = None
    knowledge_graph_mid: str | None = Field(default=None, alias="mid")
```

### `GALRecord`

Bases: `SchemaEvolutionMixin`, `BaseModel`

Article List record.

Represents a news article with basic metadata.

Attributes:

| Name          | Type       | Description           |
| ------------- | ---------- | --------------------- |
| `date`        | `datetime` | Publication date/time |
| `url`         | `str`      | Article URL           |
| `title`       | \`str      | None\`                |
| `image`       | \`str      | None\`                |
| `description` | \`str      | None\`                |
| `author`      | \`str      | None\`                |
| `lang`        | `str`      | Language code         |

Source code in `src/py_gdelt/models/graphs.py`

```
class GALRecord(SchemaEvolutionMixin, BaseModel):
    """Article List record.

    Represents a news article with basic metadata.

    Attributes:
        date: Publication date/time
        url: Article URL
        title: Article title if available
        image: Primary image URL if available
        description: Article description if available
        author: Article author if available
        lang: Language code
    """

    date: datetime
    url: str
    title: str | None = None
    image: str | None = None
    description: str | None = None
    author: str | None = None
    lang: str

    @field_validator("date", mode="before")
    @classmethod
    def parse_date(cls, v: Any) -> datetime:
        """Parse date from ISO or GDELT format."""
        return parse_gdelt_datetime(v)
```

#### `parse_date(v)`

Parse date from ISO or GDELT format.

Source code in `src/py_gdelt/models/graphs.py`

```
@field_validator("date", mode="before")
@classmethod
def parse_date(cls, v: Any) -> datetime:
    """Parse date from ISO or GDELT format."""
    return parse_gdelt_datetime(v)
```

### `GEGRecord`

Bases: `SchemaEvolutionMixin`, `BaseModel`

Global Entity Graph record.

Represents a document with extracted entities and their metadata.

Attributes:

| Name       | Type           | Description                |
| ---------- | -------------- | -------------------------- |
| `date`     | `datetime`     | Publication date/time      |
| `url`      | `str`          | Source document URL        |
| `lang`     | `str`          | Language code              |
| `entities` | `list[Entity]` | List of extracted entities |

Source code in `src/py_gdelt/models/graphs.py`

```
class GEGRecord(SchemaEvolutionMixin, BaseModel):
    """Global Entity Graph record.

    Represents a document with extracted entities and their metadata.

    Attributes:
        date: Publication date/time
        url: Source document URL
        lang: Language code
        entities: List of extracted entities
    """

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    date: datetime
    url: str
    lang: str
    entities: list[Entity] = Field(default_factory=list)

    @field_validator("date", mode="before")
    @classmethod
    def parse_date(cls, v: Any) -> datetime:
        """Parse date from ISO or GDELT format."""
        return parse_gdelt_datetime(v)
```

#### `parse_date(v)`

Parse date from ISO or GDELT format.

Source code in `src/py_gdelt/models/graphs.py`

```
@field_validator("date", mode="before")
@classmethod
def parse_date(cls, v: Any) -> datetime:
    """Parse date from ISO or GDELT format."""
    return parse_gdelt_datetime(v)
```

### `GEMGRecord`

Bases: `SchemaEvolutionMixin`, `BaseModel`

Global Embedded Metadata Graph record.

Represents a document with extracted metadata tags and JSON-LD.

Attributes:

| Name       | Type            | Description                     |
| ---------- | --------------- | ------------------------------- |
| `date`     | `datetime`      | Publication date/time           |
| `url`      | `str`           | Source document URL             |
| `title`    | \`str           | None\`                          |
| `lang`     | `str`           | Language code                   |
| `metatags` | `list[MetaTag]` | List of extracted metadata tags |
| `jsonld`   | `list[str]`     | List of JSON-LD strings         |

Source code in `src/py_gdelt/models/graphs.py`

```
class GEMGRecord(SchemaEvolutionMixin, BaseModel):
    """Global Embedded Metadata Graph record.

    Represents a document with extracted metadata tags and JSON-LD.

    Attributes:
        date: Publication date/time
        url: Source document URL
        title: Document title if available
        lang: Language code
        metatags: List of extracted metadata tags
        jsonld: List of JSON-LD strings
    """

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    date: datetime
    url: str
    title: str | None = None
    lang: str
    metatags: list[MetaTag] = Field(default_factory=list)
    jsonld: list[str] = Field(default_factory=list)

    @field_validator("date", mode="before")
    @classmethod
    def parse_date(cls, v: Any) -> datetime:
        """Parse date from ISO or GDELT format."""
        return parse_gdelt_datetime(v)
```

#### `parse_date(v)`

Parse date from ISO or GDELT format.

Source code in `src/py_gdelt/models/graphs.py`

```
@field_validator("date", mode="before")
@classmethod
def parse_date(cls, v: Any) -> datetime:
    """Parse date from ISO or GDELT format."""
    return parse_gdelt_datetime(v)
```

### `GFGRecord`

Bases: `SchemaEvolutionMixin`, `BaseModel`

Global Frontpage Graph record (TSV format).

Represents a hyperlink from a frontpage to another URL.

Attributes:

| Name                 | Type       | Description              |
| -------------------- | ---------- | ------------------------ |
| `date`               | `datetime` | Publication date/time    |
| `from_frontpage_url` | `str`      | Source frontpage URL     |
| `link_url`           | `str`      | Destination link URL     |
| `link_text`          | `str`      | Anchor text of the link  |
| `page_position`      | `int`      | Position of link on page |
| `lang`               | `str`      | Language code            |

Source code in `src/py_gdelt/models/graphs.py`

```
class GFGRecord(SchemaEvolutionMixin, BaseModel):
    """Global Frontpage Graph record (TSV format).

    Represents a hyperlink from a frontpage to another URL.

    Attributes:
        date: Publication date/time
        from_frontpage_url: Source frontpage URL
        link_url: Destination link URL
        link_text: Anchor text of the link
        page_position: Position of link on page
        lang: Language code
    """

    date: datetime
    from_frontpage_url: str
    link_url: str
    link_text: str
    page_position: int
    lang: str

    @classmethod
    def from_raw(cls, raw: _RawGFGRecord) -> Self:
        """Convert internal _RawGFGRecord to public GFGRecord model.

        Args:
            raw: Internal _RawGFGRecord dataclass from TSV parsing.

        Returns:
            Validated GFGRecord with all fields parsed and typed.
        """
        date = parse_gdelt_datetime(raw.date)

        # Parse page_position (default to 0 if empty)
        page_position = 0
        if raw.page_position:
            try:
                page_position = int(raw.page_position)
            except ValueError:
                logger.warning("Invalid page_position '%s', defaulting to 0", raw.page_position)

        return cls(
            date=date,
            from_frontpage_url=raw.from_frontpage_url,
            link_url=raw.link_url,
            link_text=raw.link_text,
            page_position=page_position,
            lang=raw.lang,
        )
```

#### `from_raw(raw)`

Convert internal \_RawGFGRecord to public GFGRecord model.

Parameters:

| Name  | Type            | Description                                         | Default    |
| ----- | --------------- | --------------------------------------------------- | ---------- |
| `raw` | `_RawGFGRecord` | Internal \_RawGFGRecord dataclass from TSV parsing. | *required* |

Returns:

| Type   | Description                                           |
| ------ | ----------------------------------------------------- |
| `Self` | Validated GFGRecord with all fields parsed and typed. |

Source code in `src/py_gdelt/models/graphs.py`

```
@classmethod
def from_raw(cls, raw: _RawGFGRecord) -> Self:
    """Convert internal _RawGFGRecord to public GFGRecord model.

    Args:
        raw: Internal _RawGFGRecord dataclass from TSV parsing.

    Returns:
        Validated GFGRecord with all fields parsed and typed.
    """
    date = parse_gdelt_datetime(raw.date)

    # Parse page_position (default to 0 if empty)
    page_position = 0
    if raw.page_position:
        try:
            page_position = int(raw.page_position)
        except ValueError:
            logger.warning("Invalid page_position '%s', defaulting to 0", raw.page_position)

    return cls(
        date=date,
        from_frontpage_url=raw.from_frontpage_url,
        link_url=raw.link_url,
        link_text=raw.link_text,
        page_position=page_position,
        lang=raw.lang,
    )
```

### `GGGRecord`

Bases: `SchemaEvolutionMixin`, `BaseModel`

Global Geographic Graph record.

Represents a document with an extracted geographic location.

Attributes:

| Name            | Type       | Description              |
| --------------- | ---------- | ------------------------ |
| `date`          | `datetime` | Publication date/time    |
| `url`           | `str`      | Source document URL      |
| `location_name` | `str`      | Name of the location     |
| `lat`           | `float`    | Latitude (-90 to 90)     |
| `lon`           | `float`    | Longitude (-180 to 180)  |
| `context`       | `str`      | Surrounding text context |

Source code in `src/py_gdelt/models/graphs.py`

```
class GGGRecord(SchemaEvolutionMixin, BaseModel):
    """Global Geographic Graph record.

    Represents a document with an extracted geographic location.

    Attributes:
        date: Publication date/time
        url: Source document URL
        location_name: Name of the location
        lat: Latitude (-90 to 90)
        lon: Longitude (-180 to 180)
        context: Surrounding text context
    """

    date: datetime
    url: str
    location_name: str
    lat: float
    lon: float
    context: str

    @field_validator("date", mode="before")
    @classmethod
    def parse_date(cls, v: Any) -> datetime:
        """Parse date from ISO or GDELT format."""
        return parse_gdelt_datetime(v)

    @field_validator("lat")
    @classmethod
    def validate_lat(cls, v: float) -> float:
        """Validate latitude is within valid range.

        Args:
            v: Latitude value.

        Returns:
            Validated latitude.

        Raises:
            ValueError: If latitude is outside -90 to 90 range.
        """
        if not -90 <= v <= 90:
            msg = f"Latitude must be between -90 and 90, got {v}"
            raise ValueError(msg)
        return v

    @field_validator("lon")
    @classmethod
    def validate_lon(cls, v: float) -> float:
        """Validate longitude is within valid range.

        Args:
            v: Longitude value.

        Returns:
            Validated longitude.

        Raises:
            ValueError: If longitude is outside -180 to 180 range.
        """
        if not -180 <= v <= 180:
            msg = f"Longitude must be between -180 and 180, got {v}"
            raise ValueError(msg)
        return v
```

#### `parse_date(v)`

Parse date from ISO or GDELT format.

Source code in `src/py_gdelt/models/graphs.py`

```
@field_validator("date", mode="before")
@classmethod
def parse_date(cls, v: Any) -> datetime:
    """Parse date from ISO or GDELT format."""
    return parse_gdelt_datetime(v)
```

#### `validate_lat(v)`

Validate latitude is within valid range.

Parameters:

| Name | Type    | Description     | Default    |
| ---- | ------- | --------------- | ---------- |
| `v`  | `float` | Latitude value. | *required* |

Returns:

| Type    | Description         |
| ------- | ------------------- |
| `float` | Validated latitude. |

Raises:

| Type         | Description                             |
| ------------ | --------------------------------------- |
| `ValueError` | If latitude is outside -90 to 90 range. |

Source code in `src/py_gdelt/models/graphs.py`

```
@field_validator("lat")
@classmethod
def validate_lat(cls, v: float) -> float:
    """Validate latitude is within valid range.

    Args:
        v: Latitude value.

    Returns:
        Validated latitude.

    Raises:
        ValueError: If latitude is outside -90 to 90 range.
    """
    if not -90 <= v <= 90:
        msg = f"Latitude must be between -90 and 90, got {v}"
        raise ValueError(msg)
    return v
```

#### `validate_lon(v)`

Validate longitude is within valid range.

Parameters:

| Name | Type    | Description      | Default    |
| ---- | ------- | ---------------- | ---------- |
| `v`  | `float` | Longitude value. | *required* |

Returns:

| Type    | Description          |
| ------- | -------------------- |
| `float` | Validated longitude. |

Raises:

| Type         | Description                                |
| ------------ | ------------------------------------------ |
| `ValueError` | If longitude is outside -180 to 180 range. |

Source code in `src/py_gdelt/models/graphs.py`

```
@field_validator("lon")
@classmethod
def validate_lon(cls, v: float) -> float:
    """Validate longitude is within valid range.

    Args:
        v: Longitude value.

    Returns:
        Validated longitude.

    Raises:
        ValueError: If longitude is outside -180 to 180 range.
    """
    if not -180 <= v <= 180:
        msg = f"Longitude must be between -180 and 180, got {v}"
        raise ValueError(msg)
    return v
```

### `GQGRecord`

Bases: `SchemaEvolutionMixin`, `BaseModel`

Global Quotation Graph record.

Represents a document with extracted quotations and their context.

Attributes:

| Name     | Type          | Description                               |
| -------- | ------------- | ----------------------------------------- |
| `date`   | `datetime`    | Publication date/time                     |
| `url`    | `str`         | Source document URL                       |
| `lang`   | `str`         | Language code                             |
| `quotes` | `list[Quote]` | List of extracted quotations with context |

Source code in `src/py_gdelt/models/graphs.py`

```
class GQGRecord(SchemaEvolutionMixin, BaseModel):
    """Global Quotation Graph record.

    Represents a document with extracted quotations and their context.

    Attributes:
        date: Publication date/time
        url: Source document URL
        lang: Language code
        quotes: List of extracted quotations with context
    """

    date: datetime
    url: str
    lang: str
    quotes: list[Quote] = Field(default_factory=list)

    @field_validator("date", mode="before")
    @classmethod
    def parse_date(cls, v: Any) -> datetime:
        """Parse date from ISO or GDELT format."""
        return parse_gdelt_datetime(v)
```

#### `parse_date(v)`

Parse date from ISO or GDELT format.

Source code in `src/py_gdelt/models/graphs.py`

```
@field_validator("date", mode="before")
@classmethod
def parse_date(cls, v: Any) -> datetime:
    """Parse date from ISO or GDELT format."""
    return parse_gdelt_datetime(v)
```

### `MetaTag`

Bases: `SchemaEvolutionMixin`, `BaseModel`

Metadata tag extracted from GEMG (Global Embedded Metadata Graph).

Represents a single metadata tag from HTML meta tags or JSON-LD.

Attributes:

| Name       | Type  | Description  |
| ---------- | ----- | ------------ |
| `key`      | `str` | Tag key/name |
| `tag_type` | `str` | Tag type     |
| `value`    | `str` | Tag value    |

Source code in `src/py_gdelt/models/graphs.py`

```
class MetaTag(SchemaEvolutionMixin, BaseModel):
    """Metadata tag extracted from GEMG (Global Embedded Metadata Graph).

    Represents a single metadata tag from HTML meta tags or JSON-LD.

    Attributes:
        key: Tag key/name
        tag_type: Tag type
        value: Tag value
    """

    model_config = ConfigDict(extra="ignore", populate_by_name=True)

    key: str
    tag_type: str = Field(alias="type")
    value: str
```

### `Quote`

Bases: `SchemaEvolutionMixin`, `BaseModel`

Quote extracted from GQG (Global Quotation Graph).

Represents a single quotation with surrounding context.

Attributes:

| Name    | Type  | Description           |
| ------- | ----- | --------------------- |
| `pre`   | `str` | Text before the quote |
| `quote` | `str` | The quotation text    |
| `post`  | `str` | Text after the quote  |

Source code in `src/py_gdelt/models/graphs.py`

```
class Quote(SchemaEvolutionMixin, BaseModel):
    """Quote extracted from GQG (Global Quotation Graph).

    Represents a single quotation with surrounding context.

    Attributes:
        pre: Text before the quote
        quote: The quotation text
        post: Text after the quote
    """

    pre: str
    quote: str
    post: str
```

## Common Models

### `Location`

Bases: `BaseModel`

Geographic location from GDELT data.

Source code in `src/py_gdelt/models/common.py`

```
class Location(BaseModel):
    """Geographic location from GDELT data."""

    lat: float | None = None
    lon: float | None = None
    feature_id: str | None = None  # GNIS/ADM1 code
    name: str | None = None
    country_code: str | None = None  # FIPS code
    adm1_code: str | None = None
    adm2_code: str | None = None
    geo_type: int | None = None  # 1=Country, 2=State, 3=City, 4=Coordinates

    def as_tuple(self) -> tuple[float, float]:
        """Return (lat, lon) tuple. Raises ValueError if either is None."""
        if self.lat is None or self.lon is None:
            msg = "Cannot create tuple: lat or lon is None"
            raise ValueError(msg)
        return (self.lat, self.lon)

    def as_wkt(self) -> str:
        """Return WKT POINT string for geopandas compatibility.

        Returns:
            WKT POINT string in format "POINT(lon lat)".

        Raises:
            ValueError: If lat or lon is None.
        """
        if self.lat is None or self.lon is None:
            msg = "Cannot create WKT: lat or lon is None"
            raise ValueError(msg)
        return f"POINT({self.lon} {self.lat})"

    @property
    def has_coordinates(self) -> bool:
        """Check if location has valid coordinates."""
        return self.lat is not None and self.lon is not None
```

#### `has_coordinates`

Check if location has valid coordinates.

#### `as_tuple()`

Return (lat, lon) tuple. Raises ValueError if either is None.

Source code in `src/py_gdelt/models/common.py`

```
def as_tuple(self) -> tuple[float, float]:
    """Return (lat, lon) tuple. Raises ValueError if either is None."""
    if self.lat is None or self.lon is None:
        msg = "Cannot create tuple: lat or lon is None"
        raise ValueError(msg)
    return (self.lat, self.lon)
```

#### `as_wkt()`

Return WKT POINT string for geopandas compatibility.

Returns:

| Type  | Description                                  |
| ----- | -------------------------------------------- |
| `str` | WKT POINT string in format "POINT(lon lat)". |

Raises:

| Type         | Description            |
| ------------ | ---------------------- |
| `ValueError` | If lat or lon is None. |

Source code in `src/py_gdelt/models/common.py`

```
def as_wkt(self) -> str:
    """Return WKT POINT string for geopandas compatibility.

    Returns:
        WKT POINT string in format "POINT(lon lat)".

    Raises:
        ValueError: If lat or lon is None.
    """
    if self.lat is None or self.lon is None:
        msg = "Cannot create WKT: lat or lon is None"
        raise ValueError(msg)
    return f"POINT({self.lon} {self.lat})"
```

### `ToneScores`

Bases: `BaseModel`

Tone analysis scores from GDELT.

Source code in `src/py_gdelt/models/common.py`

```
class ToneScores(BaseModel):
    """Tone analysis scores from GDELT."""

    tone: float = Field(..., ge=-100, le=100)  # Overall tone (-100 to +100)
    positive_score: float
    negative_score: float
    polarity: float  # Emotional extremity
    activity_reference_density: float
    self_group_reference_density: float
    word_count: int | None = None
```

### `EntityMention`

Bases: `BaseModel`

Entity mention from GKG records.

Source code in `src/py_gdelt/models/common.py`

```
class EntityMention(BaseModel):
    """Entity mention from GKG records."""

    entity_type: str  # PERSON, ORG, LOCATION, etc.
    name: str
    offset: int | None = None  # Character offset in source
    confidence: float | None = None
```

## Result Models

### `FetchResult`

Bases: `Generic[T]`

Result container with partial failure tracking.

Source code in `src/py_gdelt/models/common.py`

```
@dataclass
class FetchResult(Generic[T]):
    """Result container with partial failure tracking."""

    data: list[T]
    failed: list[FailedRequest] = field(default_factory=list)

    @property
    def complete(self) -> bool:
        """True if no requests failed."""
        return len(self.failed) == 0

    @property
    def partial(self) -> bool:
        """True if some but not all requests failed."""
        return len(self.failed) > 0 and len(self.data) > 0

    @property
    def total_failed(self) -> int:
        """Number of failed requests."""
        return len(self.failed)

    def __iter__(self) -> Iterator[T]:
        """Allow direct iteration over data."""
        return iter(self.data)

    def __len__(self) -> int:
        """Return count of successful items."""
        return len(self.data)
```

#### `complete`

True if no requests failed.

#### `partial`

True if some but not all requests failed.

#### `total_failed`

Number of failed requests.

#### `__iter__()`

Allow direct iteration over data.

Source code in `src/py_gdelt/models/common.py`

```
def __iter__(self) -> Iterator[T]:
    """Allow direct iteration over data."""
    return iter(self.data)
```

#### `__len__()`

Return count of successful items.

Source code in `src/py_gdelt/models/common.py`

```
def __len__(self) -> int:
    """Return count of successful items."""
    return len(self.data)
```

### `FailedRequest`

Represents a failed request in a partial result.

Source code in `src/py_gdelt/models/common.py`

```
@dataclass(slots=True)
class FailedRequest:
    """Represents a failed request in a partial result."""

    url: str
    error: str
    status_code: int | None = None
    retry_after: int | None = None  # For rate limit errors
```

# Exceptions API

Exception hierarchy for error handling.

## Exception Hierarchy

```
GDELTError (base)
 APIError
    RateLimitError
    APIUnavailableError
    InvalidQueryError
 DataError
    ParseError
    ValidationError
        InvalidCodeError
 ConfigurationError
 BigQueryError
 SecurityError
```

## Base Exception

### `GDELTError`

Bases: `Exception`

Base exception for all GDELT client errors.

All custom exceptions in this library inherit from this class, allowing consumers to catch all library-specific errors with a single handler.

Source code in `src/py_gdelt/exceptions.py`

```
class GDELTError(Exception):
    """
    Base exception for all GDELT client errors.

    All custom exceptions in this library inherit from this class,
    allowing consumers to catch all library-specific errors with a single handler.
    """
```

## API Exceptions

### `APIError`

Bases: `GDELTError`

Base exception for all API-related errors.

Raised when errors occur during communication with GDELT REST APIs.

Source code in `src/py_gdelt/exceptions.py`

```
class APIError(GDELTError):
    """
    Base exception for all API-related errors.

    Raised when errors occur during communication with GDELT REST APIs.
    """
```

### `RateLimitError`

Bases: `APIError`

Raised when API rate limits are exceeded.

Parameters:

| Name          | Type  | Description       | Default                                                                                    |
| ------------- | ----- | ----------------- | ------------------------------------------------------------------------------------------ |
| `message`     | `str` | Error description | *required*                                                                                 |
| `retry_after` | \`int | None\`            | Optional number of seconds to wait before retrying. None if the retry duration is unknown. |

Source code in `src/py_gdelt/exceptions.py`

```
class RateLimitError(APIError):
    """
    Raised when API rate limits are exceeded.

    Args:
        message: Error description
        retry_after: Optional number of seconds to wait before retrying.
                    None if the retry duration is unknown.
    """

    def __init__(self, message: str, retry_after: int | None = None) -> None:
        super().__init__(message)
        self.retry_after = retry_after

    def __str__(self) -> str:
        """Return string representation including retry information."""
        base_message = super().__str__()
        if self.retry_after is not None:
            return f"{base_message} (retry after {self.retry_after} seconds)"
        return base_message
```

#### `__str__()`

Return string representation including retry information.

Source code in `src/py_gdelt/exceptions.py`

```
def __str__(self) -> str:
    """Return string representation including retry information."""
    base_message = super().__str__()
    if self.retry_after is not None:
        return f"{base_message} (retry after {self.retry_after} seconds)"
    return base_message
```

### `APIUnavailableError`

Bases: `APIError`

Raised when a GDELT API is temporarily unavailable.

This typically indicates server-side issues or maintenance windows. Consider falling back to BigQuery when this occurs.

Source code in `src/py_gdelt/exceptions.py`

```
class APIUnavailableError(APIError):
    """
    Raised when a GDELT API is temporarily unavailable.

    This typically indicates server-side issues or maintenance windows.
    Consider falling back to BigQuery when this occurs.
    """
```

### `InvalidQueryError`

Bases: `APIError`

Raised when API request parameters are invalid.

This indicates a client-side error in query construction or parameters.

Source code in `src/py_gdelt/exceptions.py`

```
class InvalidQueryError(APIError):
    """
    Raised when API request parameters are invalid.

    This indicates a client-side error in query construction or parameters.
    """
```

## Data Exceptions

### `DataError`

Bases: `GDELTError`

Base exception for data processing and validation errors.

Raised when errors occur during data parsing, transformation, or validation.

Source code in `src/py_gdelt/exceptions.py`

```
class DataError(GDELTError):
    """
    Base exception for data processing and validation errors.

    Raised when errors occur during data parsing, transformation, or validation.
    """
```

### `ParseError`

Bases: `DataError`, `ValueError`

Raised when data parsing fails.

Parameters:

| Name       | Type  | Description       | Default                                                         |
| ---------- | ----- | ----------------- | --------------------------------------------------------------- |
| `message`  | `str` | Error description | *required*                                                      |
| `raw_data` | \`str | None\`            | Optional raw data that failed to parse, for debugging purposes. |

Source code in `src/py_gdelt/exceptions.py`

```
class ParseError(DataError, ValueError):
    """
    Raised when data parsing fails.

    Args:
        message: Error description
        raw_data: Optional raw data that failed to parse, for debugging purposes.
    """

    def __init__(self, message: str, raw_data: str | None = None) -> None:
        super().__init__(message)
        self.raw_data = raw_data

    def __str__(self) -> str:
        """Return string representation with truncated raw data if available."""
        base_message = super().__str__()
        if self.raw_data is not None:
            # Truncate raw data to first 100 characters for readability
            truncated = self.raw_data[:100]
            if len(self.raw_data) > 100:
                truncated += "..."
            return f"{base_message} (raw data: {truncated!r})"
        return base_message
```

#### `__str__()`

Return string representation with truncated raw data if available.

Source code in `src/py_gdelt/exceptions.py`

```
def __str__(self) -> str:
    """Return string representation with truncated raw data if available."""
    base_message = super().__str__()
    if self.raw_data is not None:
        # Truncate raw data to first 100 characters for readability
        truncated = self.raw_data[:100]
        if len(self.raw_data) > 100:
            truncated += "..."
        return f"{base_message} (raw data: {truncated!r})"
    return base_message
```

### `ValidationError`

Bases: `DataError`

Raised when data validation fails.

This includes Pydantic validation errors and custom validation logic.

Source code in `src/py_gdelt/exceptions.py`

```
class ValidationError(DataError):
    """
    Raised when data validation fails.

    This includes Pydantic validation errors and custom validation logic.
    """
```

### `InvalidCodeError`

Bases: `ValidationError`

Raised when an invalid GDELT code is encountered.

Parameters:

| Name          | Type        | Description                                              | Default                                 |
| ------------- | ----------- | -------------------------------------------------------- | --------------------------------------- |
| `message`     | `str`       | Error description                                        | *required*                              |
| `code`        | `str`       | The invalid code value                                   | *required*                              |
| `code_type`   | `str`       | Type of code (e.g., "cameo", "theme", "country", "fips") | *required*                              |
| `suggestions` | \`list[str] | None\`                                                   | Optional list of suggested valid codes  |
| `help_url`    | \`str       | None\`                                                   | Optional URL to reference documentation |

Source code in `src/py_gdelt/exceptions.py`

```
class InvalidCodeError(ValidationError):
    """
    Raised when an invalid GDELT code is encountered.

    Args:
        message: Error description
        code: The invalid code value
        code_type: Type of code (e.g., "cameo", "theme", "country", "fips")
        suggestions: Optional list of suggested valid codes
        help_url: Optional URL to reference documentation
    """

    def __init__(
        self,
        message: str,
        code: str,
        code_type: str,
        suggestions: list[str] | None = None,
        help_url: str | None = None,
    ) -> None:
        super().__init__(message)
        self.code = code
        self.code_type = code_type
        self.suggestions = suggestions or []
        self.help_url = help_url

    def __str__(self) -> str:
        """Return string representation including code details and guidance."""
        base_message = super().__str__()

        # Always include code and code_type in the base representation (backward compatibility)
        lines = [f"{base_message} (code={self.code!r}, type={self.code_type!r})"]

        # Add country-specific help
        if self.code_type == "country":
            lines.extend(
                [
                    "",
                    "Accepted formats:",
                    "  - FIPS (2 chars): US, UK, IR, FR, GM, CH, RS",
                    "  - ISO3 (3 chars): USA, GBR, IRN, FRA, DEU, CHN, RUS",
                ]
            )

        # Add suggestions if available
        if self.suggestions:
            lines.extend(["", f"Did you mean: {', '.join(self.suggestions)}?"])

        # Add reference URL if available
        if self.help_url:
            lines.extend(["", f"Reference: {self.help_url}"])

        return "\n".join(lines)
```

#### `__str__()`

Return string representation including code details and guidance.

Source code in `src/py_gdelt/exceptions.py`

```
def __str__(self) -> str:
    """Return string representation including code details and guidance."""
    base_message = super().__str__()

    # Always include code and code_type in the base representation (backward compatibility)
    lines = [f"{base_message} (code={self.code!r}, type={self.code_type!r})"]

    # Add country-specific help
    if self.code_type == "country":
        lines.extend(
            [
                "",
                "Accepted formats:",
                "  - FIPS (2 chars): US, UK, IR, FR, GM, CH, RS",
                "  - ISO3 (3 chars): USA, GBR, IRN, FRA, DEU, CHN, RUS",
            ]
        )

    # Add suggestions if available
    if self.suggestions:
        lines.extend(["", f"Did you mean: {', '.join(self.suggestions)}?"])

    # Add reference URL if available
    if self.help_url:
        lines.extend(["", f"Reference: {self.help_url}"])

    return "\n".join(lines)
```

## Other Exceptions

### `ConfigurationError`

Bases: `GDELTError`

Raised when client configuration is invalid or incomplete.

This includes missing credentials, invalid settings, or misconfiguration.

Source code in `src/py_gdelt/exceptions.py`

```
class ConfigurationError(GDELTError):
    """
    Raised when client configuration is invalid or incomplete.

    This includes missing credentials, invalid settings, or misconfiguration.
    """
```

### `BigQueryError`

Bases: `GDELTError`

Raised when BigQuery operations fail.

This includes query execution errors, authentication failures, and quota/billing issues.

Source code in `src/py_gdelt/exceptions.py`

```
class BigQueryError(GDELTError):
    """
    Raised when BigQuery operations fail.

    This includes query execution errors, authentication failures,
    and quota/billing issues.
    """
```

### `SecurityError`

Bases: `GDELTError`

Raised when a security check fails.

This includes URL validation failures, path traversal detection, zip bomb detection, and other security-related issues.

Source code in `src/py_gdelt/exceptions.py`

```
class SecurityError(GDELTError):
    """
    Raised when a security check fails.

    This includes URL validation failures, path traversal detection,
    zip bomb detection, and other security-related issues.
    """
```

## Usage

```
import asyncio
import logging

from py_gdelt.exceptions import APIError, RateLimitError, DataError

logger = logging.getLogger(__name__)

try:
    result = await client.doc.query(doc_filter)
except RateLimitError as e:
    # Handle rate limiting with retry info
    if e.retry_after:
        await asyncio.sleep(e.retry_after)
except APIError as e:
    # Handle other API errors (network, unavailable, etc.)
    logger.error(f"API error: {e}")
except DataError as e:
    # Handle data parsing errors
    logger.error(f"Data error: {e}")
except Exception as e:
    # Handle unexpected errors
    logger.error(f"Unexpected error: {e}")
```
# Examples

# Basic Usage Examples

Common usage patterns for py-gdelt.

## Query Events

```
from datetime import date, timedelta
from py_gdelt import GDELTClient
from py_gdelt.filters import DateRange, EventFilter

async with GDELTClient() as client:
    yesterday = date.today() - timedelta(days=1)

    event_filter = EventFilter(
        date_range=DateRange(start=yesterday, end=yesterday),
        actor1_country="USA",
    )

    result = await client.events.query(event_filter)
    print(f"Found {len(result)} events")
```

## Search Articles

```
from py_gdelt.filters import DocFilter

async with GDELTClient() as client:
    doc_filter = DocFilter(
        query="climate change",
        timespan="24h",
        max_results=10,
    )

    articles = await client.doc.query(doc_filter)
```

## Geographic Search

```
async with GDELTClient() as client:
    result = await client.geo.search(
        "earthquake",
        timespan="7d",
        max_points=20,
    )
```

See complete examples in `examples/` directory.

# Advanced Patterns

Production-ready patterns for py-gdelt.

## Custom Configuration

```
from pathlib import Path
from py_gdelt import GDELTClient, GDELTSettings

settings = GDELTSettings(
    timeout=60,
    max_retries=5,
    cache_dir=Path("/custom/cache"),
    fallback_to_bigquery=True,
)

async with GDELTClient(settings=settings) as client:
    ...
```

## Error Handling

```
from py_gdelt.exceptions import APIError, DataError

try:
    result = await client.doc.query(doc_filter)
except APIError as e:
    logger.error(f"API error: {e}")
except DataError as e:
    logger.error(f"Data error: {e}")
```

## Streaming Large Datasets

```
async for event in client.events.stream(event_filter):
    process(event)  # Memory-efficient
```

See `notebooks/02_advanced_patterns.ipynb` for detailed examples.
